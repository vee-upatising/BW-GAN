{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from skimage.color import rgb2gray, gray2rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "        color = []\n",
    "        bw = []\n",
    "        paths = []\n",
    "        #get all files in this folder\n",
    "        for r, d, f in os.walk(r\"D:\\Downloads\\painter-by-numbers-resized\"):\n",
    "            for file in f:\n",
    "                if '.jpg' in file:\n",
    "                    paths.append(os.path.join(r, file))\n",
    "        #for each file add normal resolution and small resolution to arrays\n",
    "        for path in paths:\n",
    "            img = Image.open(path)\n",
    "            x = img.resize((128,128))\n",
    "            placeholder = np.array(x)\n",
    "            color.append(placeholder)\n",
    "            bw.append(rgb2gray(placeholder))\n",
    "            \n",
    "        del paths    \n",
    "       \n",
    "        y_train = np.array(color)\n",
    "        y_train = y_train.reshape(len(color),128,128,3)\n",
    "        x_train = np.array(bw)\n",
    "        x_train = x_train.reshape(len(bw),128,128,1)\n",
    "        return y_train, x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRGAN():\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Shape of high resolution output image\n",
    "        self.img_rows = 128\n",
    "        self.img_cols = 128\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        \n",
    "        # Shape of low resolution input image\n",
    "        self.latent_dim = (128,128,1)\n",
    "\n",
    "        #optimizer (learning rate and beta values)\n",
    "        optimizer = Adam(0.0001, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        generator = self.generator\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=self.latent_dim)\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(140, input_shape=self.latent_dim, kernel_size=(3,3), padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(140, kernel_size=(3,3), padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(140, kernel_size=(2,2), padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(140, kernel_size=(4,4), padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(140, kernel_size=(3,3), padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(140, kernel_size=(3,3), padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(3, kernel_size=(2,2), padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=self.latent_dim)\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "    \n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        Y_train, X_train = load_data()\n",
    "\n",
    "        # Rescale to be between 0 & 1\n",
    "        X_train = X_train / 255\n",
    "        Y_train = Y_train / 255\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        \n",
    "        # Placeholder for loss function values\n",
    "        g_loss_epochs = np.zeros((epochs, 1))\n",
    "        d_loss_epochs = np.zeros((epochs, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, Y_train.shape[0], batch_size)\n",
    "            imgs = Y_train[idx]\n",
    "\n",
    "            # Generate super resolution images from the random batch of images\n",
    "            gen_imgs = self.generator.predict(X_train[idx])\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(X_train[idx], valid)\n",
    "            \n",
    "            #save loss history\n",
    "            g_loss_epochs[epoch] = g_loss\n",
    "            d_loss_epochs[epoch] = d_loss[0]\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch, X_train, idx)\n",
    "                \n",
    "        return g_loss_epochs, d_loss_epochs\n",
    "\n",
    "    def save_imgs(self, epoch, X_train, idx):\n",
    "        r, c = 3, 3\n",
    "        # Select 9 random images\n",
    "        index = np.random.randint(0, X_train.shape[0], 9)\n",
    "        images = X_train[idx]\n",
    "        # Super resolution the images\n",
    "        gen_imgs = self.generator.predict(images)\n",
    "        gen_imgs = np.array(gen_imgs) * 255\n",
    "        gen_imgs = gen_imgs.astype(int)\n",
    "        # Plot each image\n",
    "        fig=plt.figure(figsize=(20, 20))\n",
    "        for i in range(1, c*r+1):\n",
    "            img = gen_imgs[i-1]\n",
    "            fig.add_subplot(r, c, i)\n",
    "            plt.imshow(img)\n",
    "        fig.savefig(r\"C:\\Users\\Vee\\Desktop\\python\\GAN\\epoch_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "        # save model to .h5 file\n",
    "        self.generator.save(\"generator.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65537     \n",
      "=================================================================\n",
      "Total params: 455,745\n",
      "Trainable params: 454,849\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 128, 128, 140)     1400      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 128, 128, 140)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 128, 128, 140)     176540    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 128, 128, 140)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 128, 128, 140)     78540     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 128, 128, 140)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 128, 128, 140)     313740    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 128, 128, 140)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 128, 128, 140)     176540    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 128, 128, 140)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 128, 128, 140)     176540    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 128, 128, 140)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 128, 128, 3)       1683      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 128, 128, 3)       0         \n",
      "=================================================================\n",
      "Total params: 924,983\n",
      "Trainable params: 924,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan = SRGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 1.116400, acc.: 16.67%] [G loss: 0.472347]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 1.077887, acc.: 50.00%] [G loss: 0.673541]\n",
      "2 [D loss: 0.911348, acc.: 50.00%] [G loss: 0.910210]\n",
      "3 [D loss: 0.795538, acc.: 55.56%] [G loss: 0.752919]\n",
      "4 [D loss: 0.766020, acc.: 50.00%] [G loss: 0.857400]\n",
      "5 [D loss: 0.621384, acc.: 77.78%] [G loss: 0.693831]\n",
      "6 [D loss: 0.490977, acc.: 77.78%] [G loss: 0.958575]\n",
      "7 [D loss: 0.589162, acc.: 66.67%] [G loss: 0.917486]\n",
      "8 [D loss: 0.647061, acc.: 66.67%] [G loss: 1.095881]\n",
      "9 [D loss: 1.065752, acc.: 38.89%] [G loss: 1.539848]\n",
      "10 [D loss: 0.549508, acc.: 61.11%] [G loss: 1.751008]\n",
      "11 [D loss: 0.750736, acc.: 55.56%] [G loss: 1.589751]\n",
      "12 [D loss: 0.681275, acc.: 66.67%] [G loss: 1.309301]\n",
      "13 [D loss: 0.862175, acc.: 50.00%] [G loss: 1.204645]\n",
      "14 [D loss: 0.689426, acc.: 72.22%] [G loss: 1.622092]\n",
      "15 [D loss: 0.667622, acc.: 61.11%] [G loss: 1.502150]\n",
      "16 [D loss: 1.045616, acc.: 50.00%] [G loss: 1.483633]\n",
      "17 [D loss: 0.721022, acc.: 66.67%] [G loss: 1.171815]\n",
      "18 [D loss: 1.136566, acc.: 27.78%] [G loss: 1.421440]\n",
      "19 [D loss: 0.911776, acc.: 55.56%] [G loss: 2.069288]\n",
      "20 [D loss: 0.828696, acc.: 50.00%] [G loss: 1.609376]\n",
      "21 [D loss: 0.794808, acc.: 33.33%] [G loss: 1.618669]\n",
      "22 [D loss: 0.911907, acc.: 50.00%] [G loss: 1.397110]\n",
      "23 [D loss: 0.498970, acc.: 72.22%] [G loss: 2.805143]\n",
      "24 [D loss: 0.628133, acc.: 55.56%] [G loss: 2.275437]\n",
      "25 [D loss: 0.378086, acc.: 77.78%] [G loss: 1.814854]\n",
      "26 [D loss: 0.413670, acc.: 77.78%] [G loss: 2.112710]\n",
      "27 [D loss: 0.448784, acc.: 72.22%] [G loss: 2.274598]\n",
      "28 [D loss: 0.559945, acc.: 66.67%] [G loss: 2.015905]\n",
      "29 [D loss: 0.590301, acc.: 61.11%] [G loss: 1.878703]\n",
      "30 [D loss: 0.547098, acc.: 72.22%] [G loss: 1.694734]\n",
      "31 [D loss: 0.578057, acc.: 83.33%] [G loss: 2.102182]\n",
      "32 [D loss: 0.582844, acc.: 66.67%] [G loss: 1.463695]\n",
      "33 [D loss: 0.950417, acc.: 50.00%] [G loss: 1.719368]\n",
      "34 [D loss: 1.427417, acc.: 38.89%] [G loss: 1.381244]\n",
      "35 [D loss: 0.754777, acc.: 61.11%] [G loss: 1.394827]\n",
      "36 [D loss: 0.788880, acc.: 55.56%] [G loss: 1.380823]\n",
      "37 [D loss: 1.125068, acc.: 50.00%] [G loss: 0.779367]\n",
      "38 [D loss: 0.884816, acc.: 61.11%] [G loss: 1.093857]\n",
      "39 [D loss: 1.056663, acc.: 38.89%] [G loss: 1.762799]\n",
      "40 [D loss: 1.176595, acc.: 38.89%] [G loss: 1.261356]\n",
      "41 [D loss: 1.167818, acc.: 22.22%] [G loss: 1.673291]\n",
      "42 [D loss: 0.595316, acc.: 72.22%] [G loss: 2.142961]\n",
      "43 [D loss: 1.120807, acc.: 27.78%] [G loss: 0.883992]\n",
      "44 [D loss: 1.040687, acc.: 50.00%] [G loss: 1.223000]\n",
      "45 [D loss: 0.824236, acc.: 55.56%] [G loss: 1.020665]\n",
      "46 [D loss: 1.002636, acc.: 44.44%] [G loss: 1.281785]\n",
      "47 [D loss: 0.719342, acc.: 66.67%] [G loss: 1.214149]\n",
      "48 [D loss: 0.831189, acc.: 38.89%] [G loss: 0.986973]\n",
      "49 [D loss: 0.953725, acc.: 55.56%] [G loss: 1.360204]\n",
      "50 [D loss: 0.715489, acc.: 61.11%] [G loss: 1.063270]\n",
      "51 [D loss: 1.008425, acc.: 38.89%] [G loss: 0.961828]\n",
      "52 [D loss: 0.898124, acc.: 44.44%] [G loss: 1.129408]\n",
      "53 [D loss: 0.773055, acc.: 50.00%] [G loss: 1.661563]\n",
      "54 [D loss: 0.772607, acc.: 61.11%] [G loss: 1.418238]\n",
      "55 [D loss: 0.733308, acc.: 33.33%] [G loss: 1.248535]\n",
      "56 [D loss: 0.748928, acc.: 66.67%] [G loss: 1.416464]\n",
      "57 [D loss: 0.733821, acc.: 50.00%] [G loss: 1.550688]\n",
      "58 [D loss: 0.445219, acc.: 72.22%] [G loss: 1.129610]\n",
      "59 [D loss: 0.722862, acc.: 55.56%] [G loss: 1.345453]\n",
      "60 [D loss: 0.754072, acc.: 50.00%] [G loss: 1.530739]\n",
      "61 [D loss: 0.823068, acc.: 55.56%] [G loss: 1.447097]\n",
      "62 [D loss: 0.640912, acc.: 61.11%] [G loss: 1.716135]\n",
      "63 [D loss: 0.735421, acc.: 44.44%] [G loss: 1.230637]\n",
      "64 [D loss: 0.666180, acc.: 55.56%] [G loss: 1.925485]\n",
      "65 [D loss: 0.921932, acc.: 38.89%] [G loss: 1.221256]\n",
      "66 [D loss: 0.886112, acc.: 44.44%] [G loss: 0.916665]\n",
      "67 [D loss: 0.517309, acc.: 72.22%] [G loss: 1.159319]\n",
      "68 [D loss: 0.581960, acc.: 66.67%] [G loss: 1.489377]\n",
      "69 [D loss: 0.800706, acc.: 44.44%] [G loss: 1.495114]\n",
      "70 [D loss: 0.830162, acc.: 55.56%] [G loss: 1.454774]\n",
      "71 [D loss: 1.002145, acc.: 44.44%] [G loss: 1.307698]\n",
      "72 [D loss: 0.960583, acc.: 33.33%] [G loss: 1.520441]\n",
      "73 [D loss: 0.639250, acc.: 61.11%] [G loss: 1.561850]\n",
      "74 [D loss: 0.845917, acc.: 38.89%] [G loss: 1.993572]\n",
      "75 [D loss: 1.163975, acc.: 33.33%] [G loss: 1.486656]\n",
      "76 [D loss: 2.155689, acc.: 16.67%] [G loss: 2.662744]\n",
      "77 [D loss: 0.952913, acc.: 61.11%] [G loss: 1.577142]\n",
      "78 [D loss: 1.019816, acc.: 44.44%] [G loss: 1.599291]\n",
      "79 [D loss: 0.600042, acc.: 72.22%] [G loss: 2.217742]\n",
      "80 [D loss: 0.290057, acc.: 88.89%] [G loss: 2.419296]\n",
      "81 [D loss: 0.292567, acc.: 100.00%] [G loss: 1.631698]\n",
      "82 [D loss: 0.599449, acc.: 72.22%] [G loss: 1.433324]\n",
      "83 [D loss: 0.274872, acc.: 88.89%] [G loss: 2.554651]\n",
      "84 [D loss: 0.582320, acc.: 61.11%] [G loss: 1.462158]\n",
      "85 [D loss: 0.490240, acc.: 77.78%] [G loss: 2.242065]\n",
      "86 [D loss: 0.556686, acc.: 83.33%] [G loss: 2.091602]\n",
      "87 [D loss: 0.596638, acc.: 72.22%] [G loss: 2.200044]\n",
      "88 [D loss: 0.447174, acc.: 77.78%] [G loss: 2.528579]\n",
      "89 [D loss: 0.328622, acc.: 88.89%] [G loss: 2.563601]\n",
      "90 [D loss: 0.382624, acc.: 83.33%] [G loss: 1.982646]\n",
      "91 [D loss: 0.455583, acc.: 77.78%] [G loss: 2.788592]\n",
      "92 [D loss: 0.524993, acc.: 77.78%] [G loss: 2.835383]\n",
      "93 [D loss: 0.265521, acc.: 88.89%] [G loss: 2.274714]\n",
      "94 [D loss: 0.201519, acc.: 94.44%] [G loss: 2.699065]\n",
      "95 [D loss: 0.240176, acc.: 88.89%] [G loss: 2.383174]\n",
      "96 [D loss: 0.284954, acc.: 83.33%] [G loss: 2.055701]\n",
      "97 [D loss: 0.415219, acc.: 83.33%] [G loss: 2.563429]\n",
      "98 [D loss: 0.236949, acc.: 94.44%] [G loss: 2.401027]\n",
      "99 [D loss: 0.305464, acc.: 83.33%] [G loss: 2.163300]\n",
      "100 [D loss: 0.277896, acc.: 88.89%] [G loss: 1.392252]\n",
      "101 [D loss: 0.506784, acc.: 72.22%] [G loss: 1.918346]\n",
      "102 [D loss: 0.484015, acc.: 66.67%] [G loss: 2.651813]\n",
      "103 [D loss: 0.581519, acc.: 61.11%] [G loss: 1.897517]\n",
      "104 [D loss: 0.362167, acc.: 77.78%] [G loss: 1.736256]\n",
      "105 [D loss: 0.738398, acc.: 61.11%] [G loss: 2.287849]\n",
      "106 [D loss: 0.912946, acc.: 55.56%] [G loss: 2.309730]\n",
      "107 [D loss: 0.878194, acc.: 44.44%] [G loss: 1.346440]\n",
      "108 [D loss: 1.244245, acc.: 33.33%] [G loss: 1.373713]\n",
      "109 [D loss: 1.296827, acc.: 44.44%] [G loss: 1.405654]\n",
      "110 [D loss: 0.878120, acc.: 61.11%] [G loss: 1.610336]\n",
      "111 [D loss: 0.687244, acc.: 72.22%] [G loss: 1.097937]\n",
      "112 [D loss: 0.842636, acc.: 55.56%] [G loss: 1.439366]\n",
      "113 [D loss: 1.229100, acc.: 22.22%] [G loss: 1.386752]\n",
      "114 [D loss: 0.911550, acc.: 55.56%] [G loss: 1.108666]\n",
      "115 [D loss: 1.212836, acc.: 33.33%] [G loss: 1.281375]\n",
      "116 [D loss: 0.804803, acc.: 44.44%] [G loss: 0.946428]\n",
      "117 [D loss: 0.973700, acc.: 38.89%] [G loss: 0.534276]\n",
      "118 [D loss: 1.046676, acc.: 38.89%] [G loss: 2.220698]\n",
      "119 [D loss: 0.834968, acc.: 55.56%] [G loss: 1.704707]\n",
      "120 [D loss: 0.660691, acc.: 61.11%] [G loss: 1.426894]\n",
      "121 [D loss: 0.662080, acc.: 61.11%] [G loss: 1.364369]\n",
      "122 [D loss: 0.801025, acc.: 55.56%] [G loss: 1.154360]\n",
      "123 [D loss: 0.680852, acc.: 61.11%] [G loss: 1.133878]\n",
      "124 [D loss: 0.738914, acc.: 61.11%] [G loss: 1.371959]\n",
      "125 [D loss: 0.738241, acc.: 61.11%] [G loss: 1.274028]\n",
      "126 [D loss: 0.740453, acc.: 50.00%] [G loss: 1.826860]\n",
      "127 [D loss: 0.741133, acc.: 66.67%] [G loss: 0.928051]\n",
      "128 [D loss: 0.781784, acc.: 50.00%] [G loss: 1.448155]\n",
      "129 [D loss: 0.927600, acc.: 33.33%] [G loss: 1.345758]\n",
      "130 [D loss: 1.080831, acc.: 38.89%] [G loss: 1.407121]\n",
      "131 [D loss: 0.567734, acc.: 66.67%] [G loss: 2.110733]\n",
      "132 [D loss: 1.024858, acc.: 44.44%] [G loss: 1.163552]\n",
      "133 [D loss: 0.688951, acc.: 61.11%] [G loss: 1.462052]\n",
      "134 [D loss: 0.665360, acc.: 61.11%] [G loss: 1.105585]\n",
      "135 [D loss: 1.153996, acc.: 50.00%] [G loss: 1.445469]\n",
      "136 [D loss: 0.499304, acc.: 66.67%] [G loss: 1.293108]\n",
      "137 [D loss: 0.478345, acc.: 66.67%] [G loss: 2.305498]\n",
      "138 [D loss: 0.774298, acc.: 61.11%] [G loss: 1.620243]\n",
      "139 [D loss: 0.854012, acc.: 55.56%] [G loss: 1.211900]\n",
      "140 [D loss: 0.778690, acc.: 55.56%] [G loss: 1.268226]\n",
      "141 [D loss: 0.474555, acc.: 77.78%] [G loss: 1.868456]\n",
      "142 [D loss: 0.649596, acc.: 61.11%] [G loss: 1.283961]\n",
      "143 [D loss: 0.484197, acc.: 77.78%] [G loss: 1.603849]\n",
      "144 [D loss: 0.684402, acc.: 72.22%] [G loss: 1.366167]\n",
      "145 [D loss: 0.912615, acc.: 44.44%] [G loss: 1.338702]\n",
      "146 [D loss: 0.687570, acc.: 66.67%] [G loss: 2.068757]\n",
      "147 [D loss: 0.804891, acc.: 50.00%] [G loss: 1.449369]\n",
      "148 [D loss: 1.019079, acc.: 44.44%] [G loss: 1.191556]\n",
      "149 [D loss: 1.636050, acc.: 16.67%] [G loss: 1.344321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 [D loss: 0.630955, acc.: 72.22%] [G loss: 1.784110]\n",
      "151 [D loss: 1.283080, acc.: 33.33%] [G loss: 0.903662]\n",
      "152 [D loss: 0.844857, acc.: 61.11%] [G loss: 1.254593]\n",
      "153 [D loss: 0.987293, acc.: 38.89%] [G loss: 1.435702]\n",
      "154 [D loss: 1.586902, acc.: 16.67%] [G loss: 1.320295]\n",
      "155 [D loss: 1.142019, acc.: 44.44%] [G loss: 1.388076]\n",
      "156 [D loss: 0.979388, acc.: 50.00%] [G loss: 1.866286]\n",
      "157 [D loss: 0.436321, acc.: 83.33%] [G loss: 1.939749]\n",
      "158 [D loss: 0.726567, acc.: 61.11%] [G loss: 1.775325]\n",
      "159 [D loss: 0.300394, acc.: 88.89%] [G loss: 2.262726]\n",
      "160 [D loss: 0.505001, acc.: 77.78%] [G loss: 1.660451]\n",
      "161 [D loss: 0.507514, acc.: 66.67%] [G loss: 1.948905]\n",
      "162 [D loss: 0.566872, acc.: 66.67%] [G loss: 1.753202]\n",
      "163 [D loss: 0.882512, acc.: 55.56%] [G loss: 2.350121]\n",
      "164 [D loss: 0.556288, acc.: 61.11%] [G loss: 1.038687]\n",
      "165 [D loss: 0.567698, acc.: 72.22%] [G loss: 1.661387]\n",
      "166 [D loss: 1.163669, acc.: 27.78%] [G loss: 0.897035]\n",
      "167 [D loss: 0.929224, acc.: 44.44%] [G loss: 1.569240]\n",
      "168 [D loss: 1.150536, acc.: 33.33%] [G loss: 1.003126]\n",
      "169 [D loss: 0.943497, acc.: 55.56%] [G loss: 1.618934]\n",
      "170 [D loss: 1.201699, acc.: 44.44%] [G loss: 1.488726]\n",
      "171 [D loss: 0.960929, acc.: 27.78%] [G loss: 1.345108]\n",
      "172 [D loss: 0.765212, acc.: 61.11%] [G loss: 2.491222]\n",
      "173 [D loss: 0.690711, acc.: 61.11%] [G loss: 1.596159]\n",
      "174 [D loss: 1.188532, acc.: 27.78%] [G loss: 1.902764]\n",
      "175 [D loss: 0.973612, acc.: 55.56%] [G loss: 1.487274]\n",
      "176 [D loss: 0.585136, acc.: 66.67%] [G loss: 1.869298]\n",
      "177 [D loss: 0.809204, acc.: 44.44%] [G loss: 1.338767]\n",
      "178 [D loss: 1.119076, acc.: 38.89%] [G loss: 0.937317]\n",
      "179 [D loss: 0.721493, acc.: 55.56%] [G loss: 2.041254]\n",
      "180 [D loss: 0.673754, acc.: 61.11%] [G loss: 2.066308]\n",
      "181 [D loss: 0.551593, acc.: 66.67%] [G loss: 2.005791]\n",
      "182 [D loss: 1.121260, acc.: 55.56%] [G loss: 1.747100]\n",
      "183 [D loss: 0.960518, acc.: 50.00%] [G loss: 1.312024]\n",
      "184 [D loss: 0.891553, acc.: 55.56%] [G loss: 1.408465]\n",
      "185 [D loss: 1.082941, acc.: 61.11%] [G loss: 1.899017]\n",
      "186 [D loss: 0.807379, acc.: 55.56%] [G loss: 0.792871]\n",
      "187 [D loss: 0.552528, acc.: 72.22%] [G loss: 1.274576]\n",
      "188 [D loss: 0.735273, acc.: 61.11%] [G loss: 0.958699]\n",
      "189 [D loss: 0.588657, acc.: 61.11%] [G loss: 0.968106]\n",
      "190 [D loss: 0.887800, acc.: 50.00%] [G loss: 1.545824]\n",
      "191 [D loss: 1.129165, acc.: 33.33%] [G loss: 1.174904]\n",
      "192 [D loss: 0.823482, acc.: 55.56%] [G loss: 1.396662]\n",
      "193 [D loss: 0.970556, acc.: 44.44%] [G loss: 1.656072]\n",
      "194 [D loss: 0.643045, acc.: 55.56%] [G loss: 1.745558]\n",
      "195 [D loss: 0.541180, acc.: 83.33%] [G loss: 1.008863]\n",
      "196 [D loss: 0.819213, acc.: 61.11%] [G loss: 1.857799]\n",
      "197 [D loss: 0.723019, acc.: 61.11%] [G loss: 1.772031]\n",
      "198 [D loss: 0.952824, acc.: 38.89%] [G loss: 1.502044]\n",
      "199 [D loss: 0.596266, acc.: 72.22%] [G loss: 1.200012]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 [D loss: 0.942092, acc.: 44.44%] [G loss: 1.396809]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 [D loss: 0.686358, acc.: 55.56%] [G loss: 1.634760]\n",
      "202 [D loss: 0.748797, acc.: 55.56%] [G loss: 1.312807]\n",
      "203 [D loss: 1.034052, acc.: 33.33%] [G loss: 1.347615]\n",
      "204 [D loss: 0.630507, acc.: 61.11%] [G loss: 1.555461]\n",
      "205 [D loss: 1.073143, acc.: 38.89%] [G loss: 1.414731]\n",
      "206 [D loss: 0.884609, acc.: 44.44%] [G loss: 1.512297]\n",
      "207 [D loss: 0.724042, acc.: 61.11%] [G loss: 1.539112]\n",
      "208 [D loss: 0.751601, acc.: 55.56%] [G loss: 0.708626]\n",
      "209 [D loss: 0.932647, acc.: 38.89%] [G loss: 1.747537]\n",
      "210 [D loss: 0.673189, acc.: 72.22%] [G loss: 1.709127]\n",
      "211 [D loss: 1.125521, acc.: 38.89%] [G loss: 1.386301]\n",
      "212 [D loss: 0.887292, acc.: 33.33%] [G loss: 1.698681]\n",
      "213 [D loss: 1.359412, acc.: 33.33%] [G loss: 1.338844]\n",
      "214 [D loss: 0.729939, acc.: 66.67%] [G loss: 1.054280]\n",
      "215 [D loss: 1.052172, acc.: 50.00%] [G loss: 1.015130]\n",
      "216 [D loss: 0.545468, acc.: 72.22%] [G loss: 1.329756]\n",
      "217 [D loss: 0.884384, acc.: 38.89%] [G loss: 1.781622]\n",
      "218 [D loss: 0.940266, acc.: 44.44%] [G loss: 1.489164]\n",
      "219 [D loss: 0.888479, acc.: 33.33%] [G loss: 1.463941]\n",
      "220 [D loss: 1.141866, acc.: 38.89%] [G loss: 1.530483]\n",
      "221 [D loss: 0.994112, acc.: 44.44%] [G loss: 0.916379]\n",
      "222 [D loss: 0.585147, acc.: 66.67%] [G loss: 1.466408]\n",
      "223 [D loss: 0.753676, acc.: 61.11%] [G loss: 1.888773]\n",
      "224 [D loss: 0.429501, acc.: 77.78%] [G loss: 1.837946]\n",
      "225 [D loss: 1.224822, acc.: 55.56%] [G loss: 1.698622]\n",
      "226 [D loss: 0.916894, acc.: 55.56%] [G loss: 1.397815]\n",
      "227 [D loss: 0.706078, acc.: 66.67%] [G loss: 2.436743]\n",
      "228 [D loss: 0.677445, acc.: 72.22%] [G loss: 2.026259]\n",
      "229 [D loss: 0.702215, acc.: 61.11%] [G loss: 1.500799]\n",
      "230 [D loss: 0.440676, acc.: 77.78%] [G loss: 1.676910]\n",
      "231 [D loss: 0.246007, acc.: 100.00%] [G loss: 2.334682]\n",
      "232 [D loss: 0.545911, acc.: 77.78%] [G loss: 2.135977]\n",
      "233 [D loss: 0.863984, acc.: 50.00%] [G loss: 1.172880]\n",
      "234 [D loss: 0.759890, acc.: 55.56%] [G loss: 1.429372]\n",
      "235 [D loss: 1.310292, acc.: 38.89%] [G loss: 1.283855]\n",
      "236 [D loss: 0.807213, acc.: 61.11%] [G loss: 2.152898]\n",
      "237 [D loss: 0.895800, acc.: 61.11%] [G loss: 2.066891]\n",
      "238 [D loss: 0.804964, acc.: 55.56%] [G loss: 2.048531]\n",
      "239 [D loss: 0.750917, acc.: 66.67%] [G loss: 1.383758]\n",
      "240 [D loss: 0.575469, acc.: 72.22%] [G loss: 1.358024]\n",
      "241 [D loss: 0.852475, acc.: 44.44%] [G loss: 1.231436]\n",
      "242 [D loss: 0.860651, acc.: 44.44%] [G loss: 1.450594]\n",
      "243 [D loss: 0.862182, acc.: 55.56%] [G loss: 1.234467]\n",
      "244 [D loss: 0.838404, acc.: 50.00%] [G loss: 1.538649]\n",
      "245 [D loss: 0.492160, acc.: 88.89%] [G loss: 2.224716]\n",
      "246 [D loss: 0.819967, acc.: 55.56%] [G loss: 1.411088]\n",
      "247 [D loss: 0.620357, acc.: 61.11%] [G loss: 2.124864]\n",
      "248 [D loss: 0.463835, acc.: 83.33%] [G loss: 1.707735]\n",
      "249 [D loss: 0.686129, acc.: 61.11%] [G loss: 1.526499]\n",
      "250 [D loss: 0.435975, acc.: 88.89%] [G loss: 1.669738]\n",
      "251 [D loss: 0.936957, acc.: 55.56%] [G loss: 1.904684]\n",
      "252 [D loss: 1.038605, acc.: 55.56%] [G loss: 2.140692]\n",
      "253 [D loss: 0.368726, acc.: 88.89%] [G loss: 1.522019]\n",
      "254 [D loss: 0.787450, acc.: 55.56%] [G loss: 1.791846]\n",
      "255 [D loss: 0.585210, acc.: 72.22%] [G loss: 1.636070]\n",
      "256 [D loss: 0.857590, acc.: 50.00%] [G loss: 1.272944]\n",
      "257 [D loss: 0.787226, acc.: 61.11%] [G loss: 1.236452]\n",
      "258 [D loss: 1.002688, acc.: 61.11%] [G loss: 1.533085]\n",
      "259 [D loss: 1.236928, acc.: 50.00%] [G loss: 1.686356]\n",
      "260 [D loss: 1.288093, acc.: 33.33%] [G loss: 1.103328]\n",
      "261 [D loss: 0.439902, acc.: 83.33%] [G loss: 1.369333]\n",
      "262 [D loss: 0.607000, acc.: 61.11%] [G loss: 2.186502]\n",
      "263 [D loss: 1.207642, acc.: 38.89%] [G loss: 1.523698]\n",
      "264 [D loss: 0.749066, acc.: 61.11%] [G loss: 1.775148]\n",
      "265 [D loss: 0.999651, acc.: 38.89%] [G loss: 1.102986]\n",
      "266 [D loss: 0.748854, acc.: 66.67%] [G loss: 1.496044]\n",
      "267 [D loss: 0.702997, acc.: 66.67%] [G loss: 1.896997]\n",
      "268 [D loss: 1.143654, acc.: 44.44%] [G loss: 1.612918]\n",
      "269 [D loss: 0.802775, acc.: 77.78%] [G loss: 1.960475]\n",
      "270 [D loss: 0.530260, acc.: 72.22%] [G loss: 1.299726]\n",
      "271 [D loss: 1.571183, acc.: 22.22%] [G loss: 0.897032]\n",
      "272 [D loss: 0.826903, acc.: 55.56%] [G loss: 1.521183]\n",
      "273 [D loss: 0.879571, acc.: 38.89%] [G loss: 1.869640]\n",
      "274 [D loss: 0.621120, acc.: 66.67%] [G loss: 2.086938]\n",
      "275 [D loss: 0.695285, acc.: 72.22%] [G loss: 0.986226]\n",
      "276 [D loss: 0.659847, acc.: 66.67%] [G loss: 1.231205]\n",
      "277 [D loss: 0.365015, acc.: 88.89%] [G loss: 1.866859]\n",
      "278 [D loss: 0.837100, acc.: 55.56%] [G loss: 1.763262]\n",
      "279 [D loss: 0.567193, acc.: 72.22%] [G loss: 1.503882]\n",
      "280 [D loss: 1.136688, acc.: 44.44%] [G loss: 0.847606]\n",
      "281 [D loss: 1.210197, acc.: 50.00%] [G loss: 1.384894]\n",
      "282 [D loss: 1.484642, acc.: 33.33%] [G loss: 1.611255]\n",
      "283 [D loss: 0.795383, acc.: 38.89%] [G loss: 0.966085]\n",
      "284 [D loss: 1.096794, acc.: 38.89%] [G loss: 2.665009]\n",
      "285 [D loss: 1.443648, acc.: 33.33%] [G loss: 1.776011]\n",
      "286 [D loss: 0.616243, acc.: 61.11%] [G loss: 1.958582]\n",
      "287 [D loss: 0.863597, acc.: 61.11%] [G loss: 1.339294]\n",
      "288 [D loss: 1.096008, acc.: 66.67%] [G loss: 2.302621]\n",
      "289 [D loss: 0.933479, acc.: 50.00%] [G loss: 1.015086]\n",
      "290 [D loss: 1.052030, acc.: 33.33%] [G loss: 1.949277]\n",
      "291 [D loss: 1.281487, acc.: 50.00%] [G loss: 1.624033]\n",
      "292 [D loss: 1.004192, acc.: 50.00%] [G loss: 1.000082]\n",
      "293 [D loss: 1.146434, acc.: 38.89%] [G loss: 1.493995]\n",
      "294 [D loss: 1.131415, acc.: 38.89%] [G loss: 2.416405]\n",
      "295 [D loss: 1.078145, acc.: 38.89%] [G loss: 1.342052]\n",
      "296 [D loss: 1.370673, acc.: 44.44%] [G loss: 1.315771]\n",
      "297 [D loss: 0.980827, acc.: 61.11%] [G loss: 1.088236]\n",
      "298 [D loss: 0.991334, acc.: 38.89%] [G loss: 1.643519]\n",
      "299 [D loss: 0.818083, acc.: 55.56%] [G loss: 1.339467]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 [D loss: 1.169963, acc.: 44.44%] [G loss: 1.288340]\n",
      "301 [D loss: 0.649763, acc.: 61.11%] [G loss: 1.412664]\n",
      "302 [D loss: 1.193640, acc.: 22.22%] [G loss: 1.442273]\n",
      "303 [D loss: 1.283639, acc.: 22.22%] [G loss: 1.906873]\n",
      "304 [D loss: 0.472205, acc.: 77.78%] [G loss: 1.343623]\n",
      "305 [D loss: 0.881243, acc.: 50.00%] [G loss: 1.747579]\n",
      "306 [D loss: 0.932226, acc.: 44.44%] [G loss: 1.548244]\n",
      "307 [D loss: 1.251115, acc.: 44.44%] [G loss: 1.374919]\n",
      "308 [D loss: 1.042493, acc.: 50.00%] [G loss: 1.316917]\n",
      "309 [D loss: 1.105965, acc.: 38.89%] [G loss: 1.526037]\n",
      "310 [D loss: 1.326330, acc.: 27.78%] [G loss: 1.626594]\n",
      "311 [D loss: 0.990162, acc.: 44.44%] [G loss: 1.401615]\n",
      "312 [D loss: 0.766826, acc.: 61.11%] [G loss: 1.149340]\n",
      "313 [D loss: 0.998977, acc.: 50.00%] [G loss: 1.301237]\n",
      "314 [D loss: 1.422867, acc.: 38.89%] [G loss: 1.702192]\n",
      "315 [D loss: 0.727991, acc.: 61.11%] [G loss: 1.110166]\n",
      "316 [D loss: 0.865222, acc.: 66.67%] [G loss: 1.949698]\n",
      "317 [D loss: 1.400602, acc.: 16.67%] [G loss: 1.736454]\n",
      "318 [D loss: 0.865851, acc.: 50.00%] [G loss: 1.015193]\n",
      "319 [D loss: 1.029997, acc.: 38.89%] [G loss: 1.437019]\n",
      "320 [D loss: 1.043016, acc.: 38.89%] [G loss: 1.980984]\n",
      "321 [D loss: 1.283110, acc.: 61.11%] [G loss: 1.899794]\n",
      "322 [D loss: 1.337190, acc.: 38.89%] [G loss: 1.017231]\n",
      "323 [D loss: 1.162547, acc.: 38.89%] [G loss: 1.549508]\n",
      "324 [D loss: 0.865263, acc.: 44.44%] [G loss: 1.341424]\n",
      "325 [D loss: 0.956505, acc.: 55.56%] [G loss: 0.828967]\n",
      "326 [D loss: 1.344234, acc.: 16.67%] [G loss: 1.527092]\n",
      "327 [D loss: 0.953015, acc.: 50.00%] [G loss: 1.698138]\n",
      "328 [D loss: 0.596277, acc.: 72.22%] [G loss: 1.317179]\n",
      "329 [D loss: 1.074764, acc.: 27.78%] [G loss: 1.158572]\n",
      "330 [D loss: 0.588136, acc.: 66.67%] [G loss: 1.377295]\n",
      "331 [D loss: 0.829108, acc.: 44.44%] [G loss: 1.163700]\n",
      "332 [D loss: 1.271633, acc.: 33.33%] [G loss: 1.337843]\n",
      "333 [D loss: 0.818309, acc.: 50.00%] [G loss: 1.823298]\n",
      "334 [D loss: 0.882843, acc.: 44.44%] [G loss: 1.619769]\n",
      "335 [D loss: 0.931195, acc.: 50.00%] [G loss: 1.216331]\n",
      "336 [D loss: 1.329577, acc.: 38.89%] [G loss: 1.425899]\n",
      "337 [D loss: 0.694378, acc.: 55.56%] [G loss: 1.534899]\n",
      "338 [D loss: 0.804159, acc.: 50.00%] [G loss: 1.197205]\n",
      "339 [D loss: 0.637660, acc.: 66.67%] [G loss: 1.568746]\n",
      "340 [D loss: 0.951667, acc.: 50.00%] [G loss: 1.469598]\n",
      "341 [D loss: 0.795150, acc.: 61.11%] [G loss: 1.006995]\n",
      "342 [D loss: 0.858565, acc.: 50.00%] [G loss: 1.157144]\n",
      "343 [D loss: 1.070201, acc.: 33.33%] [G loss: 1.498446]\n",
      "344 [D loss: 1.053130, acc.: 55.56%] [G loss: 1.976785]\n",
      "345 [D loss: 1.451131, acc.: 33.33%] [G loss: 0.888908]\n",
      "346 [D loss: 1.126909, acc.: 33.33%] [G loss: 1.116225]\n",
      "347 [D loss: 0.954939, acc.: 50.00%] [G loss: 1.348139]\n",
      "348 [D loss: 1.243257, acc.: 16.67%] [G loss: 1.061924]\n",
      "349 [D loss: 1.107077, acc.: 38.89%] [G loss: 1.011678]\n",
      "350 [D loss: 0.979821, acc.: 55.56%] [G loss: 0.916792]\n",
      "351 [D loss: 1.008524, acc.: 27.78%] [G loss: 1.049034]\n",
      "352 [D loss: 0.993658, acc.: 50.00%] [G loss: 1.528562]\n",
      "353 [D loss: 1.477775, acc.: 33.33%] [G loss: 1.442668]\n",
      "354 [D loss: 1.345478, acc.: 27.78%] [G loss: 1.625877]\n",
      "355 [D loss: 1.066071, acc.: 44.44%] [G loss: 2.177823]\n",
      "356 [D loss: 1.312965, acc.: 38.89%] [G loss: 1.100000]\n",
      "357 [D loss: 1.532632, acc.: 33.33%] [G loss: 1.519889]\n",
      "358 [D loss: 0.856503, acc.: 38.89%] [G loss: 1.667257]\n",
      "359 [D loss: 0.781125, acc.: 55.56%] [G loss: 1.223258]\n",
      "360 [D loss: 1.159460, acc.: 55.56%] [G loss: 1.096964]\n",
      "361 [D loss: 0.900021, acc.: 61.11%] [G loss: 1.054691]\n",
      "362 [D loss: 0.842789, acc.: 61.11%] [G loss: 1.276569]\n",
      "363 [D loss: 1.085270, acc.: 55.56%] [G loss: 0.901289]\n",
      "364 [D loss: 0.890974, acc.: 50.00%] [G loss: 1.199501]\n",
      "365 [D loss: 0.977618, acc.: 33.33%] [G loss: 1.910780]\n",
      "366 [D loss: 0.675170, acc.: 66.67%] [G loss: 1.962512]\n",
      "367 [D loss: 0.935288, acc.: 50.00%] [G loss: 1.585953]\n",
      "368 [D loss: 0.919951, acc.: 44.44%] [G loss: 1.502775]\n",
      "369 [D loss: 1.239038, acc.: 38.89%] [G loss: 0.709246]\n",
      "370 [D loss: 0.853752, acc.: 66.67%] [G loss: 0.892401]\n",
      "371 [D loss: 1.073479, acc.: 44.44%] [G loss: 1.148332]\n",
      "372 [D loss: 0.682592, acc.: 66.67%] [G loss: 1.629522]\n",
      "373 [D loss: 0.870515, acc.: 38.89%] [G loss: 1.136735]\n",
      "374 [D loss: 0.869312, acc.: 55.56%] [G loss: 1.012737]\n",
      "375 [D loss: 1.141468, acc.: 33.33%] [G loss: 1.366176]\n",
      "376 [D loss: 0.951102, acc.: 44.44%] [G loss: 1.039298]\n",
      "377 [D loss: 1.605733, acc.: 38.89%] [G loss: 1.095604]\n",
      "378 [D loss: 0.748822, acc.: 72.22%] [G loss: 1.136104]\n",
      "379 [D loss: 1.293389, acc.: 27.78%] [G loss: 1.359298]\n",
      "380 [D loss: 1.670370, acc.: 16.67%] [G loss: 1.337807]\n",
      "381 [D loss: 0.924446, acc.: 55.56%] [G loss: 1.385173]\n",
      "382 [D loss: 0.796209, acc.: 55.56%] [G loss: 1.082644]\n",
      "383 [D loss: 1.239741, acc.: 33.33%] [G loss: 0.963201]\n",
      "384 [D loss: 0.840131, acc.: 61.11%] [G loss: 1.696102]\n",
      "385 [D loss: 0.971379, acc.: 38.89%] [G loss: 1.854747]\n",
      "386 [D loss: 0.953001, acc.: 55.56%] [G loss: 1.324576]\n",
      "387 [D loss: 1.261551, acc.: 27.78%] [G loss: 0.907828]\n",
      "388 [D loss: 0.960042, acc.: 44.44%] [G loss: 1.052429]\n",
      "389 [D loss: 0.859515, acc.: 61.11%] [G loss: 1.107316]\n",
      "390 [D loss: 0.738534, acc.: 50.00%] [G loss: 1.184820]\n",
      "391 [D loss: 1.036785, acc.: 27.78%] [G loss: 1.253971]\n",
      "392 [D loss: 0.741783, acc.: 66.67%] [G loss: 1.993951]\n",
      "393 [D loss: 1.422898, acc.: 27.78%] [G loss: 1.159324]\n",
      "394 [D loss: 0.782897, acc.: 50.00%] [G loss: 1.320999]\n",
      "395 [D loss: 1.159946, acc.: 38.89%] [G loss: 1.196749]\n",
      "396 [D loss: 0.977531, acc.: 50.00%] [G loss: 1.240927]\n",
      "397 [D loss: 1.061856, acc.: 33.33%] [G loss: 1.543393]\n",
      "398 [D loss: 1.035472, acc.: 27.78%] [G loss: 1.555459]\n",
      "399 [D loss: 0.610066, acc.: 66.67%] [G loss: 1.516350]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 [D loss: 0.638339, acc.: 61.11%] [G loss: 0.723745]\n",
      "401 [D loss: 1.232643, acc.: 44.44%] [G loss: 1.050974]\n",
      "402 [D loss: 1.107298, acc.: 44.44%] [G loss: 0.756312]\n",
      "403 [D loss: 1.178254, acc.: 33.33%] [G loss: 0.805878]\n",
      "404 [D loss: 1.009630, acc.: 44.44%] [G loss: 1.219849]\n",
      "405 [D loss: 1.073827, acc.: 33.33%] [G loss: 1.211856]\n",
      "406 [D loss: 0.831634, acc.: 55.56%] [G loss: 1.474864]\n",
      "407 [D loss: 1.290215, acc.: 33.33%] [G loss: 1.646813]\n",
      "408 [D loss: 0.843226, acc.: 61.11%] [G loss: 1.279285]\n",
      "409 [D loss: 0.954875, acc.: 44.44%] [G loss: 1.251833]\n",
      "410 [D loss: 1.370659, acc.: 33.33%] [G loss: 1.179391]\n",
      "411 [D loss: 0.867758, acc.: 33.33%] [G loss: 1.204372]\n",
      "412 [D loss: 1.170553, acc.: 27.78%] [G loss: 1.259020]\n",
      "413 [D loss: 1.020565, acc.: 44.44%] [G loss: 1.038279]\n",
      "414 [D loss: 1.108070, acc.: 44.44%] [G loss: 0.782740]\n",
      "415 [D loss: 1.387805, acc.: 38.89%] [G loss: 0.825856]\n",
      "416 [D loss: 1.078765, acc.: 33.33%] [G loss: 1.057399]\n",
      "417 [D loss: 0.717349, acc.: 50.00%] [G loss: 1.112898]\n",
      "418 [D loss: 0.850335, acc.: 50.00%] [G loss: 0.947264]\n",
      "419 [D loss: 1.385731, acc.: 22.22%] [G loss: 0.518868]\n",
      "420 [D loss: 1.285710, acc.: 38.89%] [G loss: 0.472066]\n",
      "421 [D loss: 0.869590, acc.: 44.44%] [G loss: 1.618922]\n",
      "422 [D loss: 0.936740, acc.: 55.56%] [G loss: 1.693242]\n",
      "423 [D loss: 1.209246, acc.: 27.78%] [G loss: 1.628168]\n",
      "424 [D loss: 0.816252, acc.: 50.00%] [G loss: 1.523091]\n",
      "425 [D loss: 0.881378, acc.: 61.11%] [G loss: 1.383301]\n",
      "426 [D loss: 1.063473, acc.: 38.89%] [G loss: 1.399075]\n",
      "427 [D loss: 1.002746, acc.: 38.89%] [G loss: 1.074742]\n",
      "428 [D loss: 1.071731, acc.: 44.44%] [G loss: 1.259832]\n",
      "429 [D loss: 1.120025, acc.: 33.33%] [G loss: 1.507710]\n",
      "430 [D loss: 1.096938, acc.: 38.89%] [G loss: 1.325818]\n",
      "431 [D loss: 0.910588, acc.: 55.56%] [G loss: 1.912546]\n",
      "432 [D loss: 0.715182, acc.: 50.00%] [G loss: 0.941917]\n",
      "433 [D loss: 0.735627, acc.: 61.11%] [G loss: 1.476166]\n",
      "434 [D loss: 1.284454, acc.: 27.78%] [G loss: 1.083687]\n",
      "435 [D loss: 0.723893, acc.: 50.00%] [G loss: 1.366181]\n",
      "436 [D loss: 1.096370, acc.: 38.89%] [G loss: 1.293804]\n",
      "437 [D loss: 0.892380, acc.: 50.00%] [G loss: 0.646766]\n",
      "438 [D loss: 0.818205, acc.: 55.56%] [G loss: 2.005014]\n",
      "439 [D loss: 1.089464, acc.: 55.56%] [G loss: 1.673077]\n",
      "440 [D loss: 0.807942, acc.: 61.11%] [G loss: 1.056834]\n",
      "441 [D loss: 1.026298, acc.: 50.00%] [G loss: 0.756478]\n",
      "442 [D loss: 0.960590, acc.: 38.89%] [G loss: 0.693500]\n",
      "443 [D loss: 1.056663, acc.: 50.00%] [G loss: 1.346942]\n",
      "444 [D loss: 0.948823, acc.: 50.00%] [G loss: 1.222494]\n",
      "445 [D loss: 0.703624, acc.: 61.11%] [G loss: 0.772151]\n",
      "446 [D loss: 1.266956, acc.: 38.89%] [G loss: 0.709930]\n",
      "447 [D loss: 0.815882, acc.: 44.44%] [G loss: 1.049182]\n",
      "448 [D loss: 1.132471, acc.: 50.00%] [G loss: 0.927434]\n",
      "449 [D loss: 0.804732, acc.: 55.56%] [G loss: 1.394959]\n",
      "450 [D loss: 1.349433, acc.: 44.44%] [G loss: 1.381019]\n",
      "451 [D loss: 0.783732, acc.: 44.44%] [G loss: 1.554757]\n",
      "452 [D loss: 0.779241, acc.: 55.56%] [G loss: 1.340166]\n",
      "453 [D loss: 1.112116, acc.: 27.78%] [G loss: 1.177035]\n",
      "454 [D loss: 0.861539, acc.: 55.56%] [G loss: 1.428723]\n",
      "455 [D loss: 1.228365, acc.: 38.89%] [G loss: 1.824367]\n",
      "456 [D loss: 0.940752, acc.: 50.00%] [G loss: 1.351489]\n",
      "457 [D loss: 1.090703, acc.: 44.44%] [G loss: 1.666484]\n",
      "458 [D loss: 0.723865, acc.: 61.11%] [G loss: 1.388545]\n",
      "459 [D loss: 0.722278, acc.: 50.00%] [G loss: 1.178499]\n",
      "460 [D loss: 0.841751, acc.: 38.89%] [G loss: 1.510484]\n",
      "461 [D loss: 1.235390, acc.: 27.78%] [G loss: 1.101329]\n",
      "462 [D loss: 0.824241, acc.: 55.56%] [G loss: 1.546590]\n",
      "463 [D loss: 0.881228, acc.: 50.00%] [G loss: 0.815340]\n",
      "464 [D loss: 0.999138, acc.: 44.44%] [G loss: 1.649978]\n",
      "465 [D loss: 0.950639, acc.: 55.56%] [G loss: 0.911761]\n",
      "466 [D loss: 0.598027, acc.: 61.11%] [G loss: 1.060302]\n",
      "467 [D loss: 0.872680, acc.: 66.67%] [G loss: 0.521610]\n",
      "468 [D loss: 1.297025, acc.: 44.44%] [G loss: 1.223109]\n",
      "469 [D loss: 1.156924, acc.: 38.89%] [G loss: 0.877980]\n",
      "470 [D loss: 0.680445, acc.: 55.56%] [G loss: 1.546722]\n",
      "471 [D loss: 0.830921, acc.: 55.56%] [G loss: 1.212313]\n",
      "472 [D loss: 1.046280, acc.: 27.78%] [G loss: 0.834931]\n",
      "473 [D loss: 0.828186, acc.: 50.00%] [G loss: 1.016525]\n",
      "474 [D loss: 1.012569, acc.: 44.44%] [G loss: 0.696442]\n",
      "475 [D loss: 1.206561, acc.: 38.89%] [G loss: 1.779501]\n",
      "476 [D loss: 0.863298, acc.: 55.56%] [G loss: 0.958924]\n",
      "477 [D loss: 0.952121, acc.: 44.44%] [G loss: 1.027627]\n",
      "478 [D loss: 0.877599, acc.: 55.56%] [G loss: 1.610594]\n",
      "479 [D loss: 0.832498, acc.: 66.67%] [G loss: 0.767304]\n",
      "480 [D loss: 0.891952, acc.: 38.89%] [G loss: 1.348324]\n",
      "481 [D loss: 0.887565, acc.: 61.11%] [G loss: 1.745331]\n",
      "482 [D loss: 1.061683, acc.: 27.78%] [G loss: 1.089969]\n",
      "483 [D loss: 0.749824, acc.: 50.00%] [G loss: 1.189752]\n",
      "484 [D loss: 0.639667, acc.: 61.11%] [G loss: 1.317273]\n",
      "485 [D loss: 1.101018, acc.: 38.89%] [G loss: 1.271711]\n",
      "486 [D loss: 1.047620, acc.: 33.33%] [G loss: 0.875149]\n",
      "487 [D loss: 0.884736, acc.: 55.56%] [G loss: 1.106683]\n",
      "488 [D loss: 0.694238, acc.: 55.56%] [G loss: 1.131780]\n",
      "489 [D loss: 0.688183, acc.: 61.11%] [G loss: 1.515161]\n",
      "490 [D loss: 0.785342, acc.: 50.00%] [G loss: 1.090152]\n",
      "491 [D loss: 0.939618, acc.: 55.56%] [G loss: 1.174566]\n",
      "492 [D loss: 0.693617, acc.: 55.56%] [G loss: 1.349529]\n",
      "493 [D loss: 0.827296, acc.: 55.56%] [G loss: 1.111682]\n",
      "494 [D loss: 0.929351, acc.: 33.33%] [G loss: 0.803321]\n",
      "495 [D loss: 0.996237, acc.: 50.00%] [G loss: 1.555909]\n",
      "496 [D loss: 0.914379, acc.: 38.89%] [G loss: 1.324278]\n",
      "497 [D loss: 0.741639, acc.: 72.22%] [G loss: 1.448367]\n",
      "498 [D loss: 0.733873, acc.: 55.56%] [G loss: 0.947505]\n",
      "499 [D loss: 1.137043, acc.: 44.44%] [G loss: 1.338250]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 [D loss: 0.979489, acc.: 50.00%] [G loss: 1.661776]\n",
      "501 [D loss: 0.976641, acc.: 44.44%] [G loss: 1.580126]\n",
      "502 [D loss: 0.548141, acc.: 66.67%] [G loss: 1.468993]\n",
      "503 [D loss: 1.148751, acc.: 38.89%] [G loss: 0.913039]\n",
      "504 [D loss: 0.657391, acc.: 72.22%] [G loss: 1.349479]\n",
      "505 [D loss: 0.583877, acc.: 61.11%] [G loss: 1.171780]\n",
      "506 [D loss: 1.062416, acc.: 33.33%] [G loss: 1.819826]\n",
      "507 [D loss: 1.068186, acc.: 50.00%] [G loss: 1.140569]\n",
      "508 [D loss: 0.958022, acc.: 44.44%] [G loss: 1.380423]\n",
      "509 [D loss: 0.947978, acc.: 50.00%] [G loss: 1.130733]\n",
      "510 [D loss: 0.814948, acc.: 50.00%] [G loss: 1.030233]\n",
      "511 [D loss: 1.242362, acc.: 38.89%] [G loss: 0.668880]\n",
      "512 [D loss: 0.919834, acc.: 50.00%] [G loss: 1.527101]\n",
      "513 [D loss: 0.762263, acc.: 61.11%] [G loss: 0.815751]\n",
      "514 [D loss: 1.194803, acc.: 44.44%] [G loss: 1.167756]\n",
      "515 [D loss: 1.016894, acc.: 33.33%] [G loss: 1.080104]\n",
      "516 [D loss: 0.893571, acc.: 50.00%] [G loss: 0.622711]\n",
      "517 [D loss: 0.827555, acc.: 50.00%] [G loss: 0.801790]\n",
      "518 [D loss: 0.957936, acc.: 50.00%] [G loss: 0.966537]\n",
      "519 [D loss: 1.134868, acc.: 38.89%] [G loss: 1.321581]\n",
      "520 [D loss: 0.760232, acc.: 50.00%] [G loss: 1.448186]\n",
      "521 [D loss: 0.668646, acc.: 55.56%] [G loss: 1.505366]\n",
      "522 [D loss: 0.853608, acc.: 44.44%] [G loss: 1.892778]\n",
      "523 [D loss: 0.521099, acc.: 77.78%] [G loss: 1.561354]\n",
      "524 [D loss: 0.722635, acc.: 66.67%] [G loss: 1.428715]\n",
      "525 [D loss: 1.067783, acc.: 33.33%] [G loss: 0.952715]\n",
      "526 [D loss: 0.759122, acc.: 44.44%] [G loss: 0.871014]\n",
      "527 [D loss: 1.031011, acc.: 22.22%] [G loss: 0.848449]\n",
      "528 [D loss: 0.875942, acc.: 44.44%] [G loss: 1.813499]\n",
      "529 [D loss: 1.212003, acc.: 33.33%] [G loss: 1.962102]\n",
      "530 [D loss: 0.555145, acc.: 72.22%] [G loss: 2.207048]\n",
      "531 [D loss: 0.966202, acc.: 61.11%] [G loss: 1.431011]\n",
      "532 [D loss: 1.069634, acc.: 33.33%] [G loss: 1.040742]\n",
      "533 [D loss: 0.871525, acc.: 44.44%] [G loss: 1.007484]\n",
      "534 [D loss: 1.034757, acc.: 38.89%] [G loss: 1.154140]\n",
      "535 [D loss: 0.783188, acc.: 55.56%] [G loss: 1.809212]\n",
      "536 [D loss: 0.939387, acc.: 50.00%] [G loss: 1.548979]\n",
      "537 [D loss: 1.015051, acc.: 38.89%] [G loss: 1.326954]\n",
      "538 [D loss: 0.537370, acc.: 77.78%] [G loss: 1.672903]\n",
      "539 [D loss: 1.072664, acc.: 22.22%] [G loss: 1.182620]\n",
      "540 [D loss: 1.053035, acc.: 44.44%] [G loss: 0.908272]\n",
      "541 [D loss: 1.178761, acc.: 33.33%] [G loss: 1.080948]\n",
      "542 [D loss: 0.598148, acc.: 61.11%] [G loss: 1.935640]\n",
      "543 [D loss: 0.832509, acc.: 61.11%] [G loss: 1.085209]\n",
      "544 [D loss: 0.903437, acc.: 61.11%] [G loss: 0.753947]\n",
      "545 [D loss: 0.491207, acc.: 72.22%] [G loss: 1.679391]\n",
      "546 [D loss: 0.798173, acc.: 50.00%] [G loss: 1.656111]\n",
      "547 [D loss: 1.173457, acc.: 38.89%] [G loss: 1.133897]\n",
      "548 [D loss: 1.342307, acc.: 44.44%] [G loss: 1.139305]\n",
      "549 [D loss: 0.748948, acc.: 61.11%] [G loss: 1.173503]\n",
      "550 [D loss: 1.376463, acc.: 44.44%] [G loss: 1.841580]\n",
      "551 [D loss: 0.365878, acc.: 83.33%] [G loss: 2.323910]\n",
      "552 [D loss: 0.649734, acc.: 72.22%] [G loss: 1.966748]\n",
      "553 [D loss: 0.946422, acc.: 50.00%] [G loss: 0.867909]\n",
      "554 [D loss: 0.779404, acc.: 72.22%] [G loss: 1.760518]\n",
      "555 [D loss: 0.893076, acc.: 38.89%] [G loss: 2.068604]\n",
      "556 [D loss: 1.008324, acc.: 55.56%] [G loss: 1.388920]\n",
      "557 [D loss: 0.918833, acc.: 50.00%] [G loss: 1.444930]\n",
      "558 [D loss: 0.569266, acc.: 77.78%] [G loss: 1.451707]\n",
      "559 [D loss: 0.969225, acc.: 44.44%] [G loss: 0.659037]\n",
      "560 [D loss: 0.796910, acc.: 50.00%] [G loss: 0.884965]\n",
      "561 [D loss: 0.721630, acc.: 61.11%] [G loss: 1.364599]\n",
      "562 [D loss: 0.769796, acc.: 50.00%] [G loss: 1.790686]\n",
      "563 [D loss: 0.819869, acc.: 66.67%] [G loss: 1.290372]\n",
      "564 [D loss: 1.087097, acc.: 27.78%] [G loss: 1.106104]\n",
      "565 [D loss: 1.068609, acc.: 33.33%] [G loss: 1.634275]\n",
      "566 [D loss: 0.819272, acc.: 55.56%] [G loss: 1.540350]\n",
      "567 [D loss: 0.856702, acc.: 50.00%] [G loss: 1.317333]\n",
      "568 [D loss: 0.709965, acc.: 55.56%] [G loss: 2.174077]\n",
      "569 [D loss: 1.080891, acc.: 50.00%] [G loss: 1.200727]\n",
      "570 [D loss: 0.963703, acc.: 55.56%] [G loss: 1.619555]\n",
      "571 [D loss: 1.020587, acc.: 50.00%] [G loss: 1.500364]\n",
      "572 [D loss: 0.928741, acc.: 44.44%] [G loss: 1.290157]\n",
      "573 [D loss: 0.872540, acc.: 55.56%] [G loss: 1.614391]\n",
      "574 [D loss: 0.541782, acc.: 77.78%] [G loss: 1.643556]\n",
      "575 [D loss: 0.824166, acc.: 50.00%] [G loss: 2.280635]\n",
      "576 [D loss: 1.085275, acc.: 44.44%] [G loss: 1.307188]\n",
      "577 [D loss: 0.794289, acc.: 55.56%] [G loss: 1.687186]\n",
      "578 [D loss: 0.862727, acc.: 55.56%] [G loss: 1.193303]\n",
      "579 [D loss: 0.583648, acc.: 72.22%] [G loss: 1.808660]\n",
      "580 [D loss: 0.710367, acc.: 61.11%] [G loss: 1.746281]\n",
      "581 [D loss: 0.784117, acc.: 44.44%] [G loss: 1.931225]\n",
      "582 [D loss: 0.754103, acc.: 77.78%] [G loss: 2.010356]\n",
      "583 [D loss: 1.189731, acc.: 27.78%] [G loss: 0.825977]\n",
      "584 [D loss: 0.738544, acc.: 61.11%] [G loss: 1.703235]\n",
      "585 [D loss: 0.741990, acc.: 66.67%] [G loss: 1.893566]\n",
      "586 [D loss: 0.681916, acc.: 66.67%] [G loss: 2.867796]\n",
      "587 [D loss: 0.631477, acc.: 55.56%] [G loss: 1.615592]\n",
      "588 [D loss: 0.997022, acc.: 44.44%] [G loss: 0.677558]\n",
      "589 [D loss: 0.782001, acc.: 61.11%] [G loss: 1.382946]\n",
      "590 [D loss: 1.150023, acc.: 38.89%] [G loss: 0.881139]\n",
      "591 [D loss: 0.952238, acc.: 44.44%] [G loss: 1.362554]\n",
      "592 [D loss: 0.644570, acc.: 66.67%] [G loss: 1.692999]\n",
      "593 [D loss: 1.050834, acc.: 33.33%] [G loss: 1.780509]\n",
      "594 [D loss: 0.856437, acc.: 50.00%] [G loss: 1.415617]\n",
      "595 [D loss: 1.250790, acc.: 50.00%] [G loss: 1.046607]\n",
      "596 [D loss: 1.036304, acc.: 50.00%] [G loss: 1.600322]\n",
      "597 [D loss: 0.694482, acc.: 61.11%] [G loss: 1.395442]\n",
      "598 [D loss: 0.753977, acc.: 61.11%] [G loss: 1.457378]\n",
      "599 [D loss: 1.037852, acc.: 61.11%] [G loss: 2.451437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 [D loss: 0.773991, acc.: 55.56%] [G loss: 1.785483]\n",
      "601 [D loss: 0.729590, acc.: 55.56%] [G loss: 1.625195]\n",
      "602 [D loss: 0.919054, acc.: 61.11%] [G loss: 1.419231]\n",
      "603 [D loss: 0.729839, acc.: 61.11%] [G loss: 1.578220]\n",
      "604 [D loss: 0.584645, acc.: 66.67%] [G loss: 1.434747]\n",
      "605 [D loss: 0.864181, acc.: 50.00%] [G loss: 1.256855]\n",
      "606 [D loss: 0.829753, acc.: 55.56%] [G loss: 1.302006]\n",
      "607 [D loss: 0.944627, acc.: 55.56%] [G loss: 1.545982]\n",
      "608 [D loss: 0.693452, acc.: 61.11%] [G loss: 1.432667]\n",
      "609 [D loss: 0.673210, acc.: 72.22%] [G loss: 0.924887]\n",
      "610 [D loss: 1.033636, acc.: 50.00%] [G loss: 1.506009]\n",
      "611 [D loss: 0.651996, acc.: 72.22%] [G loss: 1.341572]\n",
      "612 [D loss: 1.223279, acc.: 44.44%] [G loss: 1.318628]\n",
      "613 [D loss: 0.757321, acc.: 72.22%] [G loss: 1.898391]\n",
      "614 [D loss: 0.935400, acc.: 50.00%] [G loss: 1.847287]\n",
      "615 [D loss: 1.064523, acc.: 50.00%] [G loss: 1.313344]\n",
      "616 [D loss: 0.420341, acc.: 88.89%] [G loss: 1.682217]\n",
      "617 [D loss: 0.871622, acc.: 55.56%] [G loss: 1.138123]\n",
      "618 [D loss: 0.704872, acc.: 55.56%] [G loss: 1.262682]\n",
      "619 [D loss: 1.010333, acc.: 38.89%] [G loss: 2.132452]\n",
      "620 [D loss: 0.753878, acc.: 55.56%] [G loss: 1.976836]\n",
      "621 [D loss: 0.832308, acc.: 44.44%] [G loss: 1.378496]\n",
      "622 [D loss: 0.899943, acc.: 50.00%] [G loss: 1.994823]\n",
      "623 [D loss: 0.863776, acc.: 44.44%] [G loss: 2.601344]\n",
      "624 [D loss: 0.689299, acc.: 55.56%] [G loss: 1.873047]\n",
      "625 [D loss: 0.572709, acc.: 72.22%] [G loss: 1.924802]\n",
      "626 [D loss: 0.744389, acc.: 50.00%] [G loss: 1.638078]\n",
      "627 [D loss: 1.152955, acc.: 38.89%] [G loss: 1.612185]\n",
      "628 [D loss: 1.190303, acc.: 33.33%] [G loss: 1.331728]\n",
      "629 [D loss: 0.499217, acc.: 66.67%] [G loss: 1.042755]\n",
      "630 [D loss: 0.762736, acc.: 61.11%] [G loss: 0.915946]\n",
      "631 [D loss: 0.785349, acc.: 44.44%] [G loss: 1.657431]\n",
      "632 [D loss: 1.224252, acc.: 27.78%] [G loss: 1.568440]\n",
      "633 [D loss: 0.675890, acc.: 66.67%] [G loss: 1.282468]\n",
      "634 [D loss: 0.718081, acc.: 66.67%] [G loss: 1.829522]\n",
      "635 [D loss: 0.700142, acc.: 55.56%] [G loss: 1.839633]\n",
      "636 [D loss: 0.537210, acc.: 77.78%] [G loss: 1.870926]\n",
      "637 [D loss: 0.738646, acc.: 55.56%] [G loss: 2.073318]\n",
      "638 [D loss: 0.623294, acc.: 61.11%] [G loss: 1.532756]\n",
      "639 [D loss: 0.803968, acc.: 50.00%] [G loss: 1.203397]\n",
      "640 [D loss: 0.950182, acc.: 55.56%] [G loss: 1.311208]\n",
      "641 [D loss: 0.431373, acc.: 88.89%] [G loss: 2.080488]\n",
      "642 [D loss: 1.028176, acc.: 44.44%] [G loss: 0.955629]\n",
      "643 [D loss: 0.650138, acc.: 66.67%] [G loss: 1.979676]\n",
      "644 [D loss: 0.632728, acc.: 55.56%] [G loss: 1.767656]\n",
      "645 [D loss: 0.947984, acc.: 55.56%] [G loss: 1.005740]\n",
      "646 [D loss: 0.893101, acc.: 61.11%] [G loss: 1.812038]\n",
      "647 [D loss: 1.088273, acc.: 44.44%] [G loss: 1.137085]\n",
      "648 [D loss: 0.675147, acc.: 61.11%] [G loss: 2.064349]\n",
      "649 [D loss: 0.758660, acc.: 50.00%] [G loss: 1.429413]\n",
      "650 [D loss: 0.955830, acc.: 50.00%] [G loss: 2.025473]\n",
      "651 [D loss: 1.011820, acc.: 38.89%] [G loss: 1.513883]\n",
      "652 [D loss: 0.936913, acc.: 61.11%] [G loss: 1.555383]\n",
      "653 [D loss: 0.694154, acc.: 61.11%] [G loss: 1.185409]\n",
      "654 [D loss: 0.901639, acc.: 50.00%] [G loss: 1.839895]\n",
      "655 [D loss: 0.621279, acc.: 55.56%] [G loss: 1.810774]\n",
      "656 [D loss: 0.716195, acc.: 50.00%] [G loss: 1.553602]\n",
      "657 [D loss: 1.505291, acc.: 22.22%] [G loss: 0.825647]\n",
      "658 [D loss: 0.798647, acc.: 55.56%] [G loss: 1.816285]\n",
      "659 [D loss: 1.092129, acc.: 38.89%] [G loss: 1.147393]\n",
      "660 [D loss: 0.564121, acc.: 72.22%] [G loss: 1.979428]\n",
      "661 [D loss: 0.765078, acc.: 61.11%] [G loss: 1.828106]\n",
      "662 [D loss: 0.835431, acc.: 61.11%] [G loss: 2.113423]\n",
      "663 [D loss: 0.597811, acc.: 61.11%] [G loss: 0.643587]\n",
      "664 [D loss: 0.750606, acc.: 55.56%] [G loss: 1.239068]\n",
      "665 [D loss: 1.060606, acc.: 44.44%] [G loss: 1.078597]\n",
      "666 [D loss: 0.524706, acc.: 66.67%] [G loss: 2.190522]\n",
      "667 [D loss: 0.871197, acc.: 50.00%] [G loss: 1.780879]\n",
      "668 [D loss: 0.852767, acc.: 38.89%] [G loss: 1.250724]\n",
      "669 [D loss: 0.804439, acc.: 50.00%] [G loss: 1.582892]\n",
      "670 [D loss: 0.360714, acc.: 88.89%] [G loss: 1.623717]\n",
      "671 [D loss: 0.825362, acc.: 66.67%] [G loss: 1.417423]\n",
      "672 [D loss: 0.841854, acc.: 61.11%] [G loss: 1.496702]\n",
      "673 [D loss: 0.343656, acc.: 94.44%] [G loss: 2.288870]\n",
      "674 [D loss: 0.559239, acc.: 66.67%] [G loss: 1.494874]\n",
      "675 [D loss: 0.374724, acc.: 83.33%] [G loss: 0.715363]\n",
      "676 [D loss: 1.005546, acc.: 50.00%] [G loss: 1.000285]\n",
      "677 [D loss: 0.596282, acc.: 61.11%] [G loss: 1.380072]\n",
      "678 [D loss: 0.733821, acc.: 61.11%] [G loss: 1.610390]\n",
      "679 [D loss: 0.588226, acc.: 61.11%] [G loss: 1.890081]\n",
      "680 [D loss: 1.042370, acc.: 44.44%] [G loss: 1.495471]\n",
      "681 [D loss: 0.951093, acc.: 61.11%] [G loss: 1.503417]\n",
      "682 [D loss: 0.655436, acc.: 77.78%] [G loss: 1.092269]\n",
      "683 [D loss: 0.504372, acc.: 61.11%] [G loss: 1.495756]\n",
      "684 [D loss: 0.416558, acc.: 77.78%] [G loss: 1.149617]\n",
      "685 [D loss: 0.755428, acc.: 55.56%] [G loss: 1.305747]\n",
      "686 [D loss: 0.519650, acc.: 72.22%] [G loss: 1.593904]\n",
      "687 [D loss: 0.909454, acc.: 55.56%] [G loss: 1.844094]\n",
      "688 [D loss: 0.787610, acc.: 61.11%] [G loss: 2.241147]\n",
      "689 [D loss: 1.030115, acc.: 50.00%] [G loss: 1.513896]\n",
      "690 [D loss: 0.617366, acc.: 61.11%] [G loss: 1.023846]\n",
      "691 [D loss: 0.931564, acc.: 33.33%] [G loss: 1.486107]\n",
      "692 [D loss: 0.495974, acc.: 72.22%] [G loss: 1.888194]\n",
      "693 [D loss: 0.505723, acc.: 72.22%] [G loss: 1.502214]\n",
      "694 [D loss: 0.536757, acc.: 77.78%] [G loss: 1.816713]\n",
      "695 [D loss: 0.614362, acc.: 66.67%] [G loss: 1.952664]\n",
      "696 [D loss: 0.952215, acc.: 50.00%] [G loss: 1.776248]\n",
      "697 [D loss: 0.397349, acc.: 83.33%] [G loss: 1.999815]\n",
      "698 [D loss: 0.870488, acc.: 55.56%] [G loss: 1.577443]\n",
      "699 [D loss: 0.848271, acc.: 55.56%] [G loss: 1.122634]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 [D loss: 0.664292, acc.: 61.11%] [G loss: 0.849382]\n",
      "701 [D loss: 0.742091, acc.: 61.11%] [G loss: 1.399357]\n",
      "702 [D loss: 0.637721, acc.: 72.22%] [G loss: 1.939463]\n",
      "703 [D loss: 0.810706, acc.: 50.00%] [G loss: 1.592378]\n",
      "704 [D loss: 1.372403, acc.: 38.89%] [G loss: 1.356634]\n",
      "705 [D loss: 0.531183, acc.: 72.22%] [G loss: 1.664716]\n",
      "706 [D loss: 0.475353, acc.: 72.22%] [G loss: 2.281463]\n",
      "707 [D loss: 0.976626, acc.: 55.56%] [G loss: 2.256082]\n",
      "708 [D loss: 0.892395, acc.: 61.11%] [G loss: 1.109616]\n",
      "709 [D loss: 0.458373, acc.: 83.33%] [G loss: 1.898504]\n",
      "710 [D loss: 0.867656, acc.: 61.11%] [G loss: 1.893666]\n",
      "711 [D loss: 0.718851, acc.: 61.11%] [G loss: 1.650522]\n",
      "712 [D loss: 0.900120, acc.: 44.44%] [G loss: 1.592016]\n",
      "713 [D loss: 1.152337, acc.: 33.33%] [G loss: 1.315338]\n",
      "714 [D loss: 0.588323, acc.: 66.67%] [G loss: 2.208001]\n",
      "715 [D loss: 0.677901, acc.: 61.11%] [G loss: 1.031257]\n",
      "716 [D loss: 0.637856, acc.: 66.67%] [G loss: 1.341972]\n",
      "717 [D loss: 0.748299, acc.: 66.67%] [G loss: 1.476870]\n",
      "718 [D loss: 0.861966, acc.: 55.56%] [G loss: 1.431705]\n",
      "719 [D loss: 0.506375, acc.: 77.78%] [G loss: 1.352256]\n",
      "720 [D loss: 0.721449, acc.: 55.56%] [G loss: 1.546537]\n",
      "721 [D loss: 0.751440, acc.: 61.11%] [G loss: 1.232504]\n",
      "722 [D loss: 0.410538, acc.: 77.78%] [G loss: 1.739510]\n",
      "723 [D loss: 0.351831, acc.: 94.44%] [G loss: 1.685116]\n",
      "724 [D loss: 1.015893, acc.: 55.56%] [G loss: 2.001884]\n",
      "725 [D loss: 0.504834, acc.: 72.22%] [G loss: 1.746964]\n",
      "726 [D loss: 1.105036, acc.: 66.67%] [G loss: 1.320409]\n",
      "727 [D loss: 0.720751, acc.: 44.44%] [G loss: 1.364179]\n",
      "728 [D loss: 0.436594, acc.: 83.33%] [G loss: 2.427693]\n",
      "729 [D loss: 0.724280, acc.: 66.67%] [G loss: 1.942653]\n",
      "730 [D loss: 0.610972, acc.: 66.67%] [G loss: 2.244528]\n",
      "731 [D loss: 0.666006, acc.: 72.22%] [G loss: 2.021074]\n",
      "732 [D loss: 0.960614, acc.: 55.56%] [G loss: 1.422252]\n",
      "733 [D loss: 0.788708, acc.: 66.67%] [G loss: 1.060667]\n",
      "734 [D loss: 0.503995, acc.: 83.33%] [G loss: 2.050088]\n",
      "735 [D loss: 0.384400, acc.: 83.33%] [G loss: 1.585016]\n",
      "736 [D loss: 0.886435, acc.: 55.56%] [G loss: 2.067084]\n",
      "737 [D loss: 0.967606, acc.: 50.00%] [G loss: 1.210900]\n",
      "738 [D loss: 0.654035, acc.: 61.11%] [G loss: 1.791538]\n",
      "739 [D loss: 0.655987, acc.: 55.56%] [G loss: 1.357786]\n",
      "740 [D loss: 0.773407, acc.: 55.56%] [G loss: 1.369502]\n",
      "741 [D loss: 0.715370, acc.: 61.11%] [G loss: 1.811175]\n",
      "742 [D loss: 0.656133, acc.: 55.56%] [G loss: 2.719712]\n",
      "743 [D loss: 0.549011, acc.: 72.22%] [G loss: 1.892684]\n",
      "744 [D loss: 0.294600, acc.: 100.00%] [G loss: 1.710359]\n",
      "745 [D loss: 0.695744, acc.: 66.67%] [G loss: 1.611521]\n",
      "746 [D loss: 0.589588, acc.: 61.11%] [G loss: 2.161469]\n",
      "747 [D loss: 0.519700, acc.: 66.67%] [G loss: 1.462988]\n",
      "748 [D loss: 0.399950, acc.: 83.33%] [G loss: 1.916944]\n",
      "749 [D loss: 0.818813, acc.: 61.11%] [G loss: 2.224110]\n",
      "750 [D loss: 0.960919, acc.: 44.44%] [G loss: 1.515737]\n",
      "751 [D loss: 0.465729, acc.: 72.22%] [G loss: 1.577154]\n",
      "752 [D loss: 1.200269, acc.: 61.11%] [G loss: 1.504211]\n",
      "753 [D loss: 0.793261, acc.: 55.56%] [G loss: 1.293380]\n",
      "754 [D loss: 0.802228, acc.: 61.11%] [G loss: 2.042055]\n",
      "755 [D loss: 0.484777, acc.: 77.78%] [G loss: 1.853886]\n",
      "756 [D loss: 0.706779, acc.: 72.22%] [G loss: 1.157382]\n",
      "757 [D loss: 0.794614, acc.: 66.67%] [G loss: 2.274336]\n",
      "758 [D loss: 0.615018, acc.: 77.78%] [G loss: 1.329561]\n",
      "759 [D loss: 0.623163, acc.: 72.22%] [G loss: 1.624840]\n",
      "760 [D loss: 0.793083, acc.: 61.11%] [G loss: 2.075381]\n",
      "761 [D loss: 0.524715, acc.: 72.22%] [G loss: 1.929060]\n",
      "762 [D loss: 0.648101, acc.: 72.22%] [G loss: 1.029351]\n",
      "763 [D loss: 0.391187, acc.: 83.33%] [G loss: 1.510092]\n",
      "764 [D loss: 1.019176, acc.: 44.44%] [G loss: 1.676437]\n",
      "765 [D loss: 0.673296, acc.: 77.78%] [G loss: 2.454059]\n",
      "766 [D loss: 0.564787, acc.: 72.22%] [G loss: 1.956210]\n",
      "767 [D loss: 0.501483, acc.: 61.11%] [G loss: 1.800775]\n",
      "768 [D loss: 0.579217, acc.: 61.11%] [G loss: 1.915452]\n",
      "769 [D loss: 0.743490, acc.: 66.67%] [G loss: 0.993580]\n",
      "770 [D loss: 0.383174, acc.: 88.89%] [G loss: 2.004735]\n",
      "771 [D loss: 0.619297, acc.: 66.67%] [G loss: 1.623667]\n",
      "772 [D loss: 0.684955, acc.: 61.11%] [G loss: 1.554963]\n",
      "773 [D loss: 0.443766, acc.: 88.89%] [G loss: 2.067605]\n",
      "774 [D loss: 0.431704, acc.: 77.78%] [G loss: 2.411751]\n",
      "775 [D loss: 0.577087, acc.: 66.67%] [G loss: 1.941251]\n",
      "776 [D loss: 0.520309, acc.: 66.67%] [G loss: 2.060744]\n",
      "777 [D loss: 0.349828, acc.: 88.89%] [G loss: 2.392826]\n",
      "778 [D loss: 0.499000, acc.: 83.33%] [G loss: 1.869104]\n",
      "779 [D loss: 0.705057, acc.: 61.11%] [G loss: 2.124476]\n",
      "780 [D loss: 1.366938, acc.: 38.89%] [G loss: 1.789647]\n",
      "781 [D loss: 0.960498, acc.: 61.11%] [G loss: 2.310709]\n",
      "782 [D loss: 0.816680, acc.: 61.11%] [G loss: 2.048002]\n",
      "783 [D loss: 0.972504, acc.: 55.56%] [G loss: 1.101551]\n",
      "784 [D loss: 0.456541, acc.: 66.67%] [G loss: 2.645227]\n",
      "785 [D loss: 1.113877, acc.: 38.89%] [G loss: 1.700790]\n",
      "786 [D loss: 0.363147, acc.: 83.33%] [G loss: 2.706238]\n",
      "787 [D loss: 0.976933, acc.: 66.67%] [G loss: 1.625300]\n",
      "788 [D loss: 0.358438, acc.: 83.33%] [G loss: 1.563847]\n",
      "789 [D loss: 0.970565, acc.: 55.56%] [G loss: 1.059524]\n",
      "790 [D loss: 0.814197, acc.: 55.56%] [G loss: 1.100749]\n",
      "791 [D loss: 0.552077, acc.: 72.22%] [G loss: 1.790122]\n",
      "792 [D loss: 0.687627, acc.: 55.56%] [G loss: 1.129250]\n",
      "793 [D loss: 0.860247, acc.: 61.11%] [G loss: 1.848488]\n",
      "794 [D loss: 0.825537, acc.: 44.44%] [G loss: 2.163159]\n",
      "795 [D loss: 0.865169, acc.: 72.22%] [G loss: 1.915892]\n",
      "796 [D loss: 0.694268, acc.: 55.56%] [G loss: 1.289185]\n",
      "797 [D loss: 0.431859, acc.: 83.33%] [G loss: 1.794203]\n",
      "798 [D loss: 0.463893, acc.: 72.22%] [G loss: 2.045117]\n",
      "799 [D loss: 0.515189, acc.: 77.78%] [G loss: 2.117874]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 [D loss: 0.684394, acc.: 72.22%] [G loss: 1.880928]\n",
      "801 [D loss: 1.134998, acc.: 61.11%] [G loss: 0.959773]\n",
      "802 [D loss: 0.609948, acc.: 77.78%] [G loss: 2.826722]\n",
      "803 [D loss: 0.846580, acc.: 38.89%] [G loss: 1.624172]\n",
      "804 [D loss: 0.901856, acc.: 44.44%] [G loss: 1.416008]\n",
      "805 [D loss: 0.699538, acc.: 72.22%] [G loss: 2.243397]\n",
      "806 [D loss: 0.694354, acc.: 66.67%] [G loss: 1.580484]\n",
      "807 [D loss: 0.795246, acc.: 50.00%] [G loss: 1.174618]\n",
      "808 [D loss: 0.376163, acc.: 83.33%] [G loss: 1.635841]\n",
      "809 [D loss: 0.608181, acc.: 55.56%] [G loss: 0.891153]\n",
      "810 [D loss: 0.718415, acc.: 66.67%] [G loss: 1.780499]\n",
      "811 [D loss: 0.565400, acc.: 72.22%] [G loss: 2.098878]\n",
      "812 [D loss: 1.056854, acc.: 61.11%] [G loss: 1.259318]\n",
      "813 [D loss: 0.760920, acc.: 55.56%] [G loss: 1.855161]\n",
      "814 [D loss: 0.538393, acc.: 66.67%] [G loss: 2.217570]\n",
      "815 [D loss: 0.804447, acc.: 55.56%] [G loss: 1.783905]\n",
      "816 [D loss: 0.835175, acc.: 55.56%] [G loss: 1.271871]\n",
      "817 [D loss: 0.917009, acc.: 66.67%] [G loss: 1.769033]\n",
      "818 [D loss: 0.880878, acc.: 55.56%] [G loss: 1.809816]\n",
      "819 [D loss: 0.653671, acc.: 66.67%] [G loss: 1.383133]\n",
      "820 [D loss: 0.978588, acc.: 50.00%] [G loss: 1.909190]\n",
      "821 [D loss: 0.551487, acc.: 72.22%] [G loss: 2.367695]\n",
      "822 [D loss: 0.893275, acc.: 44.44%] [G loss: 1.922186]\n",
      "823 [D loss: 0.824331, acc.: 61.11%] [G loss: 2.091042]\n",
      "824 [D loss: 0.570186, acc.: 77.78%] [G loss: 2.108848]\n",
      "825 [D loss: 0.846112, acc.: 50.00%] [G loss: 1.898622]\n",
      "826 [D loss: 1.063026, acc.: 50.00%] [G loss: 2.142531]\n",
      "827 [D loss: 0.984939, acc.: 38.89%] [G loss: 1.019377]\n",
      "828 [D loss: 0.748264, acc.: 66.67%] [G loss: 1.654591]\n",
      "829 [D loss: 1.160617, acc.: 44.44%] [G loss: 2.062597]\n",
      "830 [D loss: 0.718364, acc.: 61.11%] [G loss: 2.268902]\n",
      "831 [D loss: 0.754704, acc.: 44.44%] [G loss: 1.591623]\n",
      "832 [D loss: 0.685278, acc.: 66.67%] [G loss: 2.713644]\n",
      "833 [D loss: 0.753331, acc.: 55.56%] [G loss: 1.913756]\n",
      "834 [D loss: 0.795177, acc.: 44.44%] [G loss: 1.757966]\n",
      "835 [D loss: 0.909921, acc.: 55.56%] [G loss: 1.927683]\n",
      "836 [D loss: 0.907730, acc.: 66.67%] [G loss: 2.117736]\n",
      "837 [D loss: 1.073619, acc.: 44.44%] [G loss: 2.074510]\n",
      "838 [D loss: 1.070846, acc.: 55.56%] [G loss: 2.302447]\n",
      "839 [D loss: 0.999077, acc.: 44.44%] [G loss: 1.520878]\n",
      "840 [D loss: 0.441067, acc.: 77.78%] [G loss: 1.895565]\n",
      "841 [D loss: 0.629465, acc.: 72.22%] [G loss: 1.916126]\n",
      "842 [D loss: 0.901625, acc.: 72.22%] [G loss: 1.902647]\n",
      "843 [D loss: 1.041650, acc.: 44.44%] [G loss: 1.842121]\n",
      "844 [D loss: 0.340530, acc.: 83.33%] [G loss: 2.077161]\n",
      "845 [D loss: 0.790572, acc.: 55.56%] [G loss: 2.006844]\n",
      "846 [D loss: 0.403781, acc.: 83.33%] [G loss: 2.107898]\n",
      "847 [D loss: 0.933761, acc.: 50.00%] [G loss: 1.098625]\n",
      "848 [D loss: 0.715454, acc.: 50.00%] [G loss: 1.262984]\n",
      "849 [D loss: 0.692290, acc.: 77.78%] [G loss: 1.645427]\n",
      "850 [D loss: 0.857452, acc.: 55.56%] [G loss: 1.452088]\n",
      "851 [D loss: 1.129769, acc.: 38.89%] [G loss: 1.793952]\n",
      "852 [D loss: 0.731096, acc.: 66.67%] [G loss: 1.710737]\n",
      "853 [D loss: 0.846169, acc.: 66.67%] [G loss: 1.063816]\n",
      "854 [D loss: 0.416063, acc.: 77.78%] [G loss: 1.404128]\n",
      "855 [D loss: 0.632928, acc.: 61.11%] [G loss: 1.230830]\n",
      "856 [D loss: 0.688726, acc.: 66.67%] [G loss: 2.667115]\n",
      "857 [D loss: 1.344140, acc.: 44.44%] [G loss: 1.802107]\n",
      "858 [D loss: 0.582297, acc.: 72.22%] [G loss: 1.552073]\n",
      "859 [D loss: 0.438371, acc.: 72.22%] [G loss: 2.172552]\n",
      "860 [D loss: 0.880467, acc.: 55.56%] [G loss: 1.703658]\n",
      "861 [D loss: 0.580149, acc.: 72.22%] [G loss: 1.641835]\n",
      "862 [D loss: 0.673561, acc.: 72.22%] [G loss: 1.450503]\n",
      "863 [D loss: 0.802343, acc.: 66.67%] [G loss: 1.506431]\n",
      "864 [D loss: 0.807959, acc.: 44.44%] [G loss: 2.055577]\n",
      "865 [D loss: 0.645312, acc.: 55.56%] [G loss: 1.548350]\n",
      "866 [D loss: 1.189230, acc.: 38.89%] [G loss: 1.267371]\n",
      "867 [D loss: 0.701465, acc.: 55.56%] [G loss: 1.504825]\n",
      "868 [D loss: 0.656581, acc.: 61.11%] [G loss: 2.248504]\n",
      "869 [D loss: 0.669794, acc.: 55.56%] [G loss: 1.960324]\n",
      "870 [D loss: 0.766992, acc.: 55.56%] [G loss: 1.730662]\n",
      "871 [D loss: 0.282924, acc.: 88.89%] [G loss: 2.549665]\n",
      "872 [D loss: 0.307375, acc.: 88.89%] [G loss: 3.157279]\n",
      "873 [D loss: 0.694967, acc.: 55.56%] [G loss: 0.582692]\n",
      "874 [D loss: 0.451135, acc.: 66.67%] [G loss: 1.863795]\n",
      "875 [D loss: 0.566213, acc.: 66.67%] [G loss: 1.731295]\n",
      "876 [D loss: 0.810363, acc.: 66.67%] [G loss: 2.436132]\n",
      "877 [D loss: 0.480121, acc.: 77.78%] [G loss: 2.670499]\n",
      "878 [D loss: 0.462582, acc.: 72.22%] [G loss: 2.466056]\n",
      "879 [D loss: 0.336461, acc.: 88.89%] [G loss: 2.336401]\n",
      "880 [D loss: 0.659851, acc.: 77.78%] [G loss: 2.384894]\n",
      "881 [D loss: 0.624198, acc.: 77.78%] [G loss: 1.518566]\n",
      "882 [D loss: 0.605176, acc.: 61.11%] [G loss: 2.252819]\n",
      "883 [D loss: 0.673247, acc.: 61.11%] [G loss: 1.586264]\n",
      "884 [D loss: 0.661269, acc.: 66.67%] [G loss: 1.897655]\n",
      "885 [D loss: 0.506927, acc.: 72.22%] [G loss: 1.768426]\n",
      "886 [D loss: 0.612388, acc.: 66.67%] [G loss: 1.358741]\n",
      "887 [D loss: 0.381463, acc.: 77.78%] [G loss: 1.802814]\n",
      "888 [D loss: 0.688273, acc.: 61.11%] [G loss: 1.301516]\n",
      "889 [D loss: 0.541522, acc.: 61.11%] [G loss: 1.716478]\n",
      "890 [D loss: 0.458771, acc.: 83.33%] [G loss: 2.184169]\n",
      "891 [D loss: 0.944771, acc.: 61.11%] [G loss: 1.357152]\n",
      "892 [D loss: 0.866134, acc.: 44.44%] [G loss: 1.938905]\n",
      "893 [D loss: 0.570188, acc.: 66.67%] [G loss: 1.493882]\n",
      "894 [D loss: 0.426603, acc.: 72.22%] [G loss: 1.926466]\n",
      "895 [D loss: 0.568657, acc.: 72.22%] [G loss: 2.183517]\n",
      "896 [D loss: 0.856448, acc.: 72.22%] [G loss: 1.441185]\n",
      "897 [D loss: 0.453846, acc.: 77.78%] [G loss: 1.859913]\n",
      "898 [D loss: 1.399306, acc.: 38.89%] [G loss: 1.295564]\n",
      "899 [D loss: 0.784050, acc.: 61.11%] [G loss: 2.323576]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 [D loss: 0.881349, acc.: 44.44%] [G loss: 1.375077]\n",
      "901 [D loss: 0.658574, acc.: 66.67%] [G loss: 1.762882]\n",
      "902 [D loss: 0.566113, acc.: 72.22%] [G loss: 2.054252]\n",
      "903 [D loss: 0.726801, acc.: 61.11%] [G loss: 2.244574]\n",
      "904 [D loss: 0.648927, acc.: 77.78%] [G loss: 1.529392]\n",
      "905 [D loss: 0.782832, acc.: 66.67%] [G loss: 2.377808]\n",
      "906 [D loss: 0.576779, acc.: 72.22%] [G loss: 1.838182]\n",
      "907 [D loss: 0.384536, acc.: 83.33%] [G loss: 1.461373]\n",
      "908 [D loss: 0.570054, acc.: 61.11%] [G loss: 1.700218]\n",
      "909 [D loss: 0.701864, acc.: 66.67%] [G loss: 1.001429]\n",
      "910 [D loss: 0.395433, acc.: 83.33%] [G loss: 1.956014]\n",
      "911 [D loss: 1.101052, acc.: 61.11%] [G loss: 1.892712]\n",
      "912 [D loss: 0.263304, acc.: 88.89%] [G loss: 2.391429]\n",
      "913 [D loss: 0.341778, acc.: 88.89%] [G loss: 1.958352]\n",
      "914 [D loss: 0.751231, acc.: 66.67%] [G loss: 3.100779]\n",
      "915 [D loss: 0.525342, acc.: 83.33%] [G loss: 2.194203]\n",
      "916 [D loss: 0.485658, acc.: 72.22%] [G loss: 1.800488]\n",
      "917 [D loss: 0.500143, acc.: 66.67%] [G loss: 1.937371]\n",
      "918 [D loss: 0.604780, acc.: 66.67%] [G loss: 1.534409]\n",
      "919 [D loss: 0.786397, acc.: 44.44%] [G loss: 1.863394]\n",
      "920 [D loss: 0.630237, acc.: 55.56%] [G loss: 1.886745]\n",
      "921 [D loss: 0.399140, acc.: 72.22%] [G loss: 2.490540]\n",
      "922 [D loss: 0.704657, acc.: 66.67%] [G loss: 1.709746]\n",
      "923 [D loss: 1.038070, acc.: 50.00%] [G loss: 1.262612]\n",
      "924 [D loss: 0.505726, acc.: 72.22%] [G loss: 1.969466]\n",
      "925 [D loss: 0.472520, acc.: 72.22%] [G loss: 2.770494]\n",
      "926 [D loss: 0.808260, acc.: 55.56%] [G loss: 2.659512]\n",
      "927 [D loss: 0.786453, acc.: 72.22%] [G loss: 1.397648]\n",
      "928 [D loss: 0.452049, acc.: 72.22%] [G loss: 1.781211]\n",
      "929 [D loss: 0.608710, acc.: 77.78%] [G loss: 1.565124]\n",
      "930 [D loss: 0.433326, acc.: 77.78%] [G loss: 1.654795]\n",
      "931 [D loss: 0.381827, acc.: 88.89%] [G loss: 2.881022]\n",
      "932 [D loss: 0.581747, acc.: 66.67%] [G loss: 2.835185]\n",
      "933 [D loss: 0.407189, acc.: 77.78%] [G loss: 2.362398]\n",
      "934 [D loss: 0.750400, acc.: 55.56%] [G loss: 1.907942]\n",
      "935 [D loss: 0.316817, acc.: 88.89%] [G loss: 1.232740]\n",
      "936 [D loss: 0.428522, acc.: 88.89%] [G loss: 2.458798]\n",
      "937 [D loss: 0.442726, acc.: 72.22%] [G loss: 1.888666]\n",
      "938 [D loss: 0.915843, acc.: 61.11%] [G loss: 2.054861]\n",
      "939 [D loss: 0.332534, acc.: 88.89%] [G loss: 2.692895]\n",
      "940 [D loss: 0.554061, acc.: 83.33%] [G loss: 1.847710]\n",
      "941 [D loss: 0.695308, acc.: 61.11%] [G loss: 1.918555]\n",
      "942 [D loss: 0.481141, acc.: 88.89%] [G loss: 2.184823]\n",
      "943 [D loss: 0.349182, acc.: 88.89%] [G loss: 2.375019]\n",
      "944 [D loss: 0.856642, acc.: 61.11%] [G loss: 1.910832]\n",
      "945 [D loss: 0.667832, acc.: 72.22%] [G loss: 2.265544]\n",
      "946 [D loss: 0.686064, acc.: 50.00%] [G loss: 1.356408]\n",
      "947 [D loss: 0.757268, acc.: 72.22%] [G loss: 1.774570]\n",
      "948 [D loss: 0.863794, acc.: 61.11%] [G loss: 2.591482]\n",
      "949 [D loss: 0.540980, acc.: 66.67%] [G loss: 1.783131]\n",
      "950 [D loss: 1.137660, acc.: 38.89%] [G loss: 1.488842]\n",
      "951 [D loss: 0.812152, acc.: 50.00%] [G loss: 1.465799]\n",
      "952 [D loss: 0.533992, acc.: 83.33%] [G loss: 2.046821]\n",
      "953 [D loss: 0.862837, acc.: 55.56%] [G loss: 1.851119]\n",
      "954 [D loss: 0.385894, acc.: 83.33%] [G loss: 1.756034]\n",
      "955 [D loss: 0.954061, acc.: 50.00%] [G loss: 1.424363]\n",
      "956 [D loss: 0.581206, acc.: 83.33%] [G loss: 2.731503]\n",
      "957 [D loss: 0.897381, acc.: 38.89%] [G loss: 1.373584]\n",
      "958 [D loss: 0.785910, acc.: 77.78%] [G loss: 1.961408]\n",
      "959 [D loss: 0.903780, acc.: 61.11%] [G loss: 1.535280]\n",
      "960 [D loss: 0.765195, acc.: 55.56%] [G loss: 1.584331]\n",
      "961 [D loss: 0.720133, acc.: 72.22%] [G loss: 1.412390]\n",
      "962 [D loss: 0.716748, acc.: 55.56%] [G loss: 2.339221]\n",
      "963 [D loss: 0.940089, acc.: 66.67%] [G loss: 2.025960]\n",
      "964 [D loss: 0.735502, acc.: 44.44%] [G loss: 1.748704]\n",
      "965 [D loss: 0.516768, acc.: 83.33%] [G loss: 2.692095]\n",
      "966 [D loss: 0.614467, acc.: 72.22%] [G loss: 1.575238]\n",
      "967 [D loss: 0.651978, acc.: 55.56%] [G loss: 1.581448]\n",
      "968 [D loss: 0.259733, acc.: 88.89%] [G loss: 1.658507]\n",
      "969 [D loss: 0.420631, acc.: 77.78%] [G loss: 1.846498]\n",
      "970 [D loss: 0.501033, acc.: 77.78%] [G loss: 1.476234]\n",
      "971 [D loss: 0.648978, acc.: 66.67%] [G loss: 1.857146]\n",
      "972 [D loss: 0.814477, acc.: 61.11%] [G loss: 2.613242]\n",
      "973 [D loss: 0.578972, acc.: 72.22%] [G loss: 2.046054]\n",
      "974 [D loss: 1.145804, acc.: 55.56%] [G loss: 1.378674]\n",
      "975 [D loss: 0.548649, acc.: 77.78%] [G loss: 2.107671]\n",
      "976 [D loss: 1.028178, acc.: 61.11%] [G loss: 1.726320]\n",
      "977 [D loss: 0.457979, acc.: 66.67%] [G loss: 2.281427]\n",
      "978 [D loss: 1.040766, acc.: 44.44%] [G loss: 1.726886]\n",
      "979 [D loss: 0.242306, acc.: 100.00%] [G loss: 3.069598]\n",
      "980 [D loss: 0.425770, acc.: 77.78%] [G loss: 1.868722]\n",
      "981 [D loss: 0.623630, acc.: 66.67%] [G loss: 1.471453]\n",
      "982 [D loss: 0.563391, acc.: 72.22%] [G loss: 1.790549]\n",
      "983 [D loss: 0.401965, acc.: 77.78%] [G loss: 1.717733]\n",
      "984 [D loss: 1.057827, acc.: 50.00%] [G loss: 1.607265]\n",
      "985 [D loss: 0.649104, acc.: 72.22%] [G loss: 2.069680]\n",
      "986 [D loss: 0.567747, acc.: 72.22%] [G loss: 2.478605]\n",
      "987 [D loss: 0.912134, acc.: 55.56%] [G loss: 1.158510]\n",
      "988 [D loss: 0.642439, acc.: 83.33%] [G loss: 1.629730]\n",
      "989 [D loss: 0.549014, acc.: 72.22%] [G loss: 2.402769]\n",
      "990 [D loss: 0.459656, acc.: 83.33%] [G loss: 1.913550]\n",
      "991 [D loss: 0.693342, acc.: 66.67%] [G loss: 1.843823]\n",
      "992 [D loss: 0.254378, acc.: 94.44%] [G loss: 2.756314]\n",
      "993 [D loss: 0.490163, acc.: 72.22%] [G loss: 2.428292]\n",
      "994 [D loss: 0.833054, acc.: 50.00%] [G loss: 1.652874]\n",
      "995 [D loss: 0.590780, acc.: 72.22%] [G loss: 1.682822]\n",
      "996 [D loss: 0.519311, acc.: 72.22%] [G loss: 3.140258]\n",
      "997 [D loss: 0.548560, acc.: 72.22%] [G loss: 3.228813]\n",
      "998 [D loss: 0.551371, acc.: 72.22%] [G loss: 1.672028]\n",
      "999 [D loss: 0.763758, acc.: 66.67%] [G loss: 2.186660]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 [D loss: 0.581181, acc.: 77.78%] [G loss: 2.430532]\n",
      "1001 [D loss: 0.321896, acc.: 77.78%] [G loss: 2.556900]\n",
      "1002 [D loss: 0.536623, acc.: 77.78%] [G loss: 1.263294]\n",
      "1003 [D loss: 0.462611, acc.: 77.78%] [G loss: 1.698879]\n",
      "1004 [D loss: 0.262688, acc.: 88.89%] [G loss: 1.494178]\n",
      "1005 [D loss: 0.421559, acc.: 88.89%] [G loss: 2.302233]\n",
      "1006 [D loss: 1.057374, acc.: 72.22%] [G loss: 1.548770]\n",
      "1007 [D loss: 0.660808, acc.: 66.67%] [G loss: 2.230424]\n",
      "1008 [D loss: 0.301834, acc.: 88.89%] [G loss: 2.252124]\n",
      "1009 [D loss: 0.500336, acc.: 66.67%] [G loss: 2.738923]\n",
      "1010 [D loss: 0.626221, acc.: 72.22%] [G loss: 1.717647]\n",
      "1011 [D loss: 0.416426, acc.: 77.78%] [G loss: 1.886521]\n",
      "1012 [D loss: 0.410557, acc.: 77.78%] [G loss: 2.676516]\n",
      "1013 [D loss: 0.559273, acc.: 72.22%] [G loss: 1.882572]\n",
      "1014 [D loss: 0.593589, acc.: 61.11%] [G loss: 2.061398]\n",
      "1015 [D loss: 0.566709, acc.: 66.67%] [G loss: 2.127439]\n",
      "1016 [D loss: 0.723888, acc.: 61.11%] [G loss: 2.273314]\n",
      "1017 [D loss: 0.274887, acc.: 88.89%] [G loss: 3.154786]\n",
      "1018 [D loss: 0.437348, acc.: 88.89%] [G loss: 2.178290]\n",
      "1019 [D loss: 0.788361, acc.: 61.11%] [G loss: 1.784572]\n",
      "1020 [D loss: 0.851527, acc.: 55.56%] [G loss: 1.947868]\n",
      "1021 [D loss: 0.938462, acc.: 61.11%] [G loss: 1.997311]\n",
      "1022 [D loss: 0.642739, acc.: 72.22%] [G loss: 2.413504]\n",
      "1023 [D loss: 0.784348, acc.: 72.22%] [G loss: 1.421116]\n",
      "1024 [D loss: 0.523918, acc.: 66.67%] [G loss: 2.378175]\n",
      "1025 [D loss: 0.863614, acc.: 77.78%] [G loss: 1.658195]\n",
      "1026 [D loss: 0.481807, acc.: 72.22%] [G loss: 1.775296]\n",
      "1027 [D loss: 0.390935, acc.: 83.33%] [G loss: 1.516808]\n",
      "1028 [D loss: 0.520744, acc.: 72.22%] [G loss: 1.267878]\n",
      "1029 [D loss: 1.240793, acc.: 50.00%] [G loss: 2.190774]\n",
      "1030 [D loss: 0.638021, acc.: 72.22%] [G loss: 1.763376]\n",
      "1031 [D loss: 0.580285, acc.: 72.22%] [G loss: 2.013970]\n",
      "1032 [D loss: 0.460393, acc.: 77.78%] [G loss: 1.742237]\n",
      "1033 [D loss: 0.543765, acc.: 72.22%] [G loss: 2.934438]\n",
      "1034 [D loss: 0.886531, acc.: 61.11%] [G loss: 1.720387]\n",
      "1035 [D loss: 0.336142, acc.: 83.33%] [G loss: 1.512799]\n",
      "1036 [D loss: 0.390090, acc.: 77.78%] [G loss: 2.264241]\n",
      "1037 [D loss: 0.610486, acc.: 66.67%] [G loss: 2.177518]\n",
      "1038 [D loss: 1.032473, acc.: 50.00%] [G loss: 1.022070]\n",
      "1039 [D loss: 0.300247, acc.: 94.44%] [G loss: 2.081561]\n",
      "1040 [D loss: 0.568080, acc.: 72.22%] [G loss: 1.783871]\n",
      "1041 [D loss: 0.624844, acc.: 77.78%] [G loss: 2.068870]\n",
      "1042 [D loss: 0.766884, acc.: 55.56%] [G loss: 1.380660]\n",
      "1043 [D loss: 0.637223, acc.: 72.22%] [G loss: 2.363824]\n",
      "1044 [D loss: 0.429664, acc.: 83.33%] [G loss: 3.198365]\n",
      "1045 [D loss: 0.456895, acc.: 83.33%] [G loss: 2.449340]\n",
      "1046 [D loss: 0.602589, acc.: 66.67%] [G loss: 2.928833]\n",
      "1047 [D loss: 0.165268, acc.: 94.44%] [G loss: 3.407494]\n",
      "1048 [D loss: 0.500798, acc.: 72.22%] [G loss: 2.358203]\n",
      "1049 [D loss: 0.579150, acc.: 66.67%] [G loss: 2.275060]\n",
      "1050 [D loss: 0.511984, acc.: 77.78%] [G loss: 3.588525]\n",
      "1051 [D loss: 0.566487, acc.: 72.22%] [G loss: 2.023075]\n",
      "1052 [D loss: 0.702692, acc.: 66.67%] [G loss: 1.695449]\n",
      "1053 [D loss: 0.615855, acc.: 72.22%] [G loss: 2.785198]\n",
      "1054 [D loss: 0.643620, acc.: 72.22%] [G loss: 1.929426]\n",
      "1055 [D loss: 0.487828, acc.: 77.78%] [G loss: 2.837218]\n",
      "1056 [D loss: 0.545503, acc.: 66.67%] [G loss: 2.198922]\n",
      "1057 [D loss: 0.293523, acc.: 88.89%] [G loss: 2.512532]\n",
      "1058 [D loss: 0.645655, acc.: 66.67%] [G loss: 2.941083]\n",
      "1059 [D loss: 1.055743, acc.: 50.00%] [G loss: 1.707111]\n",
      "1060 [D loss: 0.672514, acc.: 55.56%] [G loss: 2.163239]\n",
      "1061 [D loss: 0.433823, acc.: 72.22%] [G loss: 2.024164]\n",
      "1062 [D loss: 0.679730, acc.: 66.67%] [G loss: 2.490546]\n",
      "1063 [D loss: 0.900394, acc.: 72.22%] [G loss: 2.200498]\n",
      "1064 [D loss: 0.129215, acc.: 100.00%] [G loss: 3.492874]\n",
      "1065 [D loss: 0.420760, acc.: 77.78%] [G loss: 2.577432]\n",
      "1066 [D loss: 0.372118, acc.: 77.78%] [G loss: 2.651287]\n",
      "1067 [D loss: 0.228954, acc.: 88.89%] [G loss: 2.873210]\n",
      "1068 [D loss: 0.155395, acc.: 94.44%] [G loss: 2.888735]\n",
      "1069 [D loss: 0.288682, acc.: 83.33%] [G loss: 1.602182]\n",
      "1070 [D loss: 0.425786, acc.: 77.78%] [G loss: 1.394321]\n",
      "1071 [D loss: 0.173197, acc.: 94.44%] [G loss: 2.398023]\n",
      "1072 [D loss: 0.339153, acc.: 83.33%] [G loss: 2.992475]\n",
      "1073 [D loss: 0.149139, acc.: 100.00%] [G loss: 3.474070]\n",
      "1074 [D loss: 0.183482, acc.: 94.44%] [G loss: 3.291783]\n",
      "1075 [D loss: 0.418338, acc.: 83.33%] [G loss: 2.423447]\n",
      "1076 [D loss: 0.254997, acc.: 88.89%] [G loss: 2.906603]\n",
      "1077 [D loss: 0.368062, acc.: 88.89%] [G loss: 2.504101]\n",
      "1078 [D loss: 0.332205, acc.: 88.89%] [G loss: 2.919072]\n",
      "1079 [D loss: 0.159322, acc.: 100.00%] [G loss: 3.676050]\n",
      "1080 [D loss: 0.563431, acc.: 61.11%] [G loss: 3.475780]\n",
      "1081 [D loss: 0.197874, acc.: 94.44%] [G loss: 3.306286]\n",
      "1082 [D loss: 0.542396, acc.: 77.78%] [G loss: 2.212829]\n",
      "1083 [D loss: 0.423487, acc.: 77.78%] [G loss: 3.943486]\n",
      "1084 [D loss: 1.011878, acc.: 44.44%] [G loss: 1.766035]\n",
      "1085 [D loss: 0.721078, acc.: 66.67%] [G loss: 1.423026]\n",
      "1086 [D loss: 0.398774, acc.: 83.33%] [G loss: 1.751822]\n",
      "1087 [D loss: 0.609853, acc.: 77.78%] [G loss: 1.937411]\n",
      "1088 [D loss: 1.004954, acc.: 72.22%] [G loss: 2.148655]\n",
      "1089 [D loss: 0.562401, acc.: 72.22%] [G loss: 3.011779]\n",
      "1090 [D loss: 0.754213, acc.: 55.56%] [G loss: 2.942532]\n",
      "1091 [D loss: 0.662671, acc.: 72.22%] [G loss: 2.851241]\n",
      "1092 [D loss: 0.794140, acc.: 61.11%] [G loss: 3.320912]\n",
      "1093 [D loss: 0.339887, acc.: 77.78%] [G loss: 2.639618]\n",
      "1094 [D loss: 0.386795, acc.: 88.89%] [G loss: 3.379769]\n",
      "1095 [D loss: 0.184459, acc.: 94.44%] [G loss: 3.954825]\n",
      "1096 [D loss: 0.456229, acc.: 72.22%] [G loss: 2.224759]\n",
      "1097 [D loss: 0.236306, acc.: 100.00%] [G loss: 2.289496]\n",
      "1098 [D loss: 0.361488, acc.: 88.89%] [G loss: 2.009107]\n",
      "1099 [D loss: 0.275408, acc.: 83.33%] [G loss: 3.257815]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100 [D loss: 0.227252, acc.: 83.33%] [G loss: 3.056751]\n",
      "1101 [D loss: 0.260190, acc.: 88.89%] [G loss: 2.502314]\n",
      "1102 [D loss: 0.244373, acc.: 88.89%] [G loss: 6.102413]\n",
      "1103 [D loss: 0.131717, acc.: 100.00%] [G loss: 5.077063]\n",
      "1104 [D loss: 0.103762, acc.: 100.00%] [G loss: 5.649890]\n",
      "1105 [D loss: 0.170151, acc.: 83.33%] [G loss: 4.599047]\n",
      "1106 [D loss: 0.080423, acc.: 100.00%] [G loss: 3.186666]\n",
      "1107 [D loss: 0.070395, acc.: 100.00%] [G loss: 2.437166]\n",
      "1108 [D loss: 0.298117, acc.: 83.33%] [G loss: 2.860711]\n",
      "1109 [D loss: 0.140169, acc.: 94.44%] [G loss: 3.440260]\n",
      "1110 [D loss: 0.124297, acc.: 100.00%] [G loss: 3.182799]\n",
      "1111 [D loss: 0.171466, acc.: 94.44%] [G loss: 2.448345]\n",
      "1112 [D loss: 0.323211, acc.: 88.89%] [G loss: 1.999066]\n",
      "1113 [D loss: 0.364960, acc.: 83.33%] [G loss: 2.570107]\n",
      "1114 [D loss: 0.424721, acc.: 77.78%] [G loss: 3.532847]\n",
      "1115 [D loss: 0.507141, acc.: 77.78%] [G loss: 3.429864]\n",
      "1116 [D loss: 0.469463, acc.: 77.78%] [G loss: 2.605648]\n",
      "1117 [D loss: 0.567260, acc.: 72.22%] [G loss: 3.438339]\n",
      "1118 [D loss: 0.655353, acc.: 66.67%] [G loss: 3.317097]\n",
      "1119 [D loss: 0.423944, acc.: 83.33%] [G loss: 2.215818]\n",
      "1120 [D loss: 0.311098, acc.: 83.33%] [G loss: 2.667703]\n",
      "1121 [D loss: 0.805463, acc.: 55.56%] [G loss: 3.430230]\n",
      "1122 [D loss: 0.744041, acc.: 66.67%] [G loss: 2.644169]\n",
      "1123 [D loss: 0.587049, acc.: 72.22%] [G loss: 1.917138]\n",
      "1124 [D loss: 0.424738, acc.: 72.22%] [G loss: 2.048547]\n",
      "1125 [D loss: 0.527459, acc.: 83.33%] [G loss: 3.184182]\n",
      "1126 [D loss: 0.501223, acc.: 72.22%] [G loss: 2.377760]\n",
      "1127 [D loss: 1.037089, acc.: 55.56%] [G loss: 2.215960]\n",
      "1128 [D loss: 0.399431, acc.: 88.89%] [G loss: 2.307881]\n",
      "1129 [D loss: 0.438678, acc.: 83.33%] [G loss: 2.618962]\n",
      "1130 [D loss: 0.793181, acc.: 66.67%] [G loss: 1.875099]\n",
      "1131 [D loss: 0.848072, acc.: 61.11%] [G loss: 2.699363]\n",
      "1132 [D loss: 0.733724, acc.: 61.11%] [G loss: 2.602899]\n",
      "1133 [D loss: 0.254025, acc.: 88.89%] [G loss: 2.031499]\n",
      "1134 [D loss: 1.076291, acc.: 55.56%] [G loss: 2.252611]\n",
      "1135 [D loss: 0.438630, acc.: 77.78%] [G loss: 2.325118]\n",
      "1136 [D loss: 1.056219, acc.: 50.00%] [G loss: 2.001437]\n",
      "1137 [D loss: 0.768045, acc.: 61.11%] [G loss: 2.312559]\n",
      "1138 [D loss: 0.532277, acc.: 77.78%] [G loss: 1.970137]\n",
      "1139 [D loss: 0.572385, acc.: 66.67%] [G loss: 2.468341]\n",
      "1140 [D loss: 0.654938, acc.: 66.67%] [G loss: 2.210674]\n",
      "1141 [D loss: 0.383849, acc.: 88.89%] [G loss: 1.563398]\n",
      "1142 [D loss: 0.463017, acc.: 77.78%] [G loss: 3.165552]\n",
      "1143 [D loss: 0.361824, acc.: 88.89%] [G loss: 2.851471]\n",
      "1144 [D loss: 0.365151, acc.: 83.33%] [G loss: 1.927031]\n",
      "1145 [D loss: 0.544703, acc.: 77.78%] [G loss: 2.108075]\n",
      "1146 [D loss: 0.606071, acc.: 66.67%] [G loss: 2.364944]\n",
      "1147 [D loss: 0.633678, acc.: 72.22%] [G loss: 3.389800]\n",
      "1148 [D loss: 0.592043, acc.: 77.78%] [G loss: 2.098534]\n",
      "1149 [D loss: 0.547170, acc.: 72.22%] [G loss: 2.220562]\n",
      "1150 [D loss: 0.441758, acc.: 83.33%] [G loss: 1.998656]\n",
      "1151 [D loss: 0.217466, acc.: 94.44%] [G loss: 2.569683]\n",
      "1152 [D loss: 0.428826, acc.: 83.33%] [G loss: 2.243775]\n",
      "1153 [D loss: 0.656357, acc.: 61.11%] [G loss: 2.188294]\n",
      "1154 [D loss: 0.362907, acc.: 88.89%] [G loss: 2.081328]\n",
      "1155 [D loss: 0.484867, acc.: 77.78%] [G loss: 2.062339]\n",
      "1156 [D loss: 1.054578, acc.: 55.56%] [G loss: 2.754439]\n",
      "1157 [D loss: 0.673387, acc.: 66.67%] [G loss: 1.596784]\n",
      "1158 [D loss: 0.455240, acc.: 77.78%] [G loss: 1.981204]\n",
      "1159 [D loss: 0.495918, acc.: 72.22%] [G loss: 3.088085]\n",
      "1160 [D loss: 0.527066, acc.: 72.22%] [G loss: 2.405418]\n",
      "1161 [D loss: 0.872385, acc.: 55.56%] [G loss: 1.936267]\n",
      "1162 [D loss: 0.494778, acc.: 83.33%] [G loss: 1.322171]\n",
      "1163 [D loss: 1.053768, acc.: 44.44%] [G loss: 2.217675]\n",
      "1164 [D loss: 0.301694, acc.: 83.33%] [G loss: 1.862666]\n",
      "1165 [D loss: 0.516119, acc.: 77.78%] [G loss: 2.011140]\n",
      "1166 [D loss: 0.866617, acc.: 44.44%] [G loss: 1.219013]\n",
      "1167 [D loss: 1.103026, acc.: 44.44%] [G loss: 1.791195]\n",
      "1168 [D loss: 0.840223, acc.: 61.11%] [G loss: 2.329180]\n",
      "1169 [D loss: 0.362059, acc.: 88.89%] [G loss: 3.776619]\n",
      "1170 [D loss: 1.024483, acc.: 38.89%] [G loss: 1.420260]\n",
      "1171 [D loss: 0.522607, acc.: 83.33%] [G loss: 2.200437]\n",
      "1172 [D loss: 0.653728, acc.: 77.78%] [G loss: 2.121041]\n",
      "1173 [D loss: 0.492154, acc.: 72.22%] [G loss: 2.405402]\n",
      "1174 [D loss: 0.613313, acc.: 66.67%] [G loss: 2.125283]\n",
      "1175 [D loss: 0.777386, acc.: 55.56%] [G loss: 1.663095]\n",
      "1176 [D loss: 0.616666, acc.: 66.67%] [G loss: 2.444998]\n",
      "1177 [D loss: 0.372701, acc.: 83.33%] [G loss: 1.453435]\n",
      "1178 [D loss: 0.680852, acc.: 72.22%] [G loss: 2.001990]\n",
      "1179 [D loss: 0.219297, acc.: 88.89%] [G loss: 2.413487]\n",
      "1180 [D loss: 0.395400, acc.: 77.78%] [G loss: 3.450130]\n",
      "1181 [D loss: 0.338888, acc.: 77.78%] [G loss: 2.481553]\n",
      "1182 [D loss: 0.495905, acc.: 72.22%] [G loss: 1.440174]\n",
      "1183 [D loss: 0.954287, acc.: 61.11%] [G loss: 1.483635]\n",
      "1184 [D loss: 0.299968, acc.: 88.89%] [G loss: 1.965272]\n",
      "1185 [D loss: 0.796350, acc.: 61.11%] [G loss: 1.797305]\n",
      "1186 [D loss: 0.715987, acc.: 66.67%] [G loss: 2.739851]\n",
      "1187 [D loss: 0.439971, acc.: 77.78%] [G loss: 2.095744]\n",
      "1188 [D loss: 0.296736, acc.: 88.89%] [G loss: 3.624471]\n",
      "1189 [D loss: 0.765721, acc.: 66.67%] [G loss: 1.719368]\n",
      "1190 [D loss: 0.503419, acc.: 77.78%] [G loss: 1.638742]\n",
      "1191 [D loss: 0.187798, acc.: 94.44%] [G loss: 2.954453]\n",
      "1192 [D loss: 0.355532, acc.: 83.33%] [G loss: 2.472776]\n",
      "1193 [D loss: 0.311183, acc.: 77.78%] [G loss: 2.372675]\n",
      "1194 [D loss: 0.168025, acc.: 100.00%] [G loss: 3.001803]\n",
      "1195 [D loss: 0.517450, acc.: 72.22%] [G loss: 3.077824]\n",
      "1196 [D loss: 0.320358, acc.: 88.89%] [G loss: 1.810645]\n",
      "1197 [D loss: 0.256971, acc.: 94.44%] [G loss: 2.793617]\n",
      "1198 [D loss: 0.434852, acc.: 83.33%] [G loss: 2.654061]\n",
      "1199 [D loss: 0.315671, acc.: 83.33%] [G loss: 2.284898]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200 [D loss: 0.488251, acc.: 83.33%] [G loss: 2.345232]\n",
      "1201 [D loss: 0.362058, acc.: 77.78%] [G loss: 2.367537]\n",
      "1202 [D loss: 0.587797, acc.: 72.22%] [G loss: 1.956707]\n",
      "1203 [D loss: 0.603617, acc.: 77.78%] [G loss: 2.230062]\n",
      "1204 [D loss: 0.983478, acc.: 61.11%] [G loss: 2.220120]\n",
      "1205 [D loss: 0.648111, acc.: 61.11%] [G loss: 2.042165]\n",
      "1206 [D loss: 0.593267, acc.: 66.67%] [G loss: 2.314799]\n",
      "1207 [D loss: 0.410824, acc.: 83.33%] [G loss: 2.946692]\n",
      "1208 [D loss: 0.387595, acc.: 83.33%] [G loss: 2.635383]\n",
      "1209 [D loss: 0.285582, acc.: 88.89%] [G loss: 2.488675]\n",
      "1210 [D loss: 1.034185, acc.: 55.56%] [G loss: 2.533767]\n",
      "1211 [D loss: 0.267535, acc.: 88.89%] [G loss: 3.105127]\n",
      "1212 [D loss: 1.090618, acc.: 61.11%] [G loss: 2.062612]\n",
      "1213 [D loss: 0.632252, acc.: 61.11%] [G loss: 2.786616]\n",
      "1214 [D loss: 0.251779, acc.: 88.89%] [G loss: 2.300272]\n",
      "1215 [D loss: 0.290571, acc.: 77.78%] [G loss: 2.279128]\n",
      "1216 [D loss: 0.497022, acc.: 66.67%] [G loss: 2.443506]\n",
      "1217 [D loss: 0.479894, acc.: 77.78%] [G loss: 3.366277]\n",
      "1218 [D loss: 0.435733, acc.: 88.89%] [G loss: 2.135698]\n",
      "1219 [D loss: 1.146840, acc.: 44.44%] [G loss: 1.019955]\n",
      "1220 [D loss: 0.495172, acc.: 77.78%] [G loss: 2.201656]\n",
      "1221 [D loss: 0.595073, acc.: 77.78%] [G loss: 1.725514]\n",
      "1222 [D loss: 0.985185, acc.: 61.11%] [G loss: 2.295538]\n",
      "1223 [D loss: 0.630459, acc.: 66.67%] [G loss: 2.687977]\n",
      "1224 [D loss: 0.335735, acc.: 88.89%] [G loss: 2.410490]\n",
      "1225 [D loss: 0.504404, acc.: 72.22%] [G loss: 2.508366]\n",
      "1226 [D loss: 0.654330, acc.: 61.11%] [G loss: 2.475016]\n",
      "1227 [D loss: 0.809654, acc.: 55.56%] [G loss: 2.021861]\n",
      "1228 [D loss: 0.807419, acc.: 66.67%] [G loss: 1.574954]\n",
      "1229 [D loss: 0.357770, acc.: 88.89%] [G loss: 2.809474]\n",
      "1230 [D loss: 0.476484, acc.: 77.78%] [G loss: 1.734512]\n",
      "1231 [D loss: 0.396904, acc.: 83.33%] [G loss: 2.208128]\n",
      "1232 [D loss: 1.263560, acc.: 38.89%] [G loss: 2.412182]\n",
      "1233 [D loss: 0.511519, acc.: 83.33%] [G loss: 2.331769]\n",
      "1234 [D loss: 0.481448, acc.: 83.33%] [G loss: 2.568101]\n",
      "1235 [D loss: 0.428535, acc.: 77.78%] [G loss: 2.335971]\n",
      "1236 [D loss: 0.440327, acc.: 72.22%] [G loss: 2.280359]\n",
      "1237 [D loss: 0.423101, acc.: 88.89%] [G loss: 1.972940]\n",
      "1238 [D loss: 0.933900, acc.: 50.00%] [G loss: 1.982274]\n",
      "1239 [D loss: 0.328096, acc.: 83.33%] [G loss: 1.972815]\n",
      "1240 [D loss: 0.403944, acc.: 88.89%] [G loss: 3.132233]\n",
      "1241 [D loss: 0.774162, acc.: 72.22%] [G loss: 2.122883]\n",
      "1242 [D loss: 1.585364, acc.: 27.78%] [G loss: 1.869794]\n",
      "1243 [D loss: 0.486869, acc.: 77.78%] [G loss: 2.566164]\n",
      "1244 [D loss: 0.617928, acc.: 72.22%] [G loss: 1.567295]\n",
      "1245 [D loss: 0.970625, acc.: 55.56%] [G loss: 1.997435]\n",
      "1246 [D loss: 0.391829, acc.: 77.78%] [G loss: 2.401515]\n",
      "1247 [D loss: 0.519474, acc.: 72.22%] [G loss: 2.167924]\n",
      "1248 [D loss: 0.502026, acc.: 83.33%] [G loss: 2.631979]\n",
      "1249 [D loss: 0.231134, acc.: 94.44%] [G loss: 2.723435]\n",
      "1250 [D loss: 0.643023, acc.: 72.22%] [G loss: 2.474678]\n",
      "1251 [D loss: 0.385494, acc.: 83.33%] [G loss: 2.214035]\n",
      "1252 [D loss: 0.511043, acc.: 61.11%] [G loss: 2.860041]\n",
      "1253 [D loss: 1.085884, acc.: 44.44%] [G loss: 2.167185]\n",
      "1254 [D loss: 0.429146, acc.: 88.89%] [G loss: 2.027695]\n",
      "1255 [D loss: 0.570346, acc.: 66.67%] [G loss: 2.641202]\n",
      "1256 [D loss: 0.652726, acc.: 61.11%] [G loss: 2.413733]\n",
      "1257 [D loss: 1.056319, acc.: 61.11%] [G loss: 2.187661]\n",
      "1258 [D loss: 0.518603, acc.: 77.78%] [G loss: 1.511524]\n",
      "1259 [D loss: 0.781739, acc.: 77.78%] [G loss: 1.834307]\n",
      "1260 [D loss: 0.980229, acc.: 61.11%] [G loss: 1.632777]\n",
      "1261 [D loss: 1.040130, acc.: 50.00%] [G loss: 2.118155]\n",
      "1262 [D loss: 0.616217, acc.: 72.22%] [G loss: 2.893590]\n",
      "1263 [D loss: 0.468690, acc.: 72.22%] [G loss: 0.964108]\n",
      "1264 [D loss: 0.470739, acc.: 77.78%] [G loss: 1.628858]\n",
      "1265 [D loss: 0.369132, acc.: 77.78%] [G loss: 2.612242]\n",
      "1266 [D loss: 0.740832, acc.: 55.56%] [G loss: 3.230557]\n",
      "1267 [D loss: 0.539001, acc.: 72.22%] [G loss: 1.674795]\n",
      "1268 [D loss: 0.814728, acc.: 55.56%] [G loss: 1.827628]\n",
      "1269 [D loss: 0.693155, acc.: 72.22%] [G loss: 2.390996]\n",
      "1270 [D loss: 0.407125, acc.: 77.78%] [G loss: 1.955478]\n",
      "1271 [D loss: 0.978381, acc.: 55.56%] [G loss: 1.300444]\n",
      "1272 [D loss: 0.593359, acc.: 66.67%] [G loss: 2.666402]\n",
      "1273 [D loss: 0.589371, acc.: 66.67%] [G loss: 2.434130]\n",
      "1274 [D loss: 0.374121, acc.: 83.33%] [G loss: 1.363183]\n",
      "1275 [D loss: 0.904356, acc.: 55.56%] [G loss: 2.153791]\n",
      "1276 [D loss: 0.945779, acc.: 38.89%] [G loss: 2.316158]\n",
      "1277 [D loss: 0.724907, acc.: 66.67%] [G loss: 2.949487]\n",
      "1278 [D loss: 1.112209, acc.: 55.56%] [G loss: 2.386823]\n",
      "1279 [D loss: 0.427535, acc.: 83.33%] [G loss: 2.593682]\n",
      "1280 [D loss: 0.308609, acc.: 88.89%] [G loss: 2.580085]\n",
      "1281 [D loss: 0.533815, acc.: 72.22%] [G loss: 2.733291]\n",
      "1282 [D loss: 0.415890, acc.: 83.33%] [G loss: 2.336091]\n",
      "1283 [D loss: 0.401259, acc.: 88.89%] [G loss: 2.335711]\n",
      "1284 [D loss: 0.403382, acc.: 77.78%] [G loss: 2.472384]\n",
      "1285 [D loss: 0.234741, acc.: 94.44%] [G loss: 3.322011]\n",
      "1286 [D loss: 0.675119, acc.: 55.56%] [G loss: 2.079579]\n",
      "1287 [D loss: 0.743505, acc.: 44.44%] [G loss: 2.069268]\n",
      "1288 [D loss: 1.142134, acc.: 50.00%] [G loss: 1.826215]\n",
      "1289 [D loss: 0.731491, acc.: 72.22%] [G loss: 2.561541]\n",
      "1290 [D loss: 0.765074, acc.: 55.56%] [G loss: 1.555656]\n",
      "1291 [D loss: 0.463160, acc.: 83.33%] [G loss: 2.285151]\n",
      "1292 [D loss: 0.370280, acc.: 83.33%] [G loss: 2.916600]\n",
      "1293 [D loss: 0.536162, acc.: 72.22%] [G loss: 2.612967]\n",
      "1294 [D loss: 0.698529, acc.: 66.67%] [G loss: 1.977667]\n",
      "1295 [D loss: 0.626679, acc.: 72.22%] [G loss: 2.001538]\n",
      "1296 [D loss: 0.619434, acc.: 77.78%] [G loss: 2.704998]\n",
      "1297 [D loss: 0.945088, acc.: 50.00%] [G loss: 2.346414]\n",
      "1298 [D loss: 0.479224, acc.: 72.22%] [G loss: 2.405663]\n",
      "1299 [D loss: 0.604665, acc.: 66.67%] [G loss: 1.720391]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300 [D loss: 0.524971, acc.: 72.22%] [G loss: 1.576638]\n",
      "1301 [D loss: 0.090895, acc.: 100.00%] [G loss: 3.461535]\n",
      "1302 [D loss: 0.656855, acc.: 83.33%] [G loss: 3.818404]\n",
      "1303 [D loss: 1.029042, acc.: 55.56%] [G loss: 1.582430]\n",
      "1304 [D loss: 0.495433, acc.: 77.78%] [G loss: 2.304944]\n",
      "1305 [D loss: 1.082171, acc.: 55.56%] [G loss: 1.731044]\n",
      "1306 [D loss: 0.484230, acc.: 72.22%] [G loss: 1.592087]\n",
      "1307 [D loss: 0.444694, acc.: 72.22%] [G loss: 2.032742]\n",
      "1308 [D loss: 0.765871, acc.: 66.67%] [G loss: 1.542265]\n",
      "1309 [D loss: 0.553408, acc.: 77.78%] [G loss: 1.958479]\n",
      "1310 [D loss: 0.509279, acc.: 72.22%] [G loss: 1.865625]\n",
      "1311 [D loss: 0.239427, acc.: 100.00%] [G loss: 3.083460]\n",
      "1312 [D loss: 0.867186, acc.: 61.11%] [G loss: 2.649681]\n",
      "1313 [D loss: 1.312892, acc.: 44.44%] [G loss: 1.948323]\n",
      "1314 [D loss: 0.625489, acc.: 66.67%] [G loss: 1.608485]\n",
      "1315 [D loss: 0.330321, acc.: 88.89%] [G loss: 3.103893]\n",
      "1316 [D loss: 0.727181, acc.: 66.67%] [G loss: 2.582321]\n",
      "1317 [D loss: 0.435068, acc.: 88.89%] [G loss: 1.662748]\n",
      "1318 [D loss: 0.477144, acc.: 83.33%] [G loss: 2.673503]\n",
      "1319 [D loss: 0.892138, acc.: 61.11%] [G loss: 2.792431]\n",
      "1320 [D loss: 0.447479, acc.: 77.78%] [G loss: 1.659441]\n",
      "1321 [D loss: 0.788160, acc.: 50.00%] [G loss: 3.063449]\n",
      "1322 [D loss: 0.745537, acc.: 72.22%] [G loss: 2.323918]\n",
      "1323 [D loss: 0.593848, acc.: 83.33%] [G loss: 1.883362]\n",
      "1324 [D loss: 0.349847, acc.: 94.44%] [G loss: 2.156766]\n",
      "1325 [D loss: 0.538186, acc.: 72.22%] [G loss: 1.627479]\n",
      "1326 [D loss: 0.546314, acc.: 66.67%] [G loss: 2.043263]\n",
      "1327 [D loss: 0.639898, acc.: 61.11%] [G loss: 2.929668]\n",
      "1328 [D loss: 0.516051, acc.: 83.33%] [G loss: 2.236232]\n",
      "1329 [D loss: 0.648134, acc.: 77.78%] [G loss: 1.651793]\n",
      "1330 [D loss: 0.545401, acc.: 77.78%] [G loss: 1.417462]\n",
      "1331 [D loss: 1.009304, acc.: 66.67%] [G loss: 1.541327]\n",
      "1332 [D loss: 0.983869, acc.: 55.56%] [G loss: 2.021443]\n",
      "1333 [D loss: 0.891940, acc.: 66.67%] [G loss: 2.336737]\n",
      "1334 [D loss: 0.316300, acc.: 88.89%] [G loss: 2.453273]\n",
      "1335 [D loss: 0.532932, acc.: 72.22%] [G loss: 2.590065]\n",
      "1336 [D loss: 0.513379, acc.: 72.22%] [G loss: 2.309042]\n",
      "1337 [D loss: 0.695754, acc.: 72.22%] [G loss: 1.849237]\n",
      "1338 [D loss: 0.676348, acc.: 66.67%] [G loss: 2.103007]\n",
      "1339 [D loss: 1.027389, acc.: 55.56%] [G loss: 1.513923]\n",
      "1340 [D loss: 0.519189, acc.: 83.33%] [G loss: 2.670493]\n",
      "1341 [D loss: 0.671758, acc.: 61.11%] [G loss: 2.448416]\n",
      "1342 [D loss: 0.436335, acc.: 77.78%] [G loss: 2.041695]\n",
      "1343 [D loss: 0.596542, acc.: 66.67%] [G loss: 2.482326]\n",
      "1344 [D loss: 0.683064, acc.: 77.78%] [G loss: 2.294343]\n",
      "1345 [D loss: 0.845411, acc.: 55.56%] [G loss: 2.274034]\n",
      "1346 [D loss: 0.292480, acc.: 88.89%] [G loss: 3.114652]\n",
      "1347 [D loss: 0.348694, acc.: 83.33%] [G loss: 3.217995]\n",
      "1348 [D loss: 0.825123, acc.: 66.67%] [G loss: 2.257154]\n",
      "1349 [D loss: 0.513233, acc.: 66.67%] [G loss: 1.928601]\n",
      "1350 [D loss: 0.401586, acc.: 83.33%] [G loss: 2.057977]\n",
      "1351 [D loss: 0.615749, acc.: 66.67%] [G loss: 2.216613]\n",
      "1352 [D loss: 0.972762, acc.: 66.67%] [G loss: 2.653516]\n",
      "1353 [D loss: 0.780941, acc.: 72.22%] [G loss: 2.410741]\n",
      "1354 [D loss: 0.768609, acc.: 66.67%] [G loss: 2.583736]\n",
      "1355 [D loss: 0.492891, acc.: 72.22%] [G loss: 2.586721]\n",
      "1356 [D loss: 0.647225, acc.: 66.67%] [G loss: 1.762726]\n",
      "1357 [D loss: 0.545548, acc.: 77.78%] [G loss: 2.550230]\n",
      "1358 [D loss: 0.828145, acc.: 72.22%] [G loss: 2.617024]\n",
      "1359 [D loss: 0.708283, acc.: 61.11%] [G loss: 1.833623]\n",
      "1360 [D loss: 0.870573, acc.: 61.11%] [G loss: 1.916446]\n",
      "1361 [D loss: 0.532479, acc.: 66.67%] [G loss: 2.713950]\n",
      "1362 [D loss: 0.449940, acc.: 83.33%] [G loss: 2.193816]\n",
      "1363 [D loss: 0.703245, acc.: 72.22%] [G loss: 1.390342]\n",
      "1364 [D loss: 0.790895, acc.: 55.56%] [G loss: 2.092858]\n",
      "1365 [D loss: 0.350984, acc.: 83.33%] [G loss: 2.845128]\n",
      "1366 [D loss: 0.620773, acc.: 66.67%] [G loss: 3.127168]\n",
      "1367 [D loss: 0.321554, acc.: 88.89%] [G loss: 2.697346]\n",
      "1368 [D loss: 0.601468, acc.: 72.22%] [G loss: 1.854665]\n",
      "1369 [D loss: 0.507205, acc.: 77.78%] [G loss: 2.554002]\n",
      "1370 [D loss: 0.474605, acc.: 77.78%] [G loss: 2.249370]\n",
      "1371 [D loss: 0.209114, acc.: 94.44%] [G loss: 2.593985]\n",
      "1372 [D loss: 0.391185, acc.: 83.33%] [G loss: 1.666435]\n",
      "1373 [D loss: 0.903837, acc.: 50.00%] [G loss: 1.031162]\n",
      "1374 [D loss: 0.634997, acc.: 72.22%] [G loss: 2.063948]\n",
      "1375 [D loss: 0.479860, acc.: 77.78%] [G loss: 2.125060]\n",
      "1376 [D loss: 0.459602, acc.: 77.78%] [G loss: 2.173675]\n",
      "1377 [D loss: 0.633068, acc.: 72.22%] [G loss: 2.219747]\n",
      "1378 [D loss: 0.495614, acc.: 77.78%] [G loss: 2.482034]\n",
      "1379 [D loss: 0.234241, acc.: 88.89%] [G loss: 3.651610]\n",
      "1380 [D loss: 0.432112, acc.: 83.33%] [G loss: 2.014905]\n",
      "1381 [D loss: 0.239226, acc.: 88.89%] [G loss: 3.098912]\n",
      "1382 [D loss: 0.693377, acc.: 66.67%] [G loss: 2.430472]\n",
      "1383 [D loss: 0.414076, acc.: 77.78%] [G loss: 2.544563]\n",
      "1384 [D loss: 0.501216, acc.: 72.22%] [G loss: 2.345289]\n",
      "1385 [D loss: 0.786169, acc.: 66.67%] [G loss: 2.125051]\n",
      "1386 [D loss: 0.099997, acc.: 100.00%] [G loss: 7.852715]\n",
      "1387 [D loss: 0.296226, acc.: 77.78%] [G loss: 3.270373]\n",
      "1388 [D loss: 0.147274, acc.: 94.44%] [G loss: 2.911455]\n",
      "1389 [D loss: 0.295036, acc.: 83.33%] [G loss: 2.743621]\n",
      "1390 [D loss: 0.439212, acc.: 72.22%] [G loss: 2.281703]\n",
      "1391 [D loss: 0.102317, acc.: 100.00%] [G loss: 3.575379]\n",
      "1392 [D loss: 0.707554, acc.: 61.11%] [G loss: 2.323370]\n",
      "1393 [D loss: 0.319472, acc.: 83.33%] [G loss: 3.589060]\n",
      "1394 [D loss: 0.329037, acc.: 83.33%] [G loss: 3.528405]\n",
      "1395 [D loss: 0.382425, acc.: 88.89%] [G loss: 3.957127]\n",
      "1396 [D loss: 0.227087, acc.: 94.44%] [G loss: 3.276055]\n",
      "1397 [D loss: 0.182970, acc.: 88.89%] [G loss: 4.288430]\n",
      "1398 [D loss: 0.161033, acc.: 94.44%] [G loss: 3.243489]\n",
      "1399 [D loss: 0.213576, acc.: 88.89%] [G loss: 3.051427]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400 [D loss: 0.218185, acc.: 88.89%] [G loss: 2.442454]\n",
      "1401 [D loss: 0.541390, acc.: 77.78%] [G loss: 2.212459]\n",
      "1402 [D loss: 0.274926, acc.: 88.89%] [G loss: 3.687039]\n",
      "1403 [D loss: 0.495105, acc.: 72.22%] [G loss: 3.979180]\n",
      "1404 [D loss: 0.351184, acc.: 83.33%] [G loss: 2.997963]\n",
      "1405 [D loss: 0.727109, acc.: 77.78%] [G loss: 2.854947]\n",
      "1406 [D loss: 0.318748, acc.: 94.44%] [G loss: 2.959428]\n",
      "1407 [D loss: 0.458670, acc.: 72.22%] [G loss: 2.200658]\n",
      "1408 [D loss: 0.887518, acc.: 77.78%] [G loss: 2.213127]\n",
      "1409 [D loss: 0.554124, acc.: 72.22%] [G loss: 3.187268]\n",
      "1410 [D loss: 0.382256, acc.: 77.78%] [G loss: 3.420327]\n",
      "1411 [D loss: 0.332433, acc.: 83.33%] [G loss: 2.965533]\n",
      "1412 [D loss: 0.527443, acc.: 66.67%] [G loss: 2.028974]\n",
      "1413 [D loss: 0.426289, acc.: 72.22%] [G loss: 3.038787]\n",
      "1414 [D loss: 0.281101, acc.: 94.44%] [G loss: 3.238553]\n",
      "1415 [D loss: 0.209329, acc.: 88.89%] [G loss: 3.614474]\n",
      "1416 [D loss: 0.370714, acc.: 72.22%] [G loss: 2.641678]\n",
      "1417 [D loss: 0.559737, acc.: 83.33%] [G loss: 3.223168]\n",
      "1418 [D loss: 0.206783, acc.: 94.44%] [G loss: 3.777704]\n",
      "1419 [D loss: 1.316785, acc.: 61.11%] [G loss: 1.916984]\n",
      "1420 [D loss: 0.676192, acc.: 72.22%] [G loss: 2.374177]\n",
      "1421 [D loss: 0.376127, acc.: 88.89%] [G loss: 2.555988]\n",
      "1422 [D loss: 0.576936, acc.: 66.67%] [G loss: 1.545183]\n",
      "1423 [D loss: 0.618093, acc.: 77.78%] [G loss: 2.644247]\n",
      "1424 [D loss: 0.516685, acc.: 77.78%] [G loss: 2.203007]\n",
      "1425 [D loss: 0.475699, acc.: 77.78%] [G loss: 1.744336]\n",
      "1426 [D loss: 0.522850, acc.: 77.78%] [G loss: 1.763298]\n",
      "1427 [D loss: 0.490604, acc.: 83.33%] [G loss: 2.432743]\n",
      "1428 [D loss: 0.565736, acc.: 61.11%] [G loss: 2.433706]\n",
      "1429 [D loss: 0.607052, acc.: 66.67%] [G loss: 2.739311]\n",
      "1430 [D loss: 0.628088, acc.: 66.67%] [G loss: 2.696424]\n",
      "1431 [D loss: 0.565500, acc.: 61.11%] [G loss: 2.183376]\n",
      "1432 [D loss: 0.370989, acc.: 77.78%] [G loss: 2.553602]\n",
      "1433 [D loss: 0.340480, acc.: 88.89%] [G loss: 3.889441]\n",
      "1434 [D loss: 0.868088, acc.: 55.56%] [G loss: 2.426184]\n",
      "1435 [D loss: 0.407835, acc.: 83.33%] [G loss: 2.775056]\n",
      "1436 [D loss: 0.456203, acc.: 83.33%] [G loss: 2.761158]\n",
      "1437 [D loss: 0.397951, acc.: 77.78%] [G loss: 2.592219]\n",
      "1438 [D loss: 0.708773, acc.: 61.11%] [G loss: 1.795745]\n",
      "1439 [D loss: 0.225942, acc.: 88.89%] [G loss: 2.644072]\n",
      "1440 [D loss: 0.564571, acc.: 66.67%] [G loss: 2.132890]\n",
      "1441 [D loss: 0.656154, acc.: 66.67%] [G loss: 2.566598]\n",
      "1442 [D loss: 0.488908, acc.: 77.78%] [G loss: 2.512240]\n",
      "1443 [D loss: 0.281946, acc.: 88.89%] [G loss: 2.577172]\n",
      "1444 [D loss: 0.758198, acc.: 66.67%] [G loss: 1.198045]\n",
      "1445 [D loss: 0.535565, acc.: 72.22%] [G loss: 1.756097]\n",
      "1446 [D loss: 0.405081, acc.: 77.78%] [G loss: 2.727948]\n",
      "1447 [D loss: 0.504789, acc.: 77.78%] [G loss: 3.158084]\n",
      "1448 [D loss: 0.804580, acc.: 55.56%] [G loss: 2.197608]\n",
      "1449 [D loss: 0.606010, acc.: 61.11%] [G loss: 2.119124]\n",
      "1450 [D loss: 0.540901, acc.: 83.33%] [G loss: 2.176145]\n",
      "1451 [D loss: 0.421430, acc.: 88.89%] [G loss: 2.021295]\n",
      "1452 [D loss: 0.603988, acc.: 66.67%] [G loss: 2.075144]\n",
      "1453 [D loss: 0.466490, acc.: 66.67%] [G loss: 2.379385]\n",
      "1454 [D loss: 0.529102, acc.: 72.22%] [G loss: 1.705233]\n",
      "1455 [D loss: 0.445244, acc.: 88.89%] [G loss: 2.387380]\n",
      "1456 [D loss: 0.383786, acc.: 94.44%] [G loss: 2.048348]\n",
      "1457 [D loss: 0.276584, acc.: 88.89%] [G loss: 2.301121]\n",
      "1458 [D loss: 0.618660, acc.: 77.78%] [G loss: 1.890245]\n",
      "1459 [D loss: 0.240197, acc.: 94.44%] [G loss: 3.791926]\n",
      "1460 [D loss: 0.740168, acc.: 61.11%] [G loss: 3.142993]\n",
      "1461 [D loss: 0.601306, acc.: 66.67%] [G loss: 2.174016]\n",
      "1462 [D loss: 0.759974, acc.: 72.22%] [G loss: 2.547198]\n",
      "1463 [D loss: 0.714478, acc.: 72.22%] [G loss: 2.152583]\n",
      "1464 [D loss: 0.370776, acc.: 72.22%] [G loss: 2.387317]\n",
      "1465 [D loss: 0.204972, acc.: 88.89%] [G loss: 2.103350]\n",
      "1466 [D loss: 0.411660, acc.: 83.33%] [G loss: 2.250626]\n",
      "1467 [D loss: 0.552345, acc.: 72.22%] [G loss: 2.576543]\n",
      "1468 [D loss: 0.717006, acc.: 77.78%] [G loss: 3.283185]\n",
      "1469 [D loss: 0.462026, acc.: 77.78%] [G loss: 2.167090]\n",
      "1470 [D loss: 0.381117, acc.: 77.78%] [G loss: 2.410845]\n",
      "1471 [D loss: 0.697503, acc.: 77.78%] [G loss: 1.745769]\n",
      "1472 [D loss: 1.036509, acc.: 50.00%] [G loss: 2.284062]\n",
      "1473 [D loss: 0.558300, acc.: 72.22%] [G loss: 2.056409]\n",
      "1474 [D loss: 0.326590, acc.: 94.44%] [G loss: 2.146720]\n",
      "1475 [D loss: 0.444436, acc.: 88.89%] [G loss: 1.850377]\n",
      "1476 [D loss: 0.564331, acc.: 61.11%] [G loss: 1.666215]\n",
      "1477 [D loss: 0.308359, acc.: 83.33%] [G loss: 2.712454]\n",
      "1478 [D loss: 0.632797, acc.: 66.67%] [G loss: 2.276277]\n",
      "1479 [D loss: 0.420678, acc.: 83.33%] [G loss: 1.868906]\n",
      "1480 [D loss: 0.895076, acc.: 55.56%] [G loss: 1.375761]\n",
      "1481 [D loss: 0.567164, acc.: 72.22%] [G loss: 2.622172]\n",
      "1482 [D loss: 0.571778, acc.: 88.89%] [G loss: 3.166734]\n",
      "1483 [D loss: 0.617267, acc.: 61.11%] [G loss: 2.248507]\n",
      "1484 [D loss: 0.624880, acc.: 72.22%] [G loss: 2.073608]\n",
      "1485 [D loss: 0.753942, acc.: 66.67%] [G loss: 3.014139]\n",
      "1486 [D loss: 0.456814, acc.: 72.22%] [G loss: 2.746799]\n",
      "1487 [D loss: 0.281715, acc.: 88.89%] [G loss: 3.393122]\n",
      "1488 [D loss: 0.468231, acc.: 72.22%] [G loss: 1.806745]\n",
      "1489 [D loss: 0.836268, acc.: 66.67%] [G loss: 1.838630]\n",
      "1490 [D loss: 0.109681, acc.: 100.00%] [G loss: 2.303823]\n",
      "1491 [D loss: 0.992948, acc.: 38.89%] [G loss: 2.078387]\n",
      "1492 [D loss: 0.203093, acc.: 100.00%] [G loss: 3.469817]\n",
      "1493 [D loss: 0.798493, acc.: 61.11%] [G loss: 2.155132]\n",
      "1494 [D loss: 0.359983, acc.: 83.33%] [G loss: 2.473193]\n",
      "1495 [D loss: 0.432234, acc.: 77.78%] [G loss: 2.764333]\n",
      "1496 [D loss: 0.409819, acc.: 72.22%] [G loss: 2.242801]\n",
      "1497 [D loss: 0.511363, acc.: 72.22%] [G loss: 2.947523]\n",
      "1498 [D loss: 0.458805, acc.: 72.22%] [G loss: 2.045999]\n",
      "1499 [D loss: 0.379827, acc.: 83.33%] [G loss: 2.479913]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 [D loss: 0.857796, acc.: 61.11%] [G loss: 1.791167]\n",
      "1501 [D loss: 0.276894, acc.: 83.33%] [G loss: 2.717435]\n",
      "1502 [D loss: 0.593697, acc.: 83.33%] [G loss: 2.551131]\n",
      "1503 [D loss: 0.508390, acc.: 72.22%] [G loss: 3.060214]\n",
      "1504 [D loss: 0.851984, acc.: 55.56%] [G loss: 3.309220]\n",
      "1505 [D loss: 0.485173, acc.: 66.67%] [G loss: 3.252485]\n",
      "1506 [D loss: 0.430160, acc.: 83.33%] [G loss: 3.160171]\n",
      "1507 [D loss: 0.385170, acc.: 83.33%] [G loss: 3.542058]\n",
      "1508 [D loss: 0.511944, acc.: 72.22%] [G loss: 2.074938]\n",
      "1509 [D loss: 0.199131, acc.: 94.44%] [G loss: 2.441458]\n",
      "1510 [D loss: 0.327204, acc.: 83.33%] [G loss: 3.498858]\n",
      "1511 [D loss: 0.490752, acc.: 72.22%] [G loss: 3.357363]\n",
      "1512 [D loss: 0.447402, acc.: 83.33%] [G loss: 2.579327]\n",
      "1513 [D loss: 0.572760, acc.: 77.78%] [G loss: 1.646358]\n",
      "1514 [D loss: 0.602397, acc.: 72.22%] [G loss: 2.264400]\n",
      "1515 [D loss: 1.107651, acc.: 61.11%] [G loss: 2.264346]\n",
      "1516 [D loss: 0.707247, acc.: 66.67%] [G loss: 2.366745]\n",
      "1517 [D loss: 0.387031, acc.: 88.89%] [G loss: 2.811434]\n",
      "1518 [D loss: 0.579675, acc.: 72.22%] [G loss: 2.872787]\n",
      "1519 [D loss: 0.879059, acc.: 55.56%] [G loss: 2.241220]\n",
      "1520 [D loss: 0.587091, acc.: 77.78%] [G loss: 2.124094]\n",
      "1521 [D loss: 0.934765, acc.: 55.56%] [G loss: 2.372887]\n",
      "1522 [D loss: 0.327527, acc.: 83.33%] [G loss: 2.542226]\n",
      "1523 [D loss: 0.501984, acc.: 66.67%] [G loss: 2.671613]\n",
      "1524 [D loss: 0.569219, acc.: 66.67%] [G loss: 2.296973]\n",
      "1525 [D loss: 0.699957, acc.: 72.22%] [G loss: 2.378615]\n",
      "1526 [D loss: 0.496366, acc.: 72.22%] [G loss: 2.100738]\n",
      "1527 [D loss: 0.390181, acc.: 77.78%] [G loss: 3.271225]\n",
      "1528 [D loss: 0.591759, acc.: 66.67%] [G loss: 2.714858]\n",
      "1529 [D loss: 0.237234, acc.: 88.89%] [G loss: 2.253155]\n",
      "1530 [D loss: 0.802130, acc.: 50.00%] [G loss: 1.431094]\n",
      "1531 [D loss: 0.970147, acc.: 55.56%] [G loss: 3.746029]\n",
      "1532 [D loss: 0.398386, acc.: 77.78%] [G loss: 3.072308]\n",
      "1533 [D loss: 0.662593, acc.: 66.67%] [G loss: 2.868270]\n",
      "1534 [D loss: 0.270818, acc.: 88.89%] [G loss: 2.408440]\n",
      "1535 [D loss: 0.538853, acc.: 72.22%] [G loss: 2.794833]\n",
      "1536 [D loss: 0.682168, acc.: 72.22%] [G loss: 3.140829]\n",
      "1537 [D loss: 0.118509, acc.: 100.00%] [G loss: 3.194841]\n",
      "1538 [D loss: 1.104289, acc.: 50.00%] [G loss: 1.752971]\n",
      "1539 [D loss: 0.571640, acc.: 77.78%] [G loss: 2.593703]\n",
      "1540 [D loss: 0.493538, acc.: 66.67%] [G loss: 3.586191]\n",
      "1541 [D loss: 0.134261, acc.: 100.00%] [G loss: 3.227611]\n",
      "1542 [D loss: 0.525697, acc.: 72.22%] [G loss: 1.907070]\n",
      "1543 [D loss: 0.474114, acc.: 83.33%] [G loss: 2.343158]\n",
      "1544 [D loss: 0.445826, acc.: 83.33%] [G loss: 2.011250]\n",
      "1545 [D loss: 0.720195, acc.: 66.67%] [G loss: 2.375448]\n",
      "1546 [D loss: 0.619325, acc.: 72.22%] [G loss: 1.765042]\n",
      "1547 [D loss: 0.576511, acc.: 94.44%] [G loss: 2.968172]\n",
      "1548 [D loss: 0.924474, acc.: 55.56%] [G loss: 1.561953]\n",
      "1549 [D loss: 0.485797, acc.: 72.22%] [G loss: 2.669406]\n",
      "1550 [D loss: 0.722166, acc.: 55.56%] [G loss: 2.335687]\n",
      "1551 [D loss: 0.278646, acc.: 88.89%] [G loss: 3.717686]\n",
      "1552 [D loss: 0.445282, acc.: 83.33%] [G loss: 2.625991]\n",
      "1553 [D loss: 0.451325, acc.: 83.33%] [G loss: 3.419106]\n",
      "1554 [D loss: 0.286244, acc.: 88.89%] [G loss: 2.284794]\n",
      "1555 [D loss: 0.336809, acc.: 88.89%] [G loss: 2.195449]\n",
      "1556 [D loss: 0.616322, acc.: 61.11%] [G loss: 2.159255]\n",
      "1557 [D loss: 0.495700, acc.: 66.67%] [G loss: 3.416463]\n",
      "1558 [D loss: 0.728886, acc.: 72.22%] [G loss: 2.066495]\n",
      "1559 [D loss: 0.979252, acc.: 50.00%] [G loss: 2.161320]\n",
      "1560 [D loss: 0.424648, acc.: 77.78%] [G loss: 3.210652]\n",
      "1561 [D loss: 0.605336, acc.: 66.67%] [G loss: 3.042935]\n",
      "1562 [D loss: 0.451898, acc.: 83.33%] [G loss: 3.326345]\n",
      "1563 [D loss: 0.371878, acc.: 77.78%] [G loss: 4.501484]\n",
      "1564 [D loss: 0.509725, acc.: 88.89%] [G loss: 2.796521]\n",
      "1565 [D loss: 0.130759, acc.: 94.44%] [G loss: 3.300684]\n",
      "1566 [D loss: 0.458251, acc.: 77.78%] [G loss: 2.492274]\n",
      "1567 [D loss: 0.437842, acc.: 83.33%] [G loss: 2.652272]\n",
      "1568 [D loss: 0.197729, acc.: 88.89%] [G loss: 2.409681]\n",
      "1569 [D loss: 0.539514, acc.: 77.78%] [G loss: 3.373972]\n",
      "1570 [D loss: 0.636298, acc.: 72.22%] [G loss: 2.115723]\n",
      "1571 [D loss: 0.614182, acc.: 77.78%] [G loss: 2.442905]\n",
      "1572 [D loss: 0.710204, acc.: 72.22%] [G loss: 1.692790]\n",
      "1573 [D loss: 0.320708, acc.: 88.89%] [G loss: 2.703836]\n",
      "1574 [D loss: 0.304127, acc.: 88.89%] [G loss: 2.887933]\n",
      "1575 [D loss: 0.756654, acc.: 66.67%] [G loss: 2.882186]\n",
      "1576 [D loss: 0.566398, acc.: 72.22%] [G loss: 1.395666]\n",
      "1577 [D loss: 0.409250, acc.: 88.89%] [G loss: 1.723014]\n",
      "1578 [D loss: 0.321705, acc.: 94.44%] [G loss: 3.456207]\n",
      "1579 [D loss: 0.102559, acc.: 100.00%] [G loss: 6.735296]\n",
      "1580 [D loss: 0.110804, acc.: 94.44%] [G loss: 5.746961]\n",
      "1581 [D loss: 0.128472, acc.: 94.44%] [G loss: 6.145262]\n",
      "1582 [D loss: 0.044852, acc.: 100.00%] [G loss: 4.599994]\n",
      "1583 [D loss: 0.310399, acc.: 83.33%] [G loss: 2.328906]\n",
      "1584 [D loss: 0.112992, acc.: 94.44%] [G loss: 4.532499]\n",
      "1585 [D loss: 0.325666, acc.: 88.89%] [G loss: 2.469550]\n",
      "1586 [D loss: 0.327821, acc.: 77.78%] [G loss: 2.798416]\n",
      "1587 [D loss: 0.363483, acc.: 88.89%] [G loss: 2.830659]\n",
      "1588 [D loss: 0.363060, acc.: 83.33%] [G loss: 2.978673]\n",
      "1589 [D loss: 0.613534, acc.: 72.22%] [G loss: 2.208090]\n",
      "1590 [D loss: 0.659067, acc.: 61.11%] [G loss: 2.503609]\n",
      "1591 [D loss: 0.407635, acc.: 77.78%] [G loss: 3.110259]\n",
      "1592 [D loss: 0.379769, acc.: 88.89%] [G loss: 2.561445]\n",
      "1593 [D loss: 0.874836, acc.: 55.56%] [G loss: 2.029057]\n",
      "1594 [D loss: 0.192040, acc.: 94.44%] [G loss: 2.990795]\n",
      "1595 [D loss: 0.557529, acc.: 83.33%] [G loss: 3.610507]\n",
      "1596 [D loss: 0.849005, acc.: 66.67%] [G loss: 2.924875]\n",
      "1597 [D loss: 0.768862, acc.: 77.78%] [G loss: 3.179832]\n",
      "1598 [D loss: 0.454878, acc.: 77.78%] [G loss: 2.904573]\n",
      "1599 [D loss: 0.525292, acc.: 72.22%] [G loss: 2.508923]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 [D loss: 0.555258, acc.: 77.78%] [G loss: 2.399395]\n",
      "1601 [D loss: 0.386171, acc.: 88.89%] [G loss: 2.261541]\n",
      "1602 [D loss: 0.565487, acc.: 55.56%] [G loss: 2.180141]\n",
      "1603 [D loss: 0.946485, acc.: 77.78%] [G loss: 2.230231]\n",
      "1604 [D loss: 0.379914, acc.: 83.33%] [G loss: 4.378908]\n",
      "1605 [D loss: 1.350140, acc.: 38.89%] [G loss: 0.990029]\n",
      "1606 [D loss: 0.553782, acc.: 88.89%] [G loss: 3.108587]\n",
      "1607 [D loss: 0.125667, acc.: 94.44%] [G loss: 5.514957]\n",
      "1608 [D loss: 0.240639, acc.: 88.89%] [G loss: 2.901425]\n",
      "1609 [D loss: 0.348226, acc.: 77.78%] [G loss: 3.163255]\n",
      "1610 [D loss: 0.286426, acc.: 83.33%] [G loss: 2.781127]\n",
      "1611 [D loss: 0.469552, acc.: 66.67%] [G loss: 1.697224]\n",
      "1612 [D loss: 0.666572, acc.: 61.11%] [G loss: 2.739666]\n",
      "1613 [D loss: 0.566297, acc.: 83.33%] [G loss: 2.688010]\n",
      "1614 [D loss: 0.395118, acc.: 77.78%] [G loss: 3.306011]\n",
      "1615 [D loss: 0.432685, acc.: 72.22%] [G loss: 3.183594]\n",
      "1616 [D loss: 0.138027, acc.: 100.00%] [G loss: 3.072723]\n",
      "1617 [D loss: 0.665738, acc.: 72.22%] [G loss: 1.628335]\n",
      "1618 [D loss: 0.323338, acc.: 88.89%] [G loss: 2.744341]\n",
      "1619 [D loss: 0.918869, acc.: 61.11%] [G loss: 1.953409]\n",
      "1620 [D loss: 0.263792, acc.: 88.89%] [G loss: 3.434777]\n",
      "1621 [D loss: 0.461654, acc.: 83.33%] [G loss: 3.585660]\n",
      "1622 [D loss: 0.759264, acc.: 55.56%] [G loss: 2.194697]\n",
      "1623 [D loss: 0.219744, acc.: 88.89%] [G loss: 2.877475]\n",
      "1624 [D loss: 0.656329, acc.: 72.22%] [G loss: 2.591774]\n",
      "1625 [D loss: 0.457231, acc.: 77.78%] [G loss: 2.534005]\n",
      "1626 [D loss: 0.669629, acc.: 66.67%] [G loss: 2.809272]\n",
      "1627 [D loss: 0.894122, acc.: 61.11%] [G loss: 2.016837]\n",
      "1628 [D loss: 0.103448, acc.: 94.44%] [G loss: 3.396106]\n",
      "1629 [D loss: 0.559569, acc.: 83.33%] [G loss: 2.940456]\n",
      "1630 [D loss: 0.768156, acc.: 66.67%] [G loss: 2.452494]\n",
      "1631 [D loss: 0.907315, acc.: 55.56%] [G loss: 1.740345]\n",
      "1632 [D loss: 0.459412, acc.: 83.33%] [G loss: 2.458481]\n",
      "1633 [D loss: 0.292307, acc.: 83.33%] [G loss: 3.591603]\n",
      "1634 [D loss: 0.453577, acc.: 72.22%] [G loss: 1.796166]\n",
      "1635 [D loss: 0.232521, acc.: 94.44%] [G loss: 3.481366]\n",
      "1636 [D loss: 0.495559, acc.: 77.78%] [G loss: 1.683199]\n",
      "1637 [D loss: 0.373780, acc.: 83.33%] [G loss: 2.540567]\n",
      "1638 [D loss: 1.158494, acc.: 44.44%] [G loss: 1.856591]\n",
      "1639 [D loss: 0.349572, acc.: 83.33%] [G loss: 3.417396]\n",
      "1640 [D loss: 1.139158, acc.: 55.56%] [G loss: 1.630151]\n",
      "1641 [D loss: 0.726209, acc.: 61.11%] [G loss: 2.069342]\n",
      "1642 [D loss: 0.347709, acc.: 83.33%] [G loss: 2.670701]\n",
      "1643 [D loss: 0.506243, acc.: 77.78%] [G loss: 3.133481]\n",
      "1644 [D loss: 0.323505, acc.: 88.89%] [G loss: 2.665807]\n",
      "1645 [D loss: 0.968943, acc.: 50.00%] [G loss: 2.379814]\n",
      "1646 [D loss: 0.649463, acc.: 72.22%] [G loss: 2.161754]\n",
      "1647 [D loss: 0.235478, acc.: 94.44%] [G loss: 3.393179]\n",
      "1648 [D loss: 0.427375, acc.: 72.22%] [G loss: 2.369634]\n",
      "1649 [D loss: 0.472687, acc.: 66.67%] [G loss: 2.376814]\n",
      "1650 [D loss: 0.494574, acc.: 72.22%] [G loss: 2.363961]\n",
      "1651 [D loss: 0.489069, acc.: 72.22%] [G loss: 2.096658]\n",
      "1652 [D loss: 0.976418, acc.: 55.56%] [G loss: 1.939532]\n",
      "1653 [D loss: 0.264683, acc.: 88.89%] [G loss: 2.138543]\n",
      "1654 [D loss: 0.504249, acc.: 77.78%] [G loss: 2.961586]\n",
      "1655 [D loss: 0.524814, acc.: 77.78%] [G loss: 3.589805]\n",
      "1656 [D loss: 0.240270, acc.: 88.89%] [G loss: 3.097586]\n",
      "1657 [D loss: 0.520408, acc.: 72.22%] [G loss: 1.928840]\n",
      "1658 [D loss: 0.294458, acc.: 77.78%] [G loss: 2.309702]\n",
      "1659 [D loss: 0.487867, acc.: 77.78%] [G loss: 3.065437]\n",
      "1660 [D loss: 0.217735, acc.: 94.44%] [G loss: 3.664483]\n",
      "1661 [D loss: 0.685823, acc.: 72.22%] [G loss: 2.054306]\n",
      "1662 [D loss: 0.747516, acc.: 66.67%] [G loss: 3.068244]\n",
      "1663 [D loss: 0.813560, acc.: 66.67%] [G loss: 4.111455]\n",
      "1664 [D loss: 0.369337, acc.: 77.78%] [G loss: 3.298299]\n",
      "1665 [D loss: 0.462206, acc.: 77.78%] [G loss: 3.878436]\n",
      "1666 [D loss: 0.468253, acc.: 72.22%] [G loss: 3.417249]\n",
      "1667 [D loss: 0.798280, acc.: 61.11%] [G loss: 3.350255]\n",
      "1668 [D loss: 0.714734, acc.: 61.11%] [G loss: 1.910676]\n",
      "1669 [D loss: 0.855270, acc.: 55.56%] [G loss: 3.861480]\n",
      "1670 [D loss: 0.098444, acc.: 100.00%] [G loss: 3.757054]\n",
      "1671 [D loss: 0.526433, acc.: 77.78%] [G loss: 2.466988]\n",
      "1672 [D loss: 0.370484, acc.: 88.89%] [G loss: 2.443236]\n",
      "1673 [D loss: 0.616932, acc.: 83.33%] [G loss: 1.984962]\n",
      "1674 [D loss: 0.082824, acc.: 100.00%] [G loss: 4.132461]\n",
      "1675 [D loss: 0.101293, acc.: 94.44%] [G loss: 4.091873]\n",
      "1676 [D loss: 0.184712, acc.: 94.44%] [G loss: 3.488900]\n",
      "1677 [D loss: 0.227820, acc.: 94.44%] [G loss: 2.838104]\n",
      "1678 [D loss: 0.458379, acc.: 83.33%] [G loss: 3.153558]\n",
      "1679 [D loss: 0.430970, acc.: 83.33%] [G loss: 3.039590]\n",
      "1680 [D loss: 0.138862, acc.: 94.44%] [G loss: 4.776651]\n",
      "1681 [D loss: 0.159412, acc.: 88.89%] [G loss: 4.762880]\n",
      "1682 [D loss: 0.942443, acc.: 83.33%] [G loss: 1.955266]\n",
      "1683 [D loss: 0.314193, acc.: 88.89%] [G loss: 3.415098]\n",
      "1684 [D loss: 0.073102, acc.: 100.00%] [G loss: 3.166654]\n",
      "1685 [D loss: 0.748451, acc.: 55.56%] [G loss: 2.746211]\n",
      "1686 [D loss: 0.275586, acc.: 83.33%] [G loss: 2.771493]\n",
      "1687 [D loss: 0.122926, acc.: 94.44%] [G loss: 4.764304]\n",
      "1688 [D loss: 0.161125, acc.: 94.44%] [G loss: 3.106887]\n",
      "1689 [D loss: 0.040679, acc.: 100.00%] [G loss: 4.375392]\n",
      "1690 [D loss: 0.635125, acc.: 61.11%] [G loss: 2.266611]\n",
      "1691 [D loss: 0.291080, acc.: 88.89%] [G loss: 3.197003]\n",
      "1692 [D loss: 0.078808, acc.: 94.44%] [G loss: 3.505791]\n",
      "1693 [D loss: 0.164337, acc.: 94.44%] [G loss: 3.402363]\n",
      "1694 [D loss: 0.246060, acc.: 88.89%] [G loss: 4.187648]\n",
      "1695 [D loss: 0.282245, acc.: 83.33%] [G loss: 2.978819]\n",
      "1696 [D loss: 0.287654, acc.: 88.89%] [G loss: 3.363796]\n",
      "1697 [D loss: 0.200142, acc.: 94.44%] [G loss: 3.605483]\n",
      "1698 [D loss: 0.406409, acc.: 83.33%] [G loss: 2.487100]\n",
      "1699 [D loss: 0.106857, acc.: 94.44%] [G loss: 3.633122]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700 [D loss: 0.724059, acc.: 83.33%] [G loss: 3.778045]\n",
      "1701 [D loss: 0.424619, acc.: 88.89%] [G loss: 4.063580]\n",
      "1702 [D loss: 0.165099, acc.: 94.44%] [G loss: 3.708433]\n",
      "1703 [D loss: 0.398096, acc.: 88.89%] [G loss: 3.920415]\n",
      "1704 [D loss: 0.219662, acc.: 83.33%] [G loss: 4.947618]\n",
      "1705 [D loss: 0.378570, acc.: 77.78%] [G loss: 4.208911]\n",
      "1706 [D loss: 0.094864, acc.: 100.00%] [G loss: 4.156482]\n",
      "1707 [D loss: 0.564985, acc.: 66.67%] [G loss: 3.024055]\n",
      "1708 [D loss: 0.236358, acc.: 94.44%] [G loss: 3.071608]\n",
      "1709 [D loss: 0.397260, acc.: 88.89%] [G loss: 4.294117]\n",
      "1710 [D loss: 0.346112, acc.: 88.89%] [G loss: 4.649339]\n",
      "1711 [D loss: 0.493488, acc.: 83.33%] [G loss: 2.880729]\n",
      "1712 [D loss: 0.304343, acc.: 77.78%] [G loss: 3.686387]\n",
      "1713 [D loss: 0.122666, acc.: 100.00%] [G loss: 3.504462]\n",
      "1714 [D loss: 0.114087, acc.: 94.44%] [G loss: 7.890635]\n",
      "1715 [D loss: 0.217943, acc.: 88.89%] [G loss: 4.131022]\n",
      "1716 [D loss: 0.277283, acc.: 88.89%] [G loss: 3.727436]\n",
      "1717 [D loss: 0.937720, acc.: 66.67%] [G loss: 2.662488]\n",
      "1718 [D loss: 0.302421, acc.: 83.33%] [G loss: 3.675329]\n",
      "1719 [D loss: 0.458158, acc.: 77.78%] [G loss: 3.411283]\n",
      "1720 [D loss: 0.897176, acc.: 66.67%] [G loss: 1.208277]\n",
      "1721 [D loss: 0.489384, acc.: 77.78%] [G loss: 2.564689]\n",
      "1722 [D loss: 0.442169, acc.: 77.78%] [G loss: 3.371392]\n",
      "1723 [D loss: 0.489081, acc.: 77.78%] [G loss: 3.958855]\n",
      "1724 [D loss: 0.478576, acc.: 72.22%] [G loss: 2.697784]\n",
      "1725 [D loss: 0.304948, acc.: 77.78%] [G loss: 2.258448]\n",
      "1726 [D loss: 0.234884, acc.: 83.33%] [G loss: 2.405524]\n",
      "1727 [D loss: 0.517066, acc.: 83.33%] [G loss: 2.298886]\n",
      "1728 [D loss: 0.371628, acc.: 83.33%] [G loss: 3.636299]\n",
      "1729 [D loss: 0.247961, acc.: 94.44%] [G loss: 3.398547]\n",
      "1730 [D loss: 0.475216, acc.: 77.78%] [G loss: 2.390422]\n",
      "1731 [D loss: 0.323756, acc.: 94.44%] [G loss: 2.504355]\n",
      "1732 [D loss: 0.510065, acc.: 72.22%] [G loss: 2.052392]\n",
      "1733 [D loss: 0.323356, acc.: 83.33%] [G loss: 3.354839]\n",
      "1734 [D loss: 0.547974, acc.: 77.78%] [G loss: 3.094856]\n",
      "1735 [D loss: 0.311587, acc.: 88.89%] [G loss: 3.910471]\n",
      "1736 [D loss: 0.971233, acc.: 72.22%] [G loss: 2.520745]\n",
      "1737 [D loss: 0.470770, acc.: 77.78%] [G loss: 2.063149]\n",
      "1738 [D loss: 0.185658, acc.: 94.44%] [G loss: 3.617712]\n",
      "1739 [D loss: 0.714209, acc.: 66.67%] [G loss: 2.703894]\n",
      "1740 [D loss: 0.612598, acc.: 83.33%] [G loss: 2.671930]\n",
      "1741 [D loss: 0.417166, acc.: 83.33%] [G loss: 1.703040]\n",
      "1742 [D loss: 0.597620, acc.: 72.22%] [G loss: 2.211481]\n",
      "1743 [D loss: 0.169505, acc.: 94.44%] [G loss: 3.134515]\n",
      "1744 [D loss: 0.400398, acc.: 77.78%] [G loss: 2.800594]\n",
      "1745 [D loss: 0.690033, acc.: 72.22%] [G loss: 3.722095]\n",
      "1746 [D loss: 0.428369, acc.: 77.78%] [G loss: 2.201572]\n",
      "1747 [D loss: 0.294338, acc.: 88.89%] [G loss: 1.984055]\n",
      "1748 [D loss: 0.621536, acc.: 72.22%] [G loss: 2.097322]\n",
      "1749 [D loss: 0.483745, acc.: 77.78%] [G loss: 2.812375]\n",
      "1750 [D loss: 0.651475, acc.: 72.22%] [G loss: 3.502964]\n",
      "1751 [D loss: 0.444327, acc.: 94.44%] [G loss: 1.868137]\n",
      "1752 [D loss: 0.265156, acc.: 88.89%] [G loss: 2.693090]\n",
      "1753 [D loss: 0.203901, acc.: 94.44%] [G loss: 3.178951]\n",
      "1754 [D loss: 0.934546, acc.: 72.22%] [G loss: 1.524382]\n",
      "1755 [D loss: 0.711870, acc.: 66.67%] [G loss: 2.564534]\n",
      "1756 [D loss: 0.357380, acc.: 77.78%] [G loss: 3.870610]\n",
      "1757 [D loss: 0.360207, acc.: 83.33%] [G loss: 3.552691]\n",
      "1758 [D loss: 0.571379, acc.: 77.78%] [G loss: 1.647639]\n",
      "1759 [D loss: 0.579452, acc.: 77.78%] [G loss: 3.068232]\n",
      "1760 [D loss: 0.314678, acc.: 83.33%] [G loss: 2.697819]\n",
      "1761 [D loss: 0.265455, acc.: 83.33%] [G loss: 3.657013]\n",
      "1762 [D loss: 0.145280, acc.: 100.00%] [G loss: 3.869496]\n",
      "1763 [D loss: 0.692328, acc.: 66.67%] [G loss: 2.508742]\n",
      "1764 [D loss: 0.224626, acc.: 88.89%] [G loss: 3.751483]\n",
      "1765 [D loss: 0.466146, acc.: 83.33%] [G loss: 2.188970]\n",
      "1766 [D loss: 0.446283, acc.: 72.22%] [G loss: 1.880987]\n",
      "1767 [D loss: 0.181870, acc.: 94.44%] [G loss: 3.184571]\n",
      "1768 [D loss: 0.315759, acc.: 94.44%] [G loss: 3.772632]\n",
      "1769 [D loss: 0.325804, acc.: 94.44%] [G loss: 3.266315]\n",
      "1770 [D loss: 0.369162, acc.: 83.33%] [G loss: 2.486987]\n",
      "1771 [D loss: 0.273891, acc.: 94.44%] [G loss: 3.003078]\n",
      "1772 [D loss: 0.273863, acc.: 83.33%] [G loss: 3.033586]\n",
      "1773 [D loss: 0.586973, acc.: 77.78%] [G loss: 2.203581]\n",
      "1774 [D loss: 0.334567, acc.: 83.33%] [G loss: 2.372833]\n",
      "1775 [D loss: 0.226831, acc.: 88.89%] [G loss: 2.717014]\n",
      "1776 [D loss: 0.184405, acc.: 94.44%] [G loss: 3.978579]\n",
      "1777 [D loss: 0.363545, acc.: 88.89%] [G loss: 2.409094]\n",
      "1778 [D loss: 0.159061, acc.: 100.00%] [G loss: 3.064851]\n",
      "1779 [D loss: 0.391368, acc.: 77.78%] [G loss: 3.158266]\n",
      "1780 [D loss: 0.194702, acc.: 94.44%] [G loss: 3.398353]\n",
      "1781 [D loss: 0.323034, acc.: 83.33%] [G loss: 2.978426]\n",
      "1782 [D loss: 0.618418, acc.: 77.78%] [G loss: 2.484014]\n",
      "1783 [D loss: 0.654140, acc.: 72.22%] [G loss: 2.279344]\n",
      "1784 [D loss: 0.126843, acc.: 100.00%] [G loss: 4.053373]\n",
      "1785 [D loss: 0.200777, acc.: 100.00%] [G loss: 2.890986]\n",
      "1786 [D loss: 0.218977, acc.: 88.89%] [G loss: 2.972219]\n",
      "1787 [D loss: 0.436391, acc.: 83.33%] [G loss: 3.310184]\n",
      "1788 [D loss: 0.504130, acc.: 88.89%] [G loss: 2.662278]\n",
      "1789 [D loss: 0.798012, acc.: 55.56%] [G loss: 1.309323]\n",
      "1790 [D loss: 0.518298, acc.: 72.22%] [G loss: 2.871546]\n",
      "1791 [D loss: 0.268951, acc.: 88.89%] [G loss: 2.198763]\n",
      "1792 [D loss: 0.123421, acc.: 100.00%] [G loss: 3.076919]\n",
      "1793 [D loss: 0.496319, acc.: 77.78%] [G loss: 3.805853]\n",
      "1794 [D loss: 0.234029, acc.: 88.89%] [G loss: 2.775170]\n",
      "1795 [D loss: 0.149629, acc.: 94.44%] [G loss: 3.488714]\n",
      "1796 [D loss: 0.256668, acc.: 94.44%] [G loss: 3.675071]\n",
      "1797 [D loss: 0.161477, acc.: 94.44%] [G loss: 2.353475]\n",
      "1798 [D loss: 0.429500, acc.: 77.78%] [G loss: 4.399220]\n",
      "1799 [D loss: 0.139303, acc.: 94.44%] [G loss: 3.146882]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800 [D loss: 0.197431, acc.: 94.44%] [G loss: 2.976505]\n",
      "1801 [D loss: 0.133764, acc.: 94.44%] [G loss: 3.429998]\n",
      "1802 [D loss: 0.073001, acc.: 100.00%] [G loss: 3.158149]\n",
      "1803 [D loss: 0.135293, acc.: 94.44%] [G loss: 3.634705]\n",
      "1804 [D loss: 0.322751, acc.: 77.78%] [G loss: 3.778283]\n",
      "1805 [D loss: 0.074526, acc.: 100.00%] [G loss: 4.423905]\n",
      "1806 [D loss: 0.116355, acc.: 94.44%] [G loss: 3.636841]\n",
      "1807 [D loss: 1.099386, acc.: 61.11%] [G loss: 2.374664]\n",
      "1808 [D loss: 0.436398, acc.: 83.33%] [G loss: 4.677692]\n",
      "1809 [D loss: 0.820394, acc.: 66.67%] [G loss: 2.284570]\n",
      "1810 [D loss: 0.403754, acc.: 88.89%] [G loss: 4.165663]\n",
      "1811 [D loss: 0.350483, acc.: 83.33%] [G loss: 2.704364]\n",
      "1812 [D loss: 0.227151, acc.: 94.44%] [G loss: 3.065152]\n",
      "1813 [D loss: 0.117429, acc.: 94.44%] [G loss: 5.511314]\n",
      "1814 [D loss: 0.393893, acc.: 88.89%] [G loss: 2.750124]\n",
      "1815 [D loss: 0.261222, acc.: 88.89%] [G loss: 3.264181]\n",
      "1816 [D loss: 0.290329, acc.: 88.89%] [G loss: 2.369435]\n",
      "1817 [D loss: 0.298602, acc.: 77.78%] [G loss: 3.420534]\n",
      "1818 [D loss: 0.274131, acc.: 88.89%] [G loss: 3.593265]\n",
      "1819 [D loss: 0.232492, acc.: 88.89%] [G loss: 4.319095]\n",
      "1820 [D loss: 0.167454, acc.: 94.44%] [G loss: 3.455542]\n",
      "1821 [D loss: 0.524694, acc.: 83.33%] [G loss: 3.677569]\n",
      "1822 [D loss: 0.239637, acc.: 94.44%] [G loss: 3.574543]\n",
      "1823 [D loss: 0.204015, acc.: 88.89%] [G loss: 3.146463]\n",
      "1824 [D loss: 0.246033, acc.: 94.44%] [G loss: 3.222690]\n",
      "1825 [D loss: 0.918104, acc.: 55.56%] [G loss: 2.009137]\n",
      "1826 [D loss: 0.396427, acc.: 88.89%] [G loss: 2.449970]\n",
      "1827 [D loss: 0.360068, acc.: 77.78%] [G loss: 2.780653]\n",
      "1828 [D loss: 0.200019, acc.: 94.44%] [G loss: 4.020042]\n",
      "1829 [D loss: 0.366482, acc.: 83.33%] [G loss: 3.818431]\n",
      "1830 [D loss: 0.222314, acc.: 94.44%] [G loss: 2.537611]\n",
      "1831 [D loss: 0.212088, acc.: 88.89%] [G loss: 2.658886]\n",
      "1832 [D loss: 0.535510, acc.: 55.56%] [G loss: 3.630466]\n",
      "1833 [D loss: 0.514442, acc.: 77.78%] [G loss: 2.347210]\n",
      "1834 [D loss: 0.438479, acc.: 83.33%] [G loss: 2.530944]\n",
      "1835 [D loss: 0.650156, acc.: 72.22%] [G loss: 2.635129]\n",
      "1836 [D loss: 0.446514, acc.: 77.78%] [G loss: 2.878232]\n",
      "1837 [D loss: 0.097333, acc.: 100.00%] [G loss: 3.500060]\n",
      "1838 [D loss: 0.621889, acc.: 61.11%] [G loss: 3.281331]\n",
      "1839 [D loss: 0.411991, acc.: 77.78%] [G loss: 3.071776]\n",
      "1840 [D loss: 0.142970, acc.: 94.44%] [G loss: 3.684385]\n",
      "1841 [D loss: 0.809623, acc.: 77.78%] [G loss: 1.761230]\n",
      "1842 [D loss: 0.778150, acc.: 61.11%] [G loss: 2.635002]\n",
      "1843 [D loss: 0.685232, acc.: 77.78%] [G loss: 3.293806]\n",
      "1844 [D loss: 0.455693, acc.: 83.33%] [G loss: 2.101311]\n",
      "1845 [D loss: 0.324363, acc.: 88.89%] [G loss: 3.147191]\n",
      "1846 [D loss: 0.101067, acc.: 100.00%] [G loss: 2.730375]\n",
      "1847 [D loss: 0.127749, acc.: 100.00%] [G loss: 2.545221]\n",
      "1848 [D loss: 0.378330, acc.: 88.89%] [G loss: 2.810130]\n",
      "1849 [D loss: 0.365309, acc.: 72.22%] [G loss: 2.154930]\n",
      "1850 [D loss: 0.261039, acc.: 88.89%] [G loss: 3.446523]\n",
      "1851 [D loss: 0.218226, acc.: 94.44%] [G loss: 2.601103]\n",
      "1852 [D loss: 0.440012, acc.: 88.89%] [G loss: 3.044787]\n",
      "1853 [D loss: 0.195886, acc.: 94.44%] [G loss: 3.285789]\n",
      "1854 [D loss: 0.271006, acc.: 94.44%] [G loss: 2.808035]\n",
      "1855 [D loss: 0.311162, acc.: 94.44%] [G loss: 3.792526]\n",
      "1856 [D loss: 0.502030, acc.: 77.78%] [G loss: 2.626107]\n",
      "1857 [D loss: 0.445948, acc.: 77.78%] [G loss: 2.436000]\n",
      "1858 [D loss: 0.376224, acc.: 94.44%] [G loss: 3.181079]\n",
      "1859 [D loss: 0.191921, acc.: 83.33%] [G loss: 2.884798]\n",
      "1860 [D loss: 0.451393, acc.: 83.33%] [G loss: 2.139567]\n",
      "1861 [D loss: 0.442707, acc.: 77.78%] [G loss: 2.578767]\n",
      "1862 [D loss: 0.133897, acc.: 94.44%] [G loss: 3.131912]\n",
      "1863 [D loss: 0.523310, acc.: 77.78%] [G loss: 2.037305]\n",
      "1864 [D loss: 0.241129, acc.: 88.89%] [G loss: 3.667731]\n",
      "1865 [D loss: 0.225218, acc.: 83.33%] [G loss: 2.346520]\n",
      "1866 [D loss: 0.316617, acc.: 83.33%] [G loss: 2.430443]\n",
      "1867 [D loss: 0.169794, acc.: 88.89%] [G loss: 2.902989]\n",
      "1868 [D loss: 0.370721, acc.: 83.33%] [G loss: 3.547018]\n",
      "1869 [D loss: 0.283724, acc.: 83.33%] [G loss: 3.938491]\n",
      "1870 [D loss: 0.468424, acc.: 77.78%] [G loss: 2.524335]\n",
      "1871 [D loss: 0.083005, acc.: 100.00%] [G loss: 4.100402]\n",
      "1872 [D loss: 0.203835, acc.: 94.44%] [G loss: 2.278080]\n",
      "1873 [D loss: 0.367895, acc.: 77.78%] [G loss: 4.429635]\n",
      "1874 [D loss: 0.285339, acc.: 88.89%] [G loss: 4.845509]\n",
      "1875 [D loss: 1.080682, acc.: 55.56%] [G loss: 2.564374]\n",
      "1876 [D loss: 0.715496, acc.: 72.22%] [G loss: 1.830224]\n",
      "1877 [D loss: 0.422777, acc.: 83.33%] [G loss: 3.280893]\n",
      "1878 [D loss: 0.503272, acc.: 72.22%] [G loss: 2.769005]\n",
      "1879 [D loss: 0.165003, acc.: 94.44%] [G loss: 3.694174]\n",
      "1880 [D loss: 0.222353, acc.: 94.44%] [G loss: 2.520793]\n",
      "1881 [D loss: 0.272527, acc.: 83.33%] [G loss: 3.035246]\n",
      "1882 [D loss: 0.105717, acc.: 100.00%] [G loss: 3.598895]\n",
      "1883 [D loss: 0.130997, acc.: 94.44%] [G loss: 3.307955]\n",
      "1884 [D loss: 0.183513, acc.: 94.44%] [G loss: 3.584336]\n",
      "1885 [D loss: 0.483111, acc.: 72.22%] [G loss: 1.534951]\n",
      "1886 [D loss: 0.541515, acc.: 72.22%] [G loss: 3.761451]\n",
      "1887 [D loss: 0.442030, acc.: 77.78%] [G loss: 2.473179]\n",
      "1888 [D loss: 0.331582, acc.: 88.89%] [G loss: 3.828767]\n",
      "1889 [D loss: 0.325037, acc.: 83.33%] [G loss: 3.746284]\n",
      "1890 [D loss: 0.184322, acc.: 88.89%] [G loss: 4.088967]\n",
      "1891 [D loss: 0.517312, acc.: 77.78%] [G loss: 2.506200]\n",
      "1892 [D loss: 0.441956, acc.: 72.22%] [G loss: 2.434071]\n",
      "1893 [D loss: 0.080655, acc.: 100.00%] [G loss: 3.379527]\n",
      "1894 [D loss: 0.211678, acc.: 88.89%] [G loss: 3.328418]\n",
      "1895 [D loss: 0.443760, acc.: 77.78%] [G loss: 3.183167]\n",
      "1896 [D loss: 0.291216, acc.: 83.33%] [G loss: 4.185893]\n",
      "1897 [D loss: 0.101588, acc.: 94.44%] [G loss: 3.891281]\n",
      "1898 [D loss: 0.148423, acc.: 94.44%] [G loss: 4.069196]\n",
      "1899 [D loss: 0.297215, acc.: 88.89%] [G loss: 2.000775]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1900 [D loss: 0.573402, acc.: 77.78%] [G loss: 2.645069]\n",
      "1901 [D loss: 0.244164, acc.: 94.44%] [G loss: 3.676513]\n",
      "1902 [D loss: 0.341589, acc.: 88.89%] [G loss: 2.542311]\n",
      "1903 [D loss: 0.471763, acc.: 83.33%] [G loss: 2.311158]\n",
      "1904 [D loss: 0.473952, acc.: 83.33%] [G loss: 3.404296]\n",
      "1905 [D loss: 0.244965, acc.: 88.89%] [G loss: 2.357752]\n",
      "1906 [D loss: 0.415064, acc.: 83.33%] [G loss: 2.924352]\n",
      "1907 [D loss: 0.566349, acc.: 55.56%] [G loss: 3.576282]\n",
      "1908 [D loss: 0.345808, acc.: 77.78%] [G loss: 3.241898]\n",
      "1909 [D loss: 0.119189, acc.: 100.00%] [G loss: 4.221720]\n",
      "1910 [D loss: 0.215686, acc.: 94.44%] [G loss: 3.167620]\n",
      "1911 [D loss: 0.342277, acc.: 88.89%] [G loss: 3.269908]\n",
      "1912 [D loss: 0.157354, acc.: 94.44%] [G loss: 3.211707]\n",
      "1913 [D loss: 0.362082, acc.: 83.33%] [G loss: 3.450078]\n",
      "1914 [D loss: 0.118885, acc.: 94.44%] [G loss: 3.923571]\n",
      "1915 [D loss: 0.137165, acc.: 94.44%] [G loss: 3.686478]\n",
      "1916 [D loss: 0.525145, acc.: 77.78%] [G loss: 2.693745]\n",
      "1917 [D loss: 0.204887, acc.: 88.89%] [G loss: 3.500712]\n",
      "1918 [D loss: 0.393264, acc.: 72.22%] [G loss: 3.637826]\n",
      "1919 [D loss: 0.307191, acc.: 83.33%] [G loss: 2.567778]\n",
      "1920 [D loss: 0.318782, acc.: 83.33%] [G loss: 4.211017]\n",
      "1921 [D loss: 0.224784, acc.: 88.89%] [G loss: 4.333247]\n",
      "1922 [D loss: 0.330537, acc.: 77.78%] [G loss: 4.461091]\n",
      "1923 [D loss: 0.226472, acc.: 88.89%] [G loss: 3.965035]\n",
      "1924 [D loss: 0.276876, acc.: 83.33%] [G loss: 3.764947]\n",
      "1925 [D loss: 0.151112, acc.: 94.44%] [G loss: 3.529706]\n",
      "1926 [D loss: 0.534390, acc.: 77.78%] [G loss: 3.141077]\n",
      "1927 [D loss: 0.336992, acc.: 83.33%] [G loss: 3.931995]\n",
      "1928 [D loss: 0.323122, acc.: 88.89%] [G loss: 2.519189]\n",
      "1929 [D loss: 0.312606, acc.: 77.78%] [G loss: 3.195919]\n",
      "1930 [D loss: 0.194310, acc.: 94.44%] [G loss: 4.419873]\n",
      "1931 [D loss: 0.282837, acc.: 88.89%] [G loss: 3.542968]\n",
      "1932 [D loss: 0.387420, acc.: 77.78%] [G loss: 2.060549]\n",
      "1933 [D loss: 0.346196, acc.: 88.89%] [G loss: 3.735021]\n",
      "1934 [D loss: 0.500368, acc.: 66.67%] [G loss: 3.250140]\n",
      "1935 [D loss: 0.847932, acc.: 55.56%] [G loss: 2.875674]\n",
      "1936 [D loss: 0.198662, acc.: 94.44%] [G loss: 3.095782]\n",
      "1937 [D loss: 0.381138, acc.: 72.22%] [G loss: 2.888916]\n",
      "1938 [D loss: 0.187169, acc.: 94.44%] [G loss: 2.203087]\n",
      "1939 [D loss: 0.235478, acc.: 88.89%] [G loss: 3.395514]\n",
      "1940 [D loss: 0.237485, acc.: 88.89%] [G loss: 3.388249]\n",
      "1941 [D loss: 0.146636, acc.: 100.00%] [G loss: 3.666136]\n",
      "1942 [D loss: 0.342167, acc.: 77.78%] [G loss: 3.270678]\n",
      "1943 [D loss: 0.439177, acc.: 83.33%] [G loss: 3.558427]\n",
      "1944 [D loss: 0.458204, acc.: 66.67%] [G loss: 3.286963]\n",
      "1945 [D loss: 0.132103, acc.: 94.44%] [G loss: 3.072079]\n",
      "1946 [D loss: 0.496552, acc.: 72.22%] [G loss: 3.310541]\n",
      "1947 [D loss: 1.038671, acc.: 55.56%] [G loss: 2.544369]\n",
      "1948 [D loss: 0.413750, acc.: 77.78%] [G loss: 3.917987]\n",
      "1949 [D loss: 0.538006, acc.: 72.22%] [G loss: 3.588688]\n",
      "1950 [D loss: 0.492782, acc.: 77.78%] [G loss: 2.032275]\n",
      "1951 [D loss: 0.445045, acc.: 83.33%] [G loss: 3.260033]\n",
      "1952 [D loss: 0.594277, acc.: 77.78%] [G loss: 4.044501]\n",
      "1953 [D loss: 0.623448, acc.: 72.22%] [G loss: 3.260168]\n",
      "1954 [D loss: 0.753879, acc.: 66.67%] [G loss: 2.894797]\n",
      "1955 [D loss: 0.335601, acc.: 88.89%] [G loss: 4.671141]\n",
      "1956 [D loss: 1.166575, acc.: 38.89%] [G loss: 1.905931]\n",
      "1957 [D loss: 0.625641, acc.: 72.22%] [G loss: 2.797112]\n",
      "1958 [D loss: 0.814096, acc.: 61.11%] [G loss: 2.463511]\n",
      "1959 [D loss: 0.216516, acc.: 100.00%] [G loss: 2.864512]\n",
      "1960 [D loss: 0.415311, acc.: 83.33%] [G loss: 2.390945]\n",
      "1961 [D loss: 0.925850, acc.: 33.33%] [G loss: 1.606008]\n",
      "1962 [D loss: 0.394262, acc.: 72.22%] [G loss: 2.842279]\n",
      "1963 [D loss: 0.300313, acc.: 88.89%] [G loss: 3.704866]\n",
      "1964 [D loss: 0.897106, acc.: 72.22%] [G loss: 2.837383]\n",
      "1965 [D loss: 0.164237, acc.: 94.44%] [G loss: 3.677934]\n",
      "1966 [D loss: 0.635786, acc.: 72.22%] [G loss: 2.828451]\n",
      "1967 [D loss: 0.213103, acc.: 88.89%] [G loss: 2.518416]\n",
      "1968 [D loss: 0.864866, acc.: 61.11%] [G loss: 3.141853]\n",
      "1969 [D loss: 0.300579, acc.: 83.33%] [G loss: 3.495912]\n",
      "1970 [D loss: 0.812748, acc.: 50.00%] [G loss: 2.987468]\n",
      "1971 [D loss: 0.354466, acc.: 72.22%] [G loss: 2.756922]\n",
      "1972 [D loss: 0.199657, acc.: 94.44%] [G loss: 3.440615]\n",
      "1973 [D loss: 0.242869, acc.: 94.44%] [G loss: 1.661836]\n",
      "1974 [D loss: 0.602070, acc.: 77.78%] [G loss: 3.682934]\n",
      "1975 [D loss: 0.319486, acc.: 88.89%] [G loss: 3.953998]\n",
      "1976 [D loss: 0.819444, acc.: 72.22%] [G loss: 2.800778]\n",
      "1977 [D loss: 0.270190, acc.: 83.33%] [G loss: 3.441249]\n",
      "1978 [D loss: 0.193897, acc.: 88.89%] [G loss: 3.028039]\n",
      "1979 [D loss: 0.744041, acc.: 72.22%] [G loss: 3.250043]\n",
      "1980 [D loss: 0.302506, acc.: 83.33%] [G loss: 4.242907]\n",
      "1981 [D loss: 0.086145, acc.: 100.00%] [G loss: 3.116594]\n",
      "1982 [D loss: 0.960758, acc.: 61.11%] [G loss: 4.925613]\n",
      "1983 [D loss: 0.835974, acc.: 55.56%] [G loss: 2.733323]\n",
      "1984 [D loss: 0.445352, acc.: 77.78%] [G loss: 2.743944]\n",
      "1985 [D loss: 0.383250, acc.: 83.33%] [G loss: 2.724900]\n",
      "1986 [D loss: 0.311049, acc.: 94.44%] [G loss: 3.223539]\n",
      "1987 [D loss: 0.158448, acc.: 100.00%] [G loss: 2.733141]\n",
      "1988 [D loss: 0.147009, acc.: 94.44%] [G loss: 3.382941]\n",
      "1989 [D loss: 0.291948, acc.: 88.89%] [G loss: 2.415741]\n",
      "1990 [D loss: 0.518146, acc.: 83.33%] [G loss: 4.549984]\n",
      "1991 [D loss: 0.315381, acc.: 88.89%] [G loss: 2.687772]\n",
      "1992 [D loss: 0.186089, acc.: 94.44%] [G loss: 3.524846]\n",
      "1993 [D loss: 0.235136, acc.: 94.44%] [G loss: 1.792116]\n",
      "1994 [D loss: 0.658406, acc.: 72.22%] [G loss: 4.027005]\n",
      "1995 [D loss: 0.248856, acc.: 94.44%] [G loss: 2.842291]\n",
      "1996 [D loss: 0.361916, acc.: 88.89%] [G loss: 2.350906]\n",
      "1997 [D loss: 0.208218, acc.: 94.44%] [G loss: 3.316226]\n",
      "1998 [D loss: 0.413928, acc.: 88.89%] [G loss: 2.960760]\n",
      "1999 [D loss: 0.470802, acc.: 83.33%] [G loss: 2.528924]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 [D loss: 0.489315, acc.: 83.33%] [G loss: 3.081338]\n",
      "2001 [D loss: 0.422732, acc.: 72.22%] [G loss: 3.496895]\n",
      "2002 [D loss: 0.484030, acc.: 83.33%] [G loss: 2.299041]\n",
      "2003 [D loss: 0.428181, acc.: 66.67%] [G loss: 3.092555]\n",
      "2004 [D loss: 0.222767, acc.: 94.44%] [G loss: 3.767537]\n",
      "2005 [D loss: 0.412740, acc.: 83.33%] [G loss: 3.021291]\n",
      "2006 [D loss: 0.325355, acc.: 83.33%] [G loss: 4.420272]\n",
      "2007 [D loss: 0.650734, acc.: 72.22%] [G loss: 3.974595]\n",
      "2008 [D loss: 0.343049, acc.: 77.78%] [G loss: 4.227122]\n",
      "2009 [D loss: 0.399085, acc.: 83.33%] [G loss: 2.936192]\n",
      "2010 [D loss: 0.486875, acc.: 77.78%] [G loss: 2.540320]\n",
      "2011 [D loss: 0.401226, acc.: 88.89%] [G loss: 2.484198]\n",
      "2012 [D loss: 0.489744, acc.: 72.22%] [G loss: 2.150179]\n",
      "2013 [D loss: 0.301823, acc.: 83.33%] [G loss: 3.536379]\n",
      "2014 [D loss: 0.682180, acc.: 83.33%] [G loss: 2.265152]\n",
      "2015 [D loss: 0.326107, acc.: 77.78%] [G loss: 2.789937]\n",
      "2016 [D loss: 0.333155, acc.: 88.89%] [G loss: 2.787484]\n",
      "2017 [D loss: 0.943113, acc.: 55.56%] [G loss: 2.082760]\n",
      "2018 [D loss: 0.392847, acc.: 88.89%] [G loss: 2.223169]\n",
      "2019 [D loss: 0.135677, acc.: 94.44%] [G loss: 4.181861]\n",
      "2020 [D loss: 0.327506, acc.: 83.33%] [G loss: 3.240988]\n",
      "2021 [D loss: 0.494953, acc.: 83.33%] [G loss: 3.565510]\n",
      "2022 [D loss: 0.226764, acc.: 94.44%] [G loss: 3.216732]\n",
      "2023 [D loss: 0.410043, acc.: 72.22%] [G loss: 3.394043]\n",
      "2024 [D loss: 0.311840, acc.: 88.89%] [G loss: 3.906561]\n",
      "2025 [D loss: 0.183128, acc.: 94.44%] [G loss: 4.554088]\n",
      "2026 [D loss: 0.206868, acc.: 88.89%] [G loss: 2.678020]\n",
      "2027 [D loss: 0.208750, acc.: 88.89%] [G loss: 2.906131]\n",
      "2028 [D loss: 0.509943, acc.: 72.22%] [G loss: 2.474009]\n",
      "2029 [D loss: 0.245355, acc.: 94.44%] [G loss: 3.384362]\n",
      "2030 [D loss: 0.374986, acc.: 83.33%] [G loss: 2.252992]\n",
      "2031 [D loss: 0.944153, acc.: 38.89%] [G loss: 1.498143]\n",
      "2032 [D loss: 0.456792, acc.: 88.89%] [G loss: 2.346423]\n",
      "2033 [D loss: 0.389466, acc.: 88.89%] [G loss: 3.839565]\n",
      "2034 [D loss: 0.395577, acc.: 77.78%] [G loss: 4.718654]\n",
      "2035 [D loss: 0.764364, acc.: 55.56%] [G loss: 2.773618]\n",
      "2036 [D loss: 0.473582, acc.: 77.78%] [G loss: 2.873937]\n",
      "2037 [D loss: 0.319888, acc.: 83.33%] [G loss: 3.717336]\n",
      "2038 [D loss: 0.773044, acc.: 72.22%] [G loss: 2.558015]\n",
      "2039 [D loss: 0.397400, acc.: 77.78%] [G loss: 3.221706]\n",
      "2040 [D loss: 0.564293, acc.: 77.78%] [G loss: 2.202050]\n",
      "2041 [D loss: 0.542096, acc.: 72.22%] [G loss: 1.939589]\n",
      "2042 [D loss: 0.081517, acc.: 100.00%] [G loss: 2.729331]\n",
      "2043 [D loss: 0.343282, acc.: 77.78%] [G loss: 3.081603]\n",
      "2044 [D loss: 1.195113, acc.: 55.56%] [G loss: 2.199500]\n",
      "2045 [D loss: 0.460372, acc.: 83.33%] [G loss: 3.631179]\n",
      "2046 [D loss: 0.330075, acc.: 77.78%] [G loss: 3.274981]\n",
      "2047 [D loss: 0.514908, acc.: 77.78%] [G loss: 3.458050]\n",
      "2048 [D loss: 0.429209, acc.: 77.78%] [G loss: 2.396438]\n",
      "2049 [D loss: 0.209740, acc.: 94.44%] [G loss: 2.895577]\n",
      "2050 [D loss: 0.229094, acc.: 94.44%] [G loss: 3.443857]\n",
      "2051 [D loss: 0.362404, acc.: 83.33%] [G loss: 2.993444]\n",
      "2052 [D loss: 0.526851, acc.: 83.33%] [G loss: 2.472713]\n",
      "2053 [D loss: 0.669679, acc.: 66.67%] [G loss: 2.935411]\n",
      "2054 [D loss: 0.570005, acc.: 66.67%] [G loss: 2.775550]\n",
      "2055 [D loss: 0.390707, acc.: 77.78%] [G loss: 2.398265]\n",
      "2056 [D loss: 0.179246, acc.: 94.44%] [G loss: 2.961217]\n",
      "2057 [D loss: 0.435456, acc.: 77.78%] [G loss: 2.041902]\n",
      "2058 [D loss: 0.316374, acc.: 88.89%] [G loss: 3.431670]\n",
      "2059 [D loss: 0.524390, acc.: 72.22%] [G loss: 3.234333]\n",
      "2060 [D loss: 0.668747, acc.: 72.22%] [G loss: 2.396990]\n",
      "2061 [D loss: 1.091870, acc.: 72.22%] [G loss: 2.862113]\n",
      "2062 [D loss: 0.102421, acc.: 100.00%] [G loss: 3.923566]\n",
      "2063 [D loss: 0.390229, acc.: 83.33%] [G loss: 2.683859]\n",
      "2064 [D loss: 0.321118, acc.: 88.89%] [G loss: 3.555845]\n",
      "2065 [D loss: 0.608399, acc.: 66.67%] [G loss: 1.708199]\n",
      "2066 [D loss: 0.375346, acc.: 77.78%] [G loss: 2.726450]\n",
      "2067 [D loss: 0.701041, acc.: 77.78%] [G loss: 3.376989]\n",
      "2068 [D loss: 0.915597, acc.: 55.56%] [G loss: 3.062101]\n",
      "2069 [D loss: 0.369513, acc.: 88.89%] [G loss: 3.114687]\n",
      "2070 [D loss: 0.520761, acc.: 72.22%] [G loss: 3.411731]\n",
      "2071 [D loss: 0.364864, acc.: 77.78%] [G loss: 2.564603]\n",
      "2072 [D loss: 0.721112, acc.: 55.56%] [G loss: 2.248374]\n",
      "2073 [D loss: 0.410802, acc.: 77.78%] [G loss: 2.637868]\n",
      "2074 [D loss: 0.270753, acc.: 88.89%] [G loss: 4.259797]\n",
      "2075 [D loss: 0.325357, acc.: 83.33%] [G loss: 3.660983]\n",
      "2076 [D loss: 0.275593, acc.: 77.78%] [G loss: 2.848685]\n",
      "2077 [D loss: 0.370987, acc.: 94.44%] [G loss: 2.756675]\n",
      "2078 [D loss: 0.669000, acc.: 66.67%] [G loss: 2.912645]\n",
      "2079 [D loss: 1.036171, acc.: 55.56%] [G loss: 2.364971]\n",
      "2080 [D loss: 0.662905, acc.: 72.22%] [G loss: 3.182971]\n",
      "2081 [D loss: 0.904747, acc.: 66.67%] [G loss: 2.569037]\n",
      "2082 [D loss: 0.309038, acc.: 94.44%] [G loss: 3.176994]\n",
      "2083 [D loss: 0.505584, acc.: 66.67%] [G loss: 2.957331]\n",
      "2084 [D loss: 0.160568, acc.: 94.44%] [G loss: 3.829430]\n",
      "2085 [D loss: 0.697054, acc.: 77.78%] [G loss: 2.920103]\n",
      "2086 [D loss: 1.069968, acc.: 44.44%] [G loss: 3.149924]\n",
      "2087 [D loss: 0.055123, acc.: 100.00%] [G loss: 3.887303]\n",
      "2088 [D loss: 0.437631, acc.: 77.78%] [G loss: 3.729250]\n",
      "2089 [D loss: 0.511708, acc.: 77.78%] [G loss: 2.196327]\n",
      "2090 [D loss: 0.582584, acc.: 88.89%] [G loss: 2.709522]\n",
      "2091 [D loss: 0.657326, acc.: 77.78%] [G loss: 3.121102]\n",
      "2092 [D loss: 0.347517, acc.: 88.89%] [G loss: 3.192076]\n",
      "2093 [D loss: 0.751751, acc.: 72.22%] [G loss: 3.483330]\n",
      "2094 [D loss: 0.951826, acc.: 61.11%] [G loss: 2.481197]\n",
      "2095 [D loss: 0.418291, acc.: 83.33%] [G loss: 3.168929]\n",
      "2096 [D loss: 0.298864, acc.: 94.44%] [G loss: 4.272706]\n",
      "2097 [D loss: 0.248203, acc.: 83.33%] [G loss: 4.107752]\n",
      "2098 [D loss: 0.325651, acc.: 83.33%] [G loss: 3.699449]\n",
      "2099 [D loss: 0.633915, acc.: 66.67%] [G loss: 2.605031]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100 [D loss: 0.283582, acc.: 83.33%] [G loss: 3.430012]\n",
      "2101 [D loss: 0.146042, acc.: 94.44%] [G loss: 3.470513]\n",
      "2102 [D loss: 0.418934, acc.: 72.22%] [G loss: 4.108246]\n",
      "2103 [D loss: 0.542366, acc.: 72.22%] [G loss: 2.357602]\n",
      "2104 [D loss: 0.590054, acc.: 66.67%] [G loss: 2.696692]\n",
      "2105 [D loss: 0.320361, acc.: 77.78%] [G loss: 2.361197]\n",
      "2106 [D loss: 0.174348, acc.: 94.44%] [G loss: 3.047712]\n",
      "2107 [D loss: 0.641424, acc.: 72.22%] [G loss: 3.089966]\n",
      "2108 [D loss: 0.492820, acc.: 77.78%] [G loss: 3.512418]\n",
      "2109 [D loss: 0.377976, acc.: 88.89%] [G loss: 3.535295]\n",
      "2110 [D loss: 0.352212, acc.: 94.44%] [G loss: 2.444525]\n",
      "2111 [D loss: 0.568002, acc.: 72.22%] [G loss: 2.447277]\n",
      "2112 [D loss: 0.235274, acc.: 88.89%] [G loss: 3.446771]\n",
      "2113 [D loss: 0.195967, acc.: 94.44%] [G loss: 3.892894]\n",
      "2114 [D loss: 0.318113, acc.: 88.89%] [G loss: 3.993390]\n",
      "2115 [D loss: 0.393681, acc.: 83.33%] [G loss: 1.944647]\n",
      "2116 [D loss: 0.124056, acc.: 100.00%] [G loss: 3.977756]\n",
      "2117 [D loss: 0.319030, acc.: 94.44%] [G loss: 3.310771]\n",
      "2118 [D loss: 0.175623, acc.: 88.89%] [G loss: 3.049600]\n",
      "2119 [D loss: 0.550318, acc.: 83.33%] [G loss: 2.811211]\n",
      "2120 [D loss: 0.989820, acc.: 44.44%] [G loss: 1.844201]\n",
      "2121 [D loss: 0.557446, acc.: 72.22%] [G loss: 2.478734]\n",
      "2122 [D loss: 0.402609, acc.: 88.89%] [G loss: 2.671431]\n",
      "2123 [D loss: 0.346328, acc.: 77.78%] [G loss: 3.576931]\n",
      "2124 [D loss: 0.371305, acc.: 88.89%] [G loss: 2.211263]\n",
      "2125 [D loss: 0.298724, acc.: 83.33%] [G loss: 3.589932]\n",
      "2126 [D loss: 0.918039, acc.: 55.56%] [G loss: 3.339365]\n",
      "2127 [D loss: 0.660036, acc.: 77.78%] [G loss: 2.828649]\n",
      "2128 [D loss: 0.463699, acc.: 72.22%] [G loss: 2.588426]\n",
      "2129 [D loss: 0.332239, acc.: 94.44%] [G loss: 3.603517]\n",
      "2130 [D loss: 0.210857, acc.: 94.44%] [G loss: 2.973745]\n",
      "2131 [D loss: 0.207086, acc.: 88.89%] [G loss: 4.268873]\n",
      "2132 [D loss: 0.352486, acc.: 83.33%] [G loss: 3.652871]\n",
      "2133 [D loss: 0.309666, acc.: 83.33%] [G loss: 4.088416]\n",
      "2134 [D loss: 0.293081, acc.: 83.33%] [G loss: 2.505707]\n",
      "2135 [D loss: 0.825603, acc.: 44.44%] [G loss: 2.680717]\n",
      "2136 [D loss: 0.252859, acc.: 88.89%] [G loss: 3.511680]\n",
      "2137 [D loss: 0.843051, acc.: 55.56%] [G loss: 3.311704]\n",
      "2138 [D loss: 0.360109, acc.: 83.33%] [G loss: 3.154584]\n",
      "2139 [D loss: 0.098245, acc.: 100.00%] [G loss: 3.229860]\n",
      "2140 [D loss: 0.518903, acc.: 72.22%] [G loss: 2.426225]\n",
      "2141 [D loss: 0.394791, acc.: 83.33%] [G loss: 2.821567]\n",
      "2142 [D loss: 0.617251, acc.: 77.78%] [G loss: 2.808456]\n",
      "2143 [D loss: 0.538850, acc.: 66.67%] [G loss: 1.850831]\n",
      "2144 [D loss: 0.411215, acc.: 88.89%] [G loss: 3.235694]\n",
      "2145 [D loss: 0.662806, acc.: 83.33%] [G loss: 2.935645]\n",
      "2146 [D loss: 0.261931, acc.: 94.44%] [G loss: 4.735757]\n",
      "2147 [D loss: 0.200092, acc.: 88.89%] [G loss: 3.046754]\n",
      "2148 [D loss: 0.276869, acc.: 88.89%] [G loss: 3.749006]\n",
      "2149 [D loss: 0.260044, acc.: 88.89%] [G loss: 3.205985]\n",
      "2150 [D loss: 0.476358, acc.: 77.78%] [G loss: 3.589643]\n",
      "2151 [D loss: 0.642097, acc.: 66.67%] [G loss: 3.132747]\n",
      "2152 [D loss: 0.241760, acc.: 94.44%] [G loss: 4.092704]\n",
      "2153 [D loss: 0.057597, acc.: 100.00%] [G loss: 4.089890]\n",
      "2154 [D loss: 0.313039, acc.: 88.89%] [G loss: 2.817630]\n",
      "2155 [D loss: 0.277505, acc.: 83.33%] [G loss: 3.086483]\n",
      "2156 [D loss: 0.300174, acc.: 88.89%] [G loss: 3.243361]\n",
      "2157 [D loss: 0.194588, acc.: 94.44%] [G loss: 4.690885]\n",
      "2158 [D loss: 0.136197, acc.: 94.44%] [G loss: 2.903351]\n",
      "2159 [D loss: 0.313611, acc.: 77.78%] [G loss: 2.898171]\n",
      "2160 [D loss: 0.184749, acc.: 100.00%] [G loss: 3.671259]\n",
      "2161 [D loss: 0.562186, acc.: 72.22%] [G loss: 3.005060]\n",
      "2162 [D loss: 0.704386, acc.: 61.11%] [G loss: 3.031705]\n",
      "2163 [D loss: 0.553461, acc.: 72.22%] [G loss: 2.575797]\n",
      "2164 [D loss: 0.618208, acc.: 66.67%] [G loss: 4.340215]\n",
      "2165 [D loss: 0.137633, acc.: 94.44%] [G loss: 6.112272]\n",
      "2166 [D loss: 0.446532, acc.: 72.22%] [G loss: 2.548361]\n",
      "2167 [D loss: 0.288086, acc.: 88.89%] [G loss: 3.278194]\n",
      "2168 [D loss: 0.255695, acc.: 88.89%] [G loss: 3.995288]\n",
      "2169 [D loss: 0.339601, acc.: 77.78%] [G loss: 3.937472]\n",
      "2170 [D loss: 1.031833, acc.: 38.89%] [G loss: 1.701341]\n",
      "2171 [D loss: 0.584559, acc.: 72.22%] [G loss: 2.744107]\n",
      "2172 [D loss: 0.448168, acc.: 77.78%] [G loss: 3.884493]\n",
      "2173 [D loss: 0.271229, acc.: 88.89%] [G loss: 2.914200]\n",
      "2174 [D loss: 0.335666, acc.: 83.33%] [G loss: 3.582879]\n",
      "2175 [D loss: 0.739276, acc.: 83.33%] [G loss: 3.947740]\n",
      "2176 [D loss: 0.301052, acc.: 83.33%] [G loss: 4.108422]\n",
      "2177 [D loss: 0.701159, acc.: 66.67%] [G loss: 3.266572]\n",
      "2178 [D loss: 0.867946, acc.: 77.78%] [G loss: 2.875145]\n",
      "2179 [D loss: 0.291968, acc.: 83.33%] [G loss: 4.705657]\n",
      "2180 [D loss: 0.409799, acc.: 66.67%] [G loss: 3.318339]\n",
      "2181 [D loss: 0.618330, acc.: 72.22%] [G loss: 2.124171]\n",
      "2182 [D loss: 0.229630, acc.: 94.44%] [G loss: 4.457674]\n",
      "2183 [D loss: 0.190291, acc.: 94.44%] [G loss: 3.316610]\n",
      "2184 [D loss: 0.198126, acc.: 88.89%] [G loss: 3.618757]\n",
      "2185 [D loss: 0.174640, acc.: 94.44%] [G loss: 3.664150]\n",
      "2186 [D loss: 0.270367, acc.: 94.44%] [G loss: 3.812785]\n",
      "2187 [D loss: 0.372688, acc.: 77.78%] [G loss: 2.351852]\n",
      "2188 [D loss: 0.363508, acc.: 83.33%] [G loss: 2.485309]\n",
      "2189 [D loss: 0.377681, acc.: 83.33%] [G loss: 2.785254]\n",
      "2190 [D loss: 0.493578, acc.: 72.22%] [G loss: 5.506322]\n",
      "2191 [D loss: 0.725422, acc.: 72.22%] [G loss: 3.962859]\n",
      "2192 [D loss: 0.580667, acc.: 77.78%] [G loss: 3.116406]\n",
      "2193 [D loss: 0.531894, acc.: 72.22%] [G loss: 2.081397]\n",
      "2194 [D loss: 0.391257, acc.: 77.78%] [G loss: 3.131771]\n",
      "2195 [D loss: 0.219146, acc.: 88.89%] [G loss: 3.499942]\n",
      "2196 [D loss: 0.397927, acc.: 83.33%] [G loss: 3.157303]\n",
      "2197 [D loss: 0.158695, acc.: 94.44%] [G loss: 3.076358]\n",
      "2198 [D loss: 0.317327, acc.: 72.22%] [G loss: 4.068800]\n",
      "2199 [D loss: 0.171650, acc.: 94.44%] [G loss: 3.153440]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200 [D loss: 0.369800, acc.: 83.33%] [G loss: 2.480622]\n",
      "2201 [D loss: 0.336945, acc.: 77.78%] [G loss: 3.552589]\n",
      "2202 [D loss: 0.494788, acc.: 83.33%] [G loss: 3.055671]\n",
      "2203 [D loss: 0.320295, acc.: 83.33%] [G loss: 2.556935]\n",
      "2204 [D loss: 0.772882, acc.: 77.78%] [G loss: 2.984367]\n",
      "2205 [D loss: 0.411140, acc.: 88.89%] [G loss: 3.252285]\n",
      "2206 [D loss: 0.275755, acc.: 77.78%] [G loss: 3.092655]\n",
      "2207 [D loss: 0.117142, acc.: 100.00%] [G loss: 4.347093]\n",
      "2208 [D loss: 0.088124, acc.: 100.00%] [G loss: 4.000132]\n",
      "2209 [D loss: 0.399421, acc.: 72.22%] [G loss: 3.093110]\n",
      "2210 [D loss: 0.593178, acc.: 77.78%] [G loss: 2.828112]\n",
      "2211 [D loss: 0.291464, acc.: 88.89%] [G loss: 3.406711]\n",
      "2212 [D loss: 0.505229, acc.: 88.89%] [G loss: 4.438129]\n",
      "2213 [D loss: 0.389163, acc.: 83.33%] [G loss: 2.896881]\n",
      "2214 [D loss: 0.114280, acc.: 94.44%] [G loss: 3.994470]\n",
      "2215 [D loss: 0.136431, acc.: 94.44%] [G loss: 3.875341]\n",
      "2216 [D loss: 0.590624, acc.: 72.22%] [G loss: 2.045500]\n",
      "2217 [D loss: 0.442853, acc.: 83.33%] [G loss: 2.823827]\n",
      "2218 [D loss: 0.366884, acc.: 72.22%] [G loss: 3.458364]\n",
      "2219 [D loss: 0.190473, acc.: 94.44%] [G loss: 2.657276]\n",
      "2220 [D loss: 0.294951, acc.: 77.78%] [G loss: 2.603774]\n",
      "2221 [D loss: 0.782158, acc.: 55.56%] [G loss: 2.679313]\n",
      "2222 [D loss: 0.461312, acc.: 72.22%] [G loss: 1.998911]\n",
      "2223 [D loss: 0.627896, acc.: 72.22%] [G loss: 3.689748]\n",
      "2224 [D loss: 0.344404, acc.: 83.33%] [G loss: 3.604602]\n",
      "2225 [D loss: 0.370954, acc.: 83.33%] [G loss: 3.126162]\n",
      "2226 [D loss: 0.673288, acc.: 83.33%] [G loss: 2.612930]\n",
      "2227 [D loss: 0.138648, acc.: 94.44%] [G loss: 5.672844]\n",
      "2228 [D loss: 0.199761, acc.: 94.44%] [G loss: 2.953454]\n",
      "2229 [D loss: 0.482080, acc.: 83.33%] [G loss: 3.109079]\n",
      "2230 [D loss: 0.527456, acc.: 77.78%] [G loss: 4.043008]\n",
      "2231 [D loss: 0.290426, acc.: 88.89%] [G loss: 3.434674]\n",
      "2232 [D loss: 0.607776, acc.: 72.22%] [G loss: 2.422324]\n",
      "2233 [D loss: 0.544530, acc.: 77.78%] [G loss: 3.240176]\n",
      "2234 [D loss: 0.611103, acc.: 66.67%] [G loss: 3.402333]\n",
      "2235 [D loss: 0.561935, acc.: 72.22%] [G loss: 2.106304]\n",
      "2236 [D loss: 0.316615, acc.: 83.33%] [G loss: 3.384428]\n",
      "2237 [D loss: 0.345350, acc.: 88.89%] [G loss: 4.173409]\n",
      "2238 [D loss: 0.712698, acc.: 72.22%] [G loss: 2.799695]\n",
      "2239 [D loss: 0.190569, acc.: 94.44%] [G loss: 3.652824]\n",
      "2240 [D loss: 0.352457, acc.: 77.78%] [G loss: 3.159746]\n",
      "2241 [D loss: 0.322432, acc.: 77.78%] [G loss: 3.660510]\n",
      "2242 [D loss: 0.165903, acc.: 100.00%] [G loss: 3.900323]\n",
      "2243 [D loss: 0.304132, acc.: 83.33%] [G loss: 3.419007]\n",
      "2244 [D loss: 0.376302, acc.: 83.33%] [G loss: 2.669890]\n",
      "2245 [D loss: 0.795338, acc.: 55.56%] [G loss: 2.340311]\n",
      "2246 [D loss: 0.351941, acc.: 77.78%] [G loss: 3.190616]\n",
      "2247 [D loss: 0.918395, acc.: 44.44%] [G loss: 4.053913]\n",
      "2248 [D loss: 0.416808, acc.: 77.78%] [G loss: 4.305467]\n",
      "2249 [D loss: 0.226107, acc.: 88.89%] [G loss: 3.659155]\n",
      "2250 [D loss: 0.294739, acc.: 88.89%] [G loss: 3.048416]\n",
      "2251 [D loss: 0.490061, acc.: 72.22%] [G loss: 3.544314]\n",
      "2252 [D loss: 0.392561, acc.: 77.78%] [G loss: 3.209252]\n",
      "2253 [D loss: 0.502901, acc.: 77.78%] [G loss: 2.192707]\n",
      "2254 [D loss: 0.161230, acc.: 100.00%] [G loss: 3.581823]\n",
      "2255 [D loss: 0.493109, acc.: 77.78%] [G loss: 2.867690]\n",
      "2256 [D loss: 0.471461, acc.: 83.33%] [G loss: 2.731690]\n",
      "2257 [D loss: 0.869650, acc.: 55.56%] [G loss: 3.106577]\n",
      "2258 [D loss: 0.449707, acc.: 83.33%] [G loss: 2.613907]\n",
      "2259 [D loss: 0.583965, acc.: 72.22%] [G loss: 3.029069]\n",
      "2260 [D loss: 0.264058, acc.: 88.89%] [G loss: 3.032730]\n",
      "2261 [D loss: 0.191070, acc.: 94.44%] [G loss: 2.816107]\n",
      "2262 [D loss: 0.365884, acc.: 88.89%] [G loss: 2.974021]\n",
      "2263 [D loss: 0.494345, acc.: 66.67%] [G loss: 3.133437]\n",
      "2264 [D loss: 0.631707, acc.: 77.78%] [G loss: 3.729171]\n",
      "2265 [D loss: 0.316045, acc.: 88.89%] [G loss: 2.790066]\n",
      "2266 [D loss: 0.463758, acc.: 77.78%] [G loss: 3.020600]\n",
      "2267 [D loss: 0.298215, acc.: 83.33%] [G loss: 3.221038]\n",
      "2268 [D loss: 1.006029, acc.: 61.11%] [G loss: 2.961549]\n",
      "2269 [D loss: 0.347828, acc.: 88.89%] [G loss: 5.744765]\n",
      "2270 [D loss: 0.330133, acc.: 94.44%] [G loss: 3.671192]\n",
      "2271 [D loss: 0.374909, acc.: 88.89%] [G loss: 1.866142]\n",
      "2272 [D loss: 0.117043, acc.: 94.44%] [G loss: 3.156625]\n",
      "2273 [D loss: 0.301879, acc.: 77.78%] [G loss: 2.373989]\n",
      "2274 [D loss: 0.402196, acc.: 77.78%] [G loss: 2.289891]\n",
      "2275 [D loss: 0.372459, acc.: 77.78%] [G loss: 3.000872]\n",
      "2276 [D loss: 0.131415, acc.: 100.00%] [G loss: 3.348333]\n",
      "2277 [D loss: 0.285702, acc.: 88.89%] [G loss: 4.024730]\n",
      "2278 [D loss: 0.896669, acc.: 50.00%] [G loss: 1.727284]\n",
      "2279 [D loss: 0.301603, acc.: 88.89%] [G loss: 3.034362]\n",
      "2280 [D loss: 0.291935, acc.: 88.89%] [G loss: 3.759765]\n",
      "2281 [D loss: 0.373612, acc.: 77.78%] [G loss: 4.228816]\n",
      "2282 [D loss: 0.260120, acc.: 83.33%] [G loss: 2.597691]\n",
      "2283 [D loss: 0.307333, acc.: 88.89%] [G loss: 3.538476]\n",
      "2284 [D loss: 0.267898, acc.: 88.89%] [G loss: 5.256861]\n",
      "2285 [D loss: 0.857031, acc.: 77.78%] [G loss: 3.300492]\n",
      "2286 [D loss: 0.409784, acc.: 83.33%] [G loss: 2.666809]\n",
      "2287 [D loss: 0.651082, acc.: 61.11%] [G loss: 2.640039]\n",
      "2288 [D loss: 0.306793, acc.: 88.89%] [G loss: 4.465498]\n",
      "2289 [D loss: 0.708045, acc.: 72.22%] [G loss: 2.678149]\n",
      "2290 [D loss: 0.104702, acc.: 94.44%] [G loss: 3.278110]\n",
      "2291 [D loss: 0.262984, acc.: 94.44%] [G loss: 3.759488]\n",
      "2292 [D loss: 0.677243, acc.: 72.22%] [G loss: 2.604270]\n",
      "2293 [D loss: 0.413753, acc.: 72.22%] [G loss: 1.957375]\n",
      "2294 [D loss: 0.121179, acc.: 94.44%] [G loss: 3.494977]\n",
      "2295 [D loss: 0.490424, acc.: 77.78%] [G loss: 3.394740]\n",
      "2296 [D loss: 0.239310, acc.: 94.44%] [G loss: 3.221902]\n",
      "2297 [D loss: 0.149252, acc.: 94.44%] [G loss: 2.781381]\n",
      "2298 [D loss: 0.176801, acc.: 94.44%] [G loss: 4.828001]\n",
      "2299 [D loss: 0.286454, acc.: 77.78%] [G loss: 4.046296]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2300 [D loss: 0.193520, acc.: 88.89%] [G loss: 3.940293]\n",
      "2301 [D loss: 0.651261, acc.: 66.67%] [G loss: 2.452966]\n",
      "2302 [D loss: 0.209484, acc.: 94.44%] [G loss: 3.517558]\n",
      "2303 [D loss: 0.245336, acc.: 88.89%] [G loss: 2.853203]\n",
      "2304 [D loss: 0.122546, acc.: 100.00%] [G loss: 3.940575]\n",
      "2305 [D loss: 0.236881, acc.: 88.89%] [G loss: 3.256648]\n",
      "2306 [D loss: 0.062159, acc.: 100.00%] [G loss: 5.128136]\n",
      "2307 [D loss: 0.393077, acc.: 88.89%] [G loss: 4.199592]\n",
      "2308 [D loss: 0.411913, acc.: 83.33%] [G loss: 3.542172]\n",
      "2309 [D loss: 0.221605, acc.: 88.89%] [G loss: 4.347761]\n",
      "2310 [D loss: 0.395105, acc.: 88.89%] [G loss: 2.941718]\n",
      "2311 [D loss: 0.169755, acc.: 94.44%] [G loss: 4.587341]\n",
      "2312 [D loss: 0.149155, acc.: 100.00%] [G loss: 3.522034]\n",
      "2313 [D loss: 0.090044, acc.: 94.44%] [G loss: 3.719702]\n",
      "2314 [D loss: 0.063314, acc.: 100.00%] [G loss: 3.960495]\n",
      "2315 [D loss: 0.043396, acc.: 100.00%] [G loss: 3.502133]\n",
      "2316 [D loss: 0.020907, acc.: 100.00%] [G loss: 5.437740]\n",
      "2317 [D loss: 0.415197, acc.: 77.78%] [G loss: 4.067980]\n",
      "2318 [D loss: 0.419048, acc.: 77.78%] [G loss: 3.092072]\n",
      "2319 [D loss: 0.217620, acc.: 94.44%] [G loss: 2.637801]\n",
      "2320 [D loss: 0.302370, acc.: 77.78%] [G loss: 3.650367]\n",
      "2321 [D loss: 0.134536, acc.: 94.44%] [G loss: 3.526204]\n",
      "2322 [D loss: 0.636871, acc.: 88.89%] [G loss: 2.220944]\n",
      "2323 [D loss: 0.289542, acc.: 77.78%] [G loss: 3.410716]\n",
      "2324 [D loss: 0.115892, acc.: 94.44%] [G loss: 5.016012]\n",
      "2325 [D loss: 0.396361, acc.: 88.89%] [G loss: 4.080651]\n",
      "2326 [D loss: 0.101980, acc.: 94.44%] [G loss: 3.884986]\n",
      "2327 [D loss: 0.318885, acc.: 88.89%] [G loss: 3.402697]\n",
      "2328 [D loss: 0.328010, acc.: 77.78%] [G loss: 3.383306]\n",
      "2329 [D loss: 0.519276, acc.: 77.78%] [G loss: 2.565471]\n",
      "2330 [D loss: 0.153827, acc.: 94.44%] [G loss: 4.408688]\n",
      "2331 [D loss: 0.230995, acc.: 94.44%] [G loss: 2.079659]\n",
      "2332 [D loss: 0.360531, acc.: 88.89%] [G loss: 3.185451]\n",
      "2333 [D loss: 0.193221, acc.: 100.00%] [G loss: 4.076433]\n",
      "2334 [D loss: 0.364110, acc.: 77.78%] [G loss: 3.313112]\n",
      "2335 [D loss: 0.381099, acc.: 83.33%] [G loss: 2.862833]\n",
      "2336 [D loss: 0.256865, acc.: 88.89%] [G loss: 4.173684]\n",
      "2337 [D loss: 0.206439, acc.: 88.89%] [G loss: 3.644029]\n",
      "2338 [D loss: 0.507989, acc.: 77.78%] [G loss: 4.237038]\n",
      "2339 [D loss: 0.267887, acc.: 83.33%] [G loss: 3.240796]\n",
      "2340 [D loss: 0.337615, acc.: 77.78%] [G loss: 3.833005]\n",
      "2341 [D loss: 0.288651, acc.: 83.33%] [G loss: 3.892591]\n",
      "2342 [D loss: 0.563618, acc.: 77.78%] [G loss: 4.207644]\n",
      "2343 [D loss: 0.320462, acc.: 83.33%] [G loss: 3.343976]\n",
      "2344 [D loss: 0.601600, acc.: 77.78%] [G loss: 2.749558]\n",
      "2345 [D loss: 0.571079, acc.: 83.33%] [G loss: 2.270062]\n",
      "2346 [D loss: 0.180652, acc.: 88.89%] [G loss: 4.175983]\n",
      "2347 [D loss: 1.038614, acc.: 61.11%] [G loss: 1.869000]\n",
      "2348 [D loss: 0.510278, acc.: 83.33%] [G loss: 2.660059]\n",
      "2349 [D loss: 0.291823, acc.: 83.33%] [G loss: 4.687551]\n",
      "2350 [D loss: 0.611946, acc.: 61.11%] [G loss: 2.238824]\n",
      "2351 [D loss: 0.188247, acc.: 94.44%] [G loss: 3.546411]\n",
      "2352 [D loss: 0.392660, acc.: 83.33%] [G loss: 3.086591]\n",
      "2353 [D loss: 0.296149, acc.: 77.78%] [G loss: 5.168484]\n",
      "2354 [D loss: 0.547395, acc.: 83.33%] [G loss: 3.943744]\n",
      "2355 [D loss: 0.322380, acc.: 77.78%] [G loss: 4.687020]\n",
      "2356 [D loss: 0.425678, acc.: 83.33%] [G loss: 4.706536]\n",
      "2357 [D loss: 0.139416, acc.: 94.44%] [G loss: 3.649230]\n",
      "2358 [D loss: 0.098863, acc.: 94.44%] [G loss: 3.795456]\n",
      "2359 [D loss: 0.277982, acc.: 88.89%] [G loss: 2.900303]\n",
      "2360 [D loss: 0.435246, acc.: 77.78%] [G loss: 2.263890]\n",
      "2361 [D loss: 0.415919, acc.: 77.78%] [G loss: 2.430001]\n",
      "2362 [D loss: 0.200682, acc.: 88.89%] [G loss: 4.296193]\n",
      "2363 [D loss: 0.449742, acc.: 66.67%] [G loss: 3.048892]\n",
      "2364 [D loss: 0.509063, acc.: 83.33%] [G loss: 3.258158]\n",
      "2365 [D loss: 0.277180, acc.: 88.89%] [G loss: 3.074183]\n",
      "2366 [D loss: 0.266661, acc.: 88.89%] [G loss: 3.923200]\n",
      "2367 [D loss: 0.458727, acc.: 77.78%] [G loss: 3.324443]\n",
      "2368 [D loss: 0.412755, acc.: 88.89%] [G loss: 2.199679]\n",
      "2369 [D loss: 0.417941, acc.: 83.33%] [G loss: 3.587799]\n",
      "2370 [D loss: 0.321535, acc.: 83.33%] [G loss: 3.286246]\n",
      "2371 [D loss: 0.295005, acc.: 88.89%] [G loss: 3.158732]\n",
      "2372 [D loss: 0.305869, acc.: 83.33%] [G loss: 3.633119]\n",
      "2373 [D loss: 0.430536, acc.: 77.78%] [G loss: 2.832900]\n",
      "2374 [D loss: 0.851586, acc.: 55.56%] [G loss: 1.984352]\n",
      "2375 [D loss: 0.570270, acc.: 72.22%] [G loss: 3.033587]\n",
      "2376 [D loss: 0.173512, acc.: 94.44%] [G loss: 4.736377]\n",
      "2377 [D loss: 0.118391, acc.: 100.00%] [G loss: 4.410036]\n",
      "2378 [D loss: 0.870825, acc.: 61.11%] [G loss: 4.463825]\n",
      "2379 [D loss: 0.384509, acc.: 83.33%] [G loss: 2.928002]\n",
      "2380 [D loss: 0.439190, acc.: 83.33%] [G loss: 3.145046]\n",
      "2381 [D loss: 0.419059, acc.: 83.33%] [G loss: 3.681012]\n",
      "2382 [D loss: 1.060599, acc.: 61.11%] [G loss: 3.044646]\n",
      "2383 [D loss: 0.527034, acc.: 72.22%] [G loss: 2.769818]\n",
      "2384 [D loss: 0.227135, acc.: 88.89%] [G loss: 3.552302]\n",
      "2385 [D loss: 0.221069, acc.: 83.33%] [G loss: 3.918312]\n",
      "2386 [D loss: 0.136135, acc.: 94.44%] [G loss: 3.818357]\n",
      "2387 [D loss: 0.265074, acc.: 88.89%] [G loss: 2.882631]\n",
      "2388 [D loss: 0.752618, acc.: 66.67%] [G loss: 2.278556]\n",
      "2389 [D loss: 0.720853, acc.: 61.11%] [G loss: 2.838361]\n",
      "2390 [D loss: 0.205979, acc.: 88.89%] [G loss: 3.627666]\n",
      "2391 [D loss: 0.431404, acc.: 72.22%] [G loss: 2.252244]\n",
      "2392 [D loss: 0.502026, acc.: 61.11%] [G loss: 3.636918]\n",
      "2393 [D loss: 0.293949, acc.: 88.89%] [G loss: 3.194185]\n",
      "2394 [D loss: 0.245079, acc.: 88.89%] [G loss: 3.887130]\n",
      "2395 [D loss: 0.448908, acc.: 77.78%] [G loss: 3.991258]\n",
      "2396 [D loss: 0.580844, acc.: 77.78%] [G loss: 2.616196]\n",
      "2397 [D loss: 0.264820, acc.: 88.89%] [G loss: 3.567023]\n",
      "2398 [D loss: 0.223498, acc.: 94.44%] [G loss: 3.710614]\n",
      "2399 [D loss: 0.306858, acc.: 83.33%] [G loss: 2.580298]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400 [D loss: 0.278033, acc.: 88.89%] [G loss: 2.723577]\n",
      "2401 [D loss: 0.363599, acc.: 88.89%] [G loss: 2.685071]\n",
      "2402 [D loss: 0.108545, acc.: 94.44%] [G loss: 4.551743]\n",
      "2403 [D loss: 0.213291, acc.: 94.44%] [G loss: 2.906407]\n",
      "2404 [D loss: 0.372944, acc.: 77.78%] [G loss: 2.750870]\n",
      "2405 [D loss: 0.122073, acc.: 100.00%] [G loss: 3.479324]\n",
      "2406 [D loss: 0.572915, acc.: 72.22%] [G loss: 3.227525]\n",
      "2407 [D loss: 0.180688, acc.: 94.44%] [G loss: 2.548974]\n",
      "2408 [D loss: 0.394552, acc.: 83.33%] [G loss: 2.816393]\n",
      "2409 [D loss: 0.580000, acc.: 72.22%] [G loss: 3.809183]\n",
      "2410 [D loss: 0.301353, acc.: 94.44%] [G loss: 3.564721]\n",
      "2411 [D loss: 0.258208, acc.: 88.89%] [G loss: 3.769814]\n",
      "2412 [D loss: 0.131877, acc.: 94.44%] [G loss: 2.722118]\n",
      "2413 [D loss: 0.193395, acc.: 94.44%] [G loss: 2.648413]\n",
      "2414 [D loss: 0.169221, acc.: 88.89%] [G loss: 3.510656]\n",
      "2415 [D loss: 0.189927, acc.: 94.44%] [G loss: 2.817461]\n",
      "2416 [D loss: 0.202979, acc.: 94.44%] [G loss: 2.299773]\n",
      "2417 [D loss: 0.674788, acc.: 72.22%] [G loss: 3.163715]\n",
      "2418 [D loss: 0.572669, acc.: 72.22%] [G loss: 3.850272]\n",
      "2419 [D loss: 0.517122, acc.: 83.33%] [G loss: 3.789141]\n",
      "2420 [D loss: 0.661739, acc.: 72.22%] [G loss: 2.613945]\n",
      "2421 [D loss: 0.585071, acc.: 77.78%] [G loss: 3.478497]\n",
      "2422 [D loss: 0.126564, acc.: 100.00%] [G loss: 4.345292]\n",
      "2423 [D loss: 0.374829, acc.: 88.89%] [G loss: 3.555833]\n",
      "2424 [D loss: 0.318815, acc.: 88.89%] [G loss: 2.210193]\n",
      "2425 [D loss: 0.484387, acc.: 88.89%] [G loss: 3.684795]\n",
      "2426 [D loss: 0.379815, acc.: 77.78%] [G loss: 2.476499]\n",
      "2427 [D loss: 0.116597, acc.: 94.44%] [G loss: 3.882498]\n",
      "2428 [D loss: 0.212674, acc.: 88.89%] [G loss: 4.508975]\n",
      "2429 [D loss: 0.465486, acc.: 83.33%] [G loss: 3.389233]\n",
      "2430 [D loss: 0.245914, acc.: 83.33%] [G loss: 3.688004]\n",
      "2431 [D loss: 0.570061, acc.: 83.33%] [G loss: 3.788153]\n",
      "2432 [D loss: 0.859309, acc.: 61.11%] [G loss: 2.956266]\n",
      "2433 [D loss: 0.316496, acc.: 83.33%] [G loss: 4.265328]\n",
      "2434 [D loss: 0.335711, acc.: 88.89%] [G loss: 5.197208]\n",
      "2435 [D loss: 0.410014, acc.: 83.33%] [G loss: 3.318534]\n",
      "2436 [D loss: 0.326369, acc.: 94.44%] [G loss: 3.607570]\n",
      "2437 [D loss: 0.201060, acc.: 94.44%] [G loss: 3.993355]\n",
      "2438 [D loss: 0.226610, acc.: 88.89%] [G loss: 4.285635]\n",
      "2439 [D loss: 0.516991, acc.: 72.22%] [G loss: 4.687779]\n",
      "2440 [D loss: 0.122408, acc.: 94.44%] [G loss: 5.502513]\n",
      "2441 [D loss: 1.197892, acc.: 55.56%] [G loss: 2.451793]\n",
      "2442 [D loss: 0.350896, acc.: 88.89%] [G loss: 3.403912]\n",
      "2443 [D loss: 0.590287, acc.: 61.11%] [G loss: 3.106840]\n",
      "2444 [D loss: 0.594177, acc.: 66.67%] [G loss: 3.787689]\n",
      "2445 [D loss: 0.433141, acc.: 77.78%] [G loss: 3.519424]\n",
      "2446 [D loss: 0.313245, acc.: 88.89%] [G loss: 4.571220]\n",
      "2447 [D loss: 0.841954, acc.: 61.11%] [G loss: 2.555951]\n",
      "2448 [D loss: 0.682636, acc.: 77.78%] [G loss: 3.456756]\n",
      "2449 [D loss: 0.332401, acc.: 77.78%] [G loss: 4.336866]\n",
      "2450 [D loss: 0.245111, acc.: 94.44%] [G loss: 4.010843]\n",
      "2451 [D loss: 0.217690, acc.: 88.89%] [G loss: 3.522938]\n",
      "2452 [D loss: 0.363902, acc.: 83.33%] [G loss: 2.883014]\n",
      "2453 [D loss: 0.698488, acc.: 83.33%] [G loss: 2.613897]\n",
      "2454 [D loss: 1.189086, acc.: 66.67%] [G loss: 4.031330]\n",
      "2455 [D loss: 0.284244, acc.: 88.89%] [G loss: 5.227427]\n",
      "2456 [D loss: 0.494942, acc.: 88.89%] [G loss: 2.700313]\n",
      "2457 [D loss: 0.340861, acc.: 88.89%] [G loss: 3.105898]\n",
      "2458 [D loss: 0.472120, acc.: 83.33%] [G loss: 3.519171]\n",
      "2459 [D loss: 0.296553, acc.: 88.89%] [G loss: 3.047256]\n",
      "2460 [D loss: 0.683643, acc.: 77.78%] [G loss: 3.228182]\n",
      "2461 [D loss: 0.357447, acc.: 77.78%] [G loss: 3.220694]\n",
      "2462 [D loss: 0.992048, acc.: 61.11%] [G loss: 2.332581]\n",
      "2463 [D loss: 0.720502, acc.: 72.22%] [G loss: 2.657743]\n",
      "2464 [D loss: 0.253091, acc.: 88.89%] [G loss: 2.429524]\n",
      "2465 [D loss: 0.491835, acc.: 83.33%] [G loss: 4.165658]\n",
      "2466 [D loss: 1.039941, acc.: 61.11%] [G loss: 1.972228]\n",
      "2467 [D loss: 0.432069, acc.: 83.33%] [G loss: 3.798669]\n",
      "2468 [D loss: 0.297148, acc.: 77.78%] [G loss: 4.350431]\n",
      "2469 [D loss: 0.197379, acc.: 94.44%] [G loss: 3.823289]\n",
      "2470 [D loss: 0.305431, acc.: 88.89%] [G loss: 3.216689]\n",
      "2471 [D loss: 0.397781, acc.: 77.78%] [G loss: 3.733557]\n",
      "2472 [D loss: 0.500835, acc.: 77.78%] [G loss: 4.018141]\n",
      "2473 [D loss: 0.225407, acc.: 88.89%] [G loss: 4.539731]\n",
      "2474 [D loss: 0.466794, acc.: 83.33%] [G loss: 3.828453]\n",
      "2475 [D loss: 0.765988, acc.: 77.78%] [G loss: 2.621093]\n",
      "2476 [D loss: 0.527364, acc.: 72.22%] [G loss: 3.847316]\n",
      "2477 [D loss: 0.610525, acc.: 61.11%] [G loss: 3.966359]\n",
      "2478 [D loss: 0.139513, acc.: 94.44%] [G loss: 3.040677]\n",
      "2479 [D loss: 0.607948, acc.: 77.78%] [G loss: 2.574732]\n",
      "2480 [D loss: 0.202796, acc.: 94.44%] [G loss: 4.498084]\n",
      "2481 [D loss: 0.318989, acc.: 88.89%] [G loss: 3.999707]\n",
      "2482 [D loss: 0.125054, acc.: 94.44%] [G loss: 4.212168]\n",
      "2483 [D loss: 0.303233, acc.: 83.33%] [G loss: 3.386795]\n",
      "2484 [D loss: 0.128271, acc.: 100.00%] [G loss: 3.055654]\n",
      "2485 [D loss: 0.254964, acc.: 94.44%] [G loss: 2.863565]\n",
      "2486 [D loss: 0.091482, acc.: 94.44%] [G loss: 3.719103]\n",
      "2487 [D loss: 0.246227, acc.: 88.89%] [G loss: 2.899817]\n",
      "2488 [D loss: 0.225921, acc.: 88.89%] [G loss: 4.219059]\n",
      "2489 [D loss: 0.391333, acc.: 83.33%] [G loss: 3.544231]\n",
      "2490 [D loss: 0.155820, acc.: 94.44%] [G loss: 3.907875]\n",
      "2491 [D loss: 0.442983, acc.: 72.22%] [G loss: 3.100662]\n",
      "2492 [D loss: 0.295196, acc.: 77.78%] [G loss: 3.601148]\n",
      "2493 [D loss: 0.546558, acc.: 66.67%] [G loss: 1.758861]\n",
      "2494 [D loss: 0.109403, acc.: 94.44%] [G loss: 4.128211]\n",
      "2495 [D loss: 0.154596, acc.: 94.44%] [G loss: 2.843981]\n",
      "2496 [D loss: 0.380657, acc.: 77.78%] [G loss: 3.127705]\n",
      "2497 [D loss: 0.680309, acc.: 61.11%] [G loss: 2.618344]\n",
      "2498 [D loss: 0.539760, acc.: 77.78%] [G loss: 2.784896]\n",
      "2499 [D loss: 0.440877, acc.: 77.78%] [G loss: 2.520906]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 [D loss: 0.496254, acc.: 72.22%] [G loss: 2.158628]\n",
      "2501 [D loss: 0.350365, acc.: 94.44%] [G loss: 3.206887]\n",
      "2502 [D loss: 0.757277, acc.: 61.11%] [G loss: 2.409808]\n",
      "2503 [D loss: 0.216301, acc.: 100.00%] [G loss: 3.397776]\n",
      "2504 [D loss: 0.718604, acc.: 66.67%] [G loss: 3.522650]\n",
      "2505 [D loss: 0.213418, acc.: 94.44%] [G loss: 3.348187]\n",
      "2506 [D loss: 0.229942, acc.: 94.44%] [G loss: 4.166449]\n",
      "2507 [D loss: 0.355386, acc.: 88.89%] [G loss: 3.047667]\n",
      "2508 [D loss: 0.200041, acc.: 100.00%] [G loss: 3.982595]\n",
      "2509 [D loss: 0.181900, acc.: 88.89%] [G loss: 4.809397]\n",
      "2510 [D loss: 0.913674, acc.: 72.22%] [G loss: 2.470047]\n",
      "2511 [D loss: 0.280299, acc.: 83.33%] [G loss: 2.851711]\n",
      "2512 [D loss: 0.255320, acc.: 88.89%] [G loss: 2.511695]\n",
      "2513 [D loss: 0.131728, acc.: 94.44%] [G loss: 3.894165]\n",
      "2514 [D loss: 0.346834, acc.: 77.78%] [G loss: 2.280159]\n",
      "2515 [D loss: 0.307326, acc.: 88.89%] [G loss: 4.248372]\n",
      "2516 [D loss: 0.299171, acc.: 88.89%] [G loss: 3.128469]\n",
      "2517 [D loss: 0.259896, acc.: 88.89%] [G loss: 4.146032]\n",
      "2518 [D loss: 0.138728, acc.: 94.44%] [G loss: 5.715775]\n",
      "2519 [D loss: 0.284190, acc.: 77.78%] [G loss: 4.264422]\n",
      "2520 [D loss: 0.148641, acc.: 100.00%] [G loss: 3.429442]\n",
      "2521 [D loss: 0.256122, acc.: 94.44%] [G loss: 3.119945]\n",
      "2522 [D loss: 0.124404, acc.: 100.00%] [G loss: 2.742698]\n",
      "2523 [D loss: 0.291783, acc.: 83.33%] [G loss: 3.029689]\n",
      "2524 [D loss: 0.284376, acc.: 77.78%] [G loss: 3.368499]\n",
      "2525 [D loss: 0.382600, acc.: 88.89%] [G loss: 2.934273]\n",
      "2526 [D loss: 0.185941, acc.: 94.44%] [G loss: 3.296960]\n",
      "2527 [D loss: 0.165776, acc.: 88.89%] [G loss: 3.660120]\n",
      "2528 [D loss: 0.309627, acc.: 83.33%] [G loss: 3.222167]\n",
      "2529 [D loss: 0.163347, acc.: 94.44%] [G loss: 5.356868]\n",
      "2530 [D loss: 0.466456, acc.: 77.78%] [G loss: 3.357437]\n",
      "2531 [D loss: 0.423674, acc.: 77.78%] [G loss: 3.243566]\n",
      "2532 [D loss: 0.503815, acc.: 83.33%] [G loss: 3.483470]\n",
      "2533 [D loss: 0.787528, acc.: 61.11%] [G loss: 2.628294]\n",
      "2534 [D loss: 0.109956, acc.: 100.00%] [G loss: 4.090908]\n",
      "2535 [D loss: 0.106145, acc.: 100.00%] [G loss: 2.622741]\n",
      "2536 [D loss: 0.563429, acc.: 88.89%] [G loss: 2.301736]\n",
      "2537 [D loss: 0.345279, acc.: 88.89%] [G loss: 4.084855]\n",
      "2538 [D loss: 0.499529, acc.: 72.22%] [G loss: 3.574801]\n",
      "2539 [D loss: 0.255978, acc.: 88.89%] [G loss: 4.784957]\n",
      "2540 [D loss: 0.046034, acc.: 100.00%] [G loss: 4.174459]\n",
      "2541 [D loss: 0.493545, acc.: 83.33%] [G loss: 3.986339]\n",
      "2542 [D loss: 0.491607, acc.: 77.78%] [G loss: 2.892531]\n",
      "2543 [D loss: 0.114348, acc.: 94.44%] [G loss: 3.049382]\n",
      "2544 [D loss: 0.569808, acc.: 77.78%] [G loss: 3.021587]\n",
      "2545 [D loss: 0.574348, acc.: 72.22%] [G loss: 3.096847]\n",
      "2546 [D loss: 0.324726, acc.: 88.89%] [G loss: 4.106541]\n",
      "2547 [D loss: 0.985539, acc.: 55.56%] [G loss: 2.081473]\n",
      "2548 [D loss: 0.391572, acc.: 94.44%] [G loss: 2.095218]\n",
      "2549 [D loss: 0.745495, acc.: 66.67%] [G loss: 3.050323]\n",
      "2550 [D loss: 0.243729, acc.: 88.89%] [G loss: 2.909391]\n",
      "2551 [D loss: 0.485521, acc.: 77.78%] [G loss: 3.230228]\n",
      "2552 [D loss: 0.206472, acc.: 83.33%] [G loss: 3.532184]\n",
      "2553 [D loss: 0.203631, acc.: 88.89%] [G loss: 3.729907]\n",
      "2554 [D loss: 0.268588, acc.: 94.44%] [G loss: 4.095573]\n",
      "2555 [D loss: 0.295765, acc.: 88.89%] [G loss: 3.597680]\n",
      "2556 [D loss: 0.177548, acc.: 88.89%] [G loss: 4.698594]\n",
      "2557 [D loss: 0.307278, acc.: 88.89%] [G loss: 4.161342]\n",
      "2558 [D loss: 0.143996, acc.: 100.00%] [G loss: 2.961612]\n",
      "2559 [D loss: 0.192868, acc.: 94.44%] [G loss: 3.654206]\n",
      "2560 [D loss: 0.599428, acc.: 83.33%] [G loss: 3.221436]\n",
      "2561 [D loss: 0.273451, acc.: 88.89%] [G loss: 4.482813]\n",
      "2562 [D loss: 0.207812, acc.: 88.89%] [G loss: 4.060781]\n",
      "2563 [D loss: 0.574968, acc.: 66.67%] [G loss: 3.430699]\n",
      "2564 [D loss: 0.206165, acc.: 100.00%] [G loss: 2.369570]\n",
      "2565 [D loss: 0.191714, acc.: 88.89%] [G loss: 2.985041]\n",
      "2566 [D loss: 0.177896, acc.: 94.44%] [G loss: 3.781662]\n",
      "2567 [D loss: 0.302561, acc.: 83.33%] [G loss: 3.957845]\n",
      "2568 [D loss: 0.328309, acc.: 88.89%] [G loss: 3.470036]\n",
      "2569 [D loss: 0.085297, acc.: 100.00%] [G loss: 4.394707]\n",
      "2570 [D loss: 0.399587, acc.: 66.67%] [G loss: 2.972042]\n",
      "2571 [D loss: 0.258334, acc.: 88.89%] [G loss: 3.062723]\n",
      "2572 [D loss: 0.410945, acc.: 88.89%] [G loss: 3.780256]\n",
      "2573 [D loss: 0.596470, acc.: 66.67%] [G loss: 2.944639]\n",
      "2574 [D loss: 0.344121, acc.: 77.78%] [G loss: 2.152471]\n",
      "2575 [D loss: 0.471340, acc.: 72.22%] [G loss: 3.380728]\n",
      "2576 [D loss: 0.105445, acc.: 100.00%] [G loss: 4.648508]\n",
      "2577 [D loss: 0.108197, acc.: 94.44%] [G loss: 4.943177]\n",
      "2578 [D loss: 0.576435, acc.: 77.78%] [G loss: 3.121430]\n",
      "2579 [D loss: 0.307813, acc.: 83.33%] [G loss: 2.638069]\n",
      "2580 [D loss: 0.279485, acc.: 88.89%] [G loss: 3.952724]\n",
      "2581 [D loss: 0.657961, acc.: 61.11%] [G loss: 3.372216]\n",
      "2582 [D loss: 0.594924, acc.: 72.22%] [G loss: 3.383857]\n",
      "2583 [D loss: 0.362972, acc.: 88.89%] [G loss: 3.243316]\n",
      "2584 [D loss: 0.283098, acc.: 88.89%] [G loss: 3.621768]\n",
      "2585 [D loss: 0.169946, acc.: 94.44%] [G loss: 3.743775]\n",
      "2586 [D loss: 0.317647, acc.: 83.33%] [G loss: 4.187595]\n",
      "2587 [D loss: 0.332556, acc.: 88.89%] [G loss: 3.705673]\n",
      "2588 [D loss: 0.110698, acc.: 100.00%] [G loss: 3.793959]\n",
      "2589 [D loss: 0.243275, acc.: 83.33%] [G loss: 2.449223]\n",
      "2590 [D loss: 1.057794, acc.: 61.11%] [G loss: 3.021349]\n",
      "2591 [D loss: 0.294451, acc.: 88.89%] [G loss: 3.401299]\n",
      "2592 [D loss: 0.464814, acc.: 83.33%] [G loss: 3.755757]\n",
      "2593 [D loss: 0.512294, acc.: 83.33%] [G loss: 3.735978]\n",
      "2594 [D loss: 0.101910, acc.: 94.44%] [G loss: 5.109048]\n",
      "2595 [D loss: 0.434396, acc.: 88.89%] [G loss: 3.908440]\n",
      "2596 [D loss: 0.243393, acc.: 88.89%] [G loss: 2.869363]\n",
      "2597 [D loss: 0.604099, acc.: 77.78%] [G loss: 4.664606]\n",
      "2598 [D loss: 0.089364, acc.: 100.00%] [G loss: 5.021931]\n",
      "2599 [D loss: 0.242004, acc.: 94.44%] [G loss: 3.180331]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2600 [D loss: 0.532243, acc.: 66.67%] [G loss: 1.971481]\n",
      "2601 [D loss: 0.253753, acc.: 88.89%] [G loss: 3.254307]\n",
      "2602 [D loss: 0.406455, acc.: 88.89%] [G loss: 2.558654]\n",
      "2603 [D loss: 0.169069, acc.: 100.00%] [G loss: 3.527396]\n",
      "2604 [D loss: 0.220526, acc.: 88.89%] [G loss: 4.208015]\n",
      "2605 [D loss: 0.159801, acc.: 94.44%] [G loss: 4.384448]\n",
      "2606 [D loss: 0.310902, acc.: 83.33%] [G loss: 4.388874]\n",
      "2607 [D loss: 0.233734, acc.: 94.44%] [G loss: 4.083988]\n",
      "2608 [D loss: 0.156736, acc.: 94.44%] [G loss: 3.675847]\n",
      "2609 [D loss: 0.189737, acc.: 94.44%] [G loss: 3.287284]\n",
      "2610 [D loss: 0.253728, acc.: 94.44%] [G loss: 2.909215]\n",
      "2611 [D loss: 0.143278, acc.: 94.44%] [G loss: 3.187536]\n",
      "2612 [D loss: 0.387538, acc.: 83.33%] [G loss: 2.750303]\n",
      "2613 [D loss: 0.331912, acc.: 88.89%] [G loss: 3.228810]\n",
      "2614 [D loss: 0.509780, acc.: 77.78%] [G loss: 4.209414]\n",
      "2615 [D loss: 0.282182, acc.: 83.33%] [G loss: 4.036925]\n",
      "2616 [D loss: 0.169824, acc.: 94.44%] [G loss: 3.915511]\n",
      "2617 [D loss: 0.253374, acc.: 94.44%] [G loss: 3.992377]\n",
      "2618 [D loss: 0.125234, acc.: 100.00%] [G loss: 3.937743]\n",
      "2619 [D loss: 0.313540, acc.: 77.78%] [G loss: 3.612032]\n",
      "2620 [D loss: 0.527675, acc.: 66.67%] [G loss: 4.165104]\n",
      "2621 [D loss: 0.111213, acc.: 94.44%] [G loss: 3.824378]\n",
      "2622 [D loss: 0.093033, acc.: 100.00%] [G loss: 3.518878]\n",
      "2623 [D loss: 0.459531, acc.: 77.78%] [G loss: 1.881240]\n",
      "2624 [D loss: 0.155060, acc.: 94.44%] [G loss: 2.826596]\n",
      "2625 [D loss: 0.160454, acc.: 100.00%] [G loss: 4.309626]\n",
      "2626 [D loss: 0.877496, acc.: 61.11%] [G loss: 3.415587]\n",
      "2627 [D loss: 0.935207, acc.: 66.67%] [G loss: 2.977017]\n",
      "2628 [D loss: 0.045081, acc.: 100.00%] [G loss: 4.584120]\n",
      "2629 [D loss: 0.850874, acc.: 66.67%] [G loss: 2.760700]\n",
      "2630 [D loss: 0.146217, acc.: 94.44%] [G loss: 3.636328]\n",
      "2631 [D loss: 0.272291, acc.: 83.33%] [G loss: 3.693149]\n",
      "2632 [D loss: 0.321905, acc.: 88.89%] [G loss: 3.401686]\n",
      "2633 [D loss: 0.212523, acc.: 88.89%] [G loss: 3.058025]\n",
      "2634 [D loss: 0.187702, acc.: 94.44%] [G loss: 3.101770]\n",
      "2635 [D loss: 0.466859, acc.: 83.33%] [G loss: 3.502819]\n",
      "2636 [D loss: 0.321466, acc.: 83.33%] [G loss: 3.340729]\n",
      "2637 [D loss: 0.358666, acc.: 83.33%] [G loss: 4.179890]\n",
      "2638 [D loss: 0.510481, acc.: 77.78%] [G loss: 4.207464]\n",
      "2639 [D loss: 0.271207, acc.: 83.33%] [G loss: 4.061519]\n",
      "2640 [D loss: 0.723730, acc.: 77.78%] [G loss: 4.019819]\n",
      "2641 [D loss: 0.267946, acc.: 88.89%] [G loss: 4.521779]\n",
      "2642 [D loss: 0.222956, acc.: 94.44%] [G loss: 3.477909]\n",
      "2643 [D loss: 0.242798, acc.: 88.89%] [G loss: 2.948788]\n",
      "2644 [D loss: 0.341328, acc.: 77.78%] [G loss: 3.510015]\n",
      "2645 [D loss: 0.649412, acc.: 72.22%] [G loss: 4.761392]\n",
      "2646 [D loss: 0.927694, acc.: 55.56%] [G loss: 4.069527]\n",
      "2647 [D loss: 0.742942, acc.: 61.11%] [G loss: 4.203672]\n",
      "2648 [D loss: 0.434734, acc.: 88.89%] [G loss: 4.604432]\n",
      "2649 [D loss: 0.372912, acc.: 83.33%] [G loss: 3.940727]\n",
      "2650 [D loss: 0.270405, acc.: 83.33%] [G loss: 3.574776]\n",
      "2651 [D loss: 0.744533, acc.: 77.78%] [G loss: 3.675307]\n",
      "2652 [D loss: 0.285223, acc.: 83.33%] [G loss: 4.096262]\n",
      "2653 [D loss: 0.743787, acc.: 55.56%] [G loss: 2.151177]\n",
      "2654 [D loss: 0.117975, acc.: 94.44%] [G loss: 5.626121]\n",
      "2655 [D loss: 0.472222, acc.: 83.33%] [G loss: 3.470243]\n",
      "2656 [D loss: 0.228703, acc.: 94.44%] [G loss: 3.244144]\n",
      "2657 [D loss: 0.099059, acc.: 94.44%] [G loss: 3.979787]\n",
      "2658 [D loss: 0.315075, acc.: 77.78%] [G loss: 3.958034]\n",
      "2659 [D loss: 0.252577, acc.: 88.89%] [G loss: 4.641328]\n",
      "2660 [D loss: 0.620376, acc.: 77.78%] [G loss: 4.886715]\n",
      "2661 [D loss: 0.117366, acc.: 100.00%] [G loss: 2.897005]\n",
      "2662 [D loss: 0.322874, acc.: 77.78%] [G loss: 3.780252]\n",
      "2663 [D loss: 0.236492, acc.: 94.44%] [G loss: 3.444333]\n",
      "2664 [D loss: 0.420559, acc.: 88.89%] [G loss: 3.482643]\n",
      "2665 [D loss: 0.294191, acc.: 94.44%] [G loss: 3.937556]\n",
      "2666 [D loss: 0.211102, acc.: 88.89%] [G loss: 2.986580]\n",
      "2667 [D loss: 0.646029, acc.: 77.78%] [G loss: 2.493870]\n",
      "2668 [D loss: 0.522710, acc.: 77.78%] [G loss: 2.570889]\n",
      "2669 [D loss: 0.417255, acc.: 77.78%] [G loss: 2.967100]\n",
      "2670 [D loss: 0.092855, acc.: 100.00%] [G loss: 5.173797]\n",
      "2671 [D loss: 1.204919, acc.: 61.11%] [G loss: 2.666575]\n",
      "2672 [D loss: 0.702146, acc.: 72.22%] [G loss: 3.533360]\n",
      "2673 [D loss: 0.139847, acc.: 94.44%] [G loss: 4.914524]\n",
      "2674 [D loss: 0.259076, acc.: 83.33%] [G loss: 3.927553]\n",
      "2675 [D loss: 0.390013, acc.: 83.33%] [G loss: 3.843298]\n",
      "2676 [D loss: 0.544827, acc.: 83.33%] [G loss: 3.423924]\n",
      "2677 [D loss: 0.312734, acc.: 83.33%] [G loss: 4.419907]\n",
      "2678 [D loss: 0.183191, acc.: 88.89%] [G loss: 5.655994]\n",
      "2679 [D loss: 0.262043, acc.: 88.89%] [G loss: 3.617925]\n",
      "2680 [D loss: 0.321099, acc.: 88.89%] [G loss: 4.102249]\n",
      "2681 [D loss: 0.420432, acc.: 77.78%] [G loss: 3.972524]\n",
      "2682 [D loss: 0.110520, acc.: 100.00%] [G loss: 3.843781]\n",
      "2683 [D loss: 0.387066, acc.: 83.33%] [G loss: 2.528637]\n",
      "2684 [D loss: 0.608306, acc.: 77.78%] [G loss: 2.783829]\n",
      "2685 [D loss: 0.430674, acc.: 72.22%] [G loss: 2.596897]\n",
      "2686 [D loss: 0.146868, acc.: 88.89%] [G loss: 5.300835]\n",
      "2687 [D loss: 0.046147, acc.: 100.00%] [G loss: 4.051166]\n",
      "2688 [D loss: 0.216086, acc.: 94.44%] [G loss: 3.010383]\n",
      "2689 [D loss: 0.350096, acc.: 83.33%] [G loss: 2.708125]\n",
      "2690 [D loss: 0.130740, acc.: 94.44%] [G loss: 4.754109]\n",
      "2691 [D loss: 0.205034, acc.: 88.89%] [G loss: 2.878068]\n",
      "2692 [D loss: 0.227050, acc.: 94.44%] [G loss: 4.322457]\n",
      "2693 [D loss: 0.277435, acc.: 83.33%] [G loss: 4.141076]\n",
      "2694 [D loss: 0.835477, acc.: 61.11%] [G loss: 3.854837]\n",
      "2695 [D loss: 0.198556, acc.: 88.89%] [G loss: 3.926539]\n",
      "2696 [D loss: 0.707996, acc.: 77.78%] [G loss: 2.519586]\n",
      "2697 [D loss: 0.448013, acc.: 83.33%] [G loss: 3.761509]\n",
      "2698 [D loss: 0.069164, acc.: 100.00%] [G loss: 3.398677]\n",
      "2699 [D loss: 0.657072, acc.: 72.22%] [G loss: 3.439570]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700 [D loss: 0.454095, acc.: 72.22%] [G loss: 3.219067]\n",
      "2701 [D loss: 0.465010, acc.: 77.78%] [G loss: 4.014391]\n",
      "2702 [D loss: 0.286156, acc.: 83.33%] [G loss: 4.472677]\n",
      "2703 [D loss: 0.264728, acc.: 94.44%] [G loss: 3.095596]\n",
      "2704 [D loss: 0.671737, acc.: 72.22%] [G loss: 3.588973]\n",
      "2705 [D loss: 0.395618, acc.: 83.33%] [G loss: 2.997807]\n",
      "2706 [D loss: 0.320964, acc.: 88.89%] [G loss: 3.626518]\n",
      "2707 [D loss: 0.321377, acc.: 83.33%] [G loss: 6.592321]\n",
      "2708 [D loss: 0.084918, acc.: 100.00%] [G loss: 6.080602]\n",
      "2709 [D loss: 0.104615, acc.: 100.00%] [G loss: 5.313385]\n",
      "2710 [D loss: 0.662127, acc.: 55.56%] [G loss: 2.676887]\n",
      "2711 [D loss: 0.571188, acc.: 72.22%] [G loss: 2.813936]\n",
      "2712 [D loss: 0.369520, acc.: 83.33%] [G loss: 4.989213]\n",
      "2713 [D loss: 0.211169, acc.: 77.78%] [G loss: 5.569183]\n",
      "2714 [D loss: 0.336341, acc.: 83.33%] [G loss: 3.828506]\n",
      "2715 [D loss: 0.034914, acc.: 100.00%] [G loss: 5.592351]\n",
      "2716 [D loss: 0.216965, acc.: 88.89%] [G loss: 3.622499]\n",
      "2717 [D loss: 0.363457, acc.: 83.33%] [G loss: 2.997494]\n",
      "2718 [D loss: 0.082048, acc.: 94.44%] [G loss: 6.159197]\n",
      "2719 [D loss: 0.171772, acc.: 88.89%] [G loss: 4.998896]\n",
      "2720 [D loss: 0.480021, acc.: 66.67%] [G loss: 3.101264]\n",
      "2721 [D loss: 0.146775, acc.: 100.00%] [G loss: 2.893160]\n",
      "2722 [D loss: 0.346878, acc.: 94.44%] [G loss: 4.270759]\n",
      "2723 [D loss: 0.131225, acc.: 94.44%] [G loss: 3.821403]\n",
      "2724 [D loss: 0.648462, acc.: 72.22%] [G loss: 3.620025]\n",
      "2725 [D loss: 0.432038, acc.: 83.33%] [G loss: 2.486039]\n",
      "2726 [D loss: 0.373536, acc.: 88.89%] [G loss: 3.632687]\n",
      "2727 [D loss: 0.302710, acc.: 83.33%] [G loss: 3.070639]\n",
      "2728 [D loss: 0.669801, acc.: 83.33%] [G loss: 3.663594]\n",
      "2729 [D loss: 0.476791, acc.: 83.33%] [G loss: 3.530973]\n",
      "2730 [D loss: 0.115089, acc.: 100.00%] [G loss: 4.538627]\n",
      "2731 [D loss: 0.463394, acc.: 83.33%] [G loss: 3.443515]\n",
      "2732 [D loss: 0.099737, acc.: 100.00%] [G loss: 4.187317]\n",
      "2733 [D loss: 0.327326, acc.: 88.89%] [G loss: 2.905304]\n",
      "2734 [D loss: 0.663949, acc.: 66.67%] [G loss: 3.928477]\n",
      "2735 [D loss: 0.570941, acc.: 77.78%] [G loss: 2.137423]\n",
      "2736 [D loss: 0.114783, acc.: 94.44%] [G loss: 4.877852]\n",
      "2737 [D loss: 0.143626, acc.: 94.44%] [G loss: 4.549626]\n",
      "2738 [D loss: 0.120009, acc.: 100.00%] [G loss: 4.270072]\n",
      "2739 [D loss: 0.195507, acc.: 94.44%] [G loss: 4.645679]\n",
      "2740 [D loss: 0.174282, acc.: 88.89%] [G loss: 4.205656]\n",
      "2741 [D loss: 0.291335, acc.: 83.33%] [G loss: 3.192056]\n",
      "2742 [D loss: 0.196838, acc.: 94.44%] [G loss: 3.486728]\n",
      "2743 [D loss: 0.180468, acc.: 94.44%] [G loss: 3.783036]\n",
      "2744 [D loss: 0.805042, acc.: 55.56%] [G loss: 2.432533]\n",
      "2745 [D loss: 0.330684, acc.: 88.89%] [G loss: 3.594864]\n",
      "2746 [D loss: 0.060631, acc.: 100.00%] [G loss: 2.862476]\n",
      "2747 [D loss: 0.414358, acc.: 83.33%] [G loss: 3.737335]\n",
      "2748 [D loss: 0.492698, acc.: 77.78%] [G loss: 4.092204]\n",
      "2749 [D loss: 0.544872, acc.: 83.33%] [G loss: 4.415693]\n",
      "2750 [D loss: 0.546069, acc.: 77.78%] [G loss: 3.784166]\n",
      "2751 [D loss: 0.318406, acc.: 88.89%] [G loss: 4.154423]\n",
      "2752 [D loss: 0.226256, acc.: 88.89%] [G loss: 3.808880]\n",
      "2753 [D loss: 0.278538, acc.: 83.33%] [G loss: 4.591974]\n",
      "2754 [D loss: 0.534106, acc.: 72.22%] [G loss: 4.421892]\n",
      "2755 [D loss: 0.499345, acc.: 72.22%] [G loss: 3.292012]\n",
      "2756 [D loss: 0.034622, acc.: 100.00%] [G loss: 4.375463]\n",
      "2757 [D loss: 0.401418, acc.: 83.33%] [G loss: 3.974093]\n",
      "2758 [D loss: 0.509173, acc.: 77.78%] [G loss: 3.294082]\n",
      "2759 [D loss: 0.214974, acc.: 88.89%] [G loss: 3.027540]\n",
      "2760 [D loss: 0.156852, acc.: 88.89%] [G loss: 4.155344]\n",
      "2761 [D loss: 0.069176, acc.: 94.44%] [G loss: 4.432015]\n",
      "2762 [D loss: 0.133089, acc.: 94.44%] [G loss: 3.948047]\n",
      "2763 [D loss: 0.447931, acc.: 94.44%] [G loss: 3.316551]\n",
      "2764 [D loss: 0.303077, acc.: 77.78%] [G loss: 3.596302]\n",
      "2765 [D loss: 0.212972, acc.: 88.89%] [G loss: 4.820214]\n",
      "2766 [D loss: 0.488441, acc.: 83.33%] [G loss: 4.009623]\n",
      "2767 [D loss: 0.040238, acc.: 100.00%] [G loss: 3.656205]\n",
      "2768 [D loss: 0.189575, acc.: 94.44%] [G loss: 2.990726]\n",
      "2769 [D loss: 0.399863, acc.: 77.78%] [G loss: 4.632765]\n",
      "2770 [D loss: 0.174834, acc.: 94.44%] [G loss: 4.862046]\n",
      "2771 [D loss: 0.195147, acc.: 94.44%] [G loss: 3.934401]\n",
      "2772 [D loss: 0.222437, acc.: 88.89%] [G loss: 3.842511]\n",
      "2773 [D loss: 0.387113, acc.: 77.78%] [G loss: 2.809198]\n",
      "2774 [D loss: 0.176017, acc.: 88.89%] [G loss: 3.970407]\n",
      "2775 [D loss: 0.161127, acc.: 94.44%] [G loss: 3.716646]\n",
      "2776 [D loss: 0.055043, acc.: 100.00%] [G loss: 4.130382]\n",
      "2777 [D loss: 0.244316, acc.: 94.44%] [G loss: 3.471136]\n",
      "2778 [D loss: 0.733734, acc.: 66.67%] [G loss: 2.572037]\n",
      "2779 [D loss: 0.159041, acc.: 100.00%] [G loss: 3.559112]\n",
      "2780 [D loss: 0.093401, acc.: 100.00%] [G loss: 4.399698]\n",
      "2781 [D loss: 0.493199, acc.: 72.22%] [G loss: 3.067011]\n",
      "2782 [D loss: 0.067095, acc.: 100.00%] [G loss: 4.170546]\n",
      "2783 [D loss: 0.136423, acc.: 94.44%] [G loss: 3.577124]\n",
      "2784 [D loss: 0.545637, acc.: 66.67%] [G loss: 3.000375]\n",
      "2785 [D loss: 0.285372, acc.: 88.89%] [G loss: 4.537957]\n",
      "2786 [D loss: 0.180255, acc.: 94.44%] [G loss: 3.511771]\n",
      "2787 [D loss: 0.640782, acc.: 83.33%] [G loss: 4.177776]\n",
      "2788 [D loss: 0.622123, acc.: 66.67%] [G loss: 3.517446]\n",
      "2789 [D loss: 0.238542, acc.: 88.89%] [G loss: 3.427464]\n",
      "2790 [D loss: 0.284401, acc.: 83.33%] [G loss: 3.913562]\n",
      "2791 [D loss: 0.197400, acc.: 94.44%] [G loss: 3.818629]\n",
      "2792 [D loss: 0.311533, acc.: 88.89%] [G loss: 4.157034]\n",
      "2793 [D loss: 0.184731, acc.: 100.00%] [G loss: 3.079936]\n",
      "2794 [D loss: 0.447803, acc.: 88.89%] [G loss: 2.736508]\n",
      "2795 [D loss: 0.136148, acc.: 94.44%] [G loss: 5.697759]\n",
      "2796 [D loss: 0.232382, acc.: 88.89%] [G loss: 4.449869]\n",
      "2797 [D loss: 0.159784, acc.: 88.89%] [G loss: 3.510080]\n",
      "2798 [D loss: 0.257965, acc.: 83.33%] [G loss: 4.758468]\n",
      "2799 [D loss: 0.275181, acc.: 88.89%] [G loss: 3.349886]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800 [D loss: 0.206021, acc.: 88.89%] [G loss: 3.212380]\n",
      "2801 [D loss: 0.680145, acc.: 61.11%] [G loss: 4.024315]\n",
      "2802 [D loss: 0.723937, acc.: 72.22%] [G loss: 3.877901]\n",
      "2803 [D loss: 0.235184, acc.: 83.33%] [G loss: 3.715675]\n",
      "2804 [D loss: 0.182453, acc.: 94.44%] [G loss: 3.390644]\n",
      "2805 [D loss: 0.920408, acc.: 66.67%] [G loss: 3.564225]\n",
      "2806 [D loss: 0.071436, acc.: 100.00%] [G loss: 4.934741]\n",
      "2807 [D loss: 0.397545, acc.: 88.89%] [G loss: 3.512626]\n",
      "2808 [D loss: 0.072344, acc.: 100.00%] [G loss: 4.642915]\n",
      "2809 [D loss: 0.091955, acc.: 100.00%] [G loss: 3.798767]\n",
      "2810 [D loss: 0.221208, acc.: 88.89%] [G loss: 2.940314]\n",
      "2811 [D loss: 0.949833, acc.: 55.56%] [G loss: 1.793205]\n",
      "2812 [D loss: 0.483109, acc.: 77.78%] [G loss: 3.702688]\n",
      "2813 [D loss: 0.498692, acc.: 83.33%] [G loss: 3.957818]\n",
      "2814 [D loss: 0.421660, acc.: 77.78%] [G loss: 3.075484]\n",
      "2815 [D loss: 0.413473, acc.: 83.33%] [G loss: 3.004299]\n",
      "2816 [D loss: 0.296495, acc.: 94.44%] [G loss: 3.121087]\n",
      "2817 [D loss: 0.217584, acc.: 94.44%] [G loss: 4.719774]\n",
      "2818 [D loss: 0.366928, acc.: 83.33%] [G loss: 3.485965]\n",
      "2819 [D loss: 0.063534, acc.: 100.00%] [G loss: 3.021408]\n",
      "2820 [D loss: 0.066072, acc.: 100.00%] [G loss: 4.353667]\n",
      "2821 [D loss: 0.746326, acc.: 72.22%] [G loss: 3.401446]\n",
      "2822 [D loss: 0.145646, acc.: 94.44%] [G loss: 3.736676]\n",
      "2823 [D loss: 0.197976, acc.: 88.89%] [G loss: 2.761656]\n",
      "2824 [D loss: 0.186459, acc.: 100.00%] [G loss: 3.359801]\n",
      "2825 [D loss: 0.674698, acc.: 72.22%] [G loss: 4.078909]\n",
      "2826 [D loss: 0.290510, acc.: 88.89%] [G loss: 3.887057]\n",
      "2827 [D loss: 0.290670, acc.: 83.33%] [G loss: 2.915938]\n",
      "2828 [D loss: 0.437356, acc.: 77.78%] [G loss: 3.092142]\n",
      "2829 [D loss: 0.461921, acc.: 83.33%] [G loss: 2.946498]\n",
      "2830 [D loss: 0.062165, acc.: 100.00%] [G loss: 6.220202]\n",
      "2831 [D loss: 0.390833, acc.: 83.33%] [G loss: 4.343786]\n",
      "2832 [D loss: 0.457085, acc.: 88.89%] [G loss: 3.314127]\n",
      "2833 [D loss: 0.363911, acc.: 94.44%] [G loss: 4.883828]\n",
      "2834 [D loss: 0.611149, acc.: 72.22%] [G loss: 3.043472]\n",
      "2835 [D loss: 0.388804, acc.: 88.89%] [G loss: 3.030624]\n",
      "2836 [D loss: 0.640594, acc.: 72.22%] [G loss: 2.383120]\n",
      "2837 [D loss: 0.448328, acc.: 77.78%] [G loss: 3.266695]\n",
      "2838 [D loss: 0.072472, acc.: 100.00%] [G loss: 4.743147]\n",
      "2839 [D loss: 0.428495, acc.: 77.78%] [G loss: 3.767761]\n",
      "2840 [D loss: 0.220197, acc.: 94.44%] [G loss: 5.634751]\n",
      "2841 [D loss: 0.134016, acc.: 94.44%] [G loss: 4.754787]\n",
      "2842 [D loss: 0.059218, acc.: 100.00%] [G loss: 5.578857]\n",
      "2843 [D loss: 0.258851, acc.: 88.89%] [G loss: 3.689704]\n",
      "2844 [D loss: 0.999142, acc.: 50.00%] [G loss: 3.167787]\n",
      "2845 [D loss: 0.410466, acc.: 94.44%] [G loss: 5.442498]\n",
      "2846 [D loss: 0.078261, acc.: 100.00%] [G loss: 6.649287]\n",
      "2847 [D loss: 0.370578, acc.: 77.78%] [G loss: 5.389104]\n",
      "2848 [D loss: 0.062627, acc.: 100.00%] [G loss: 5.690106]\n",
      "2849 [D loss: 0.032761, acc.: 100.00%] [G loss: 6.876456]\n",
      "2850 [D loss: 0.368100, acc.: 83.33%] [G loss: 3.439767]\n",
      "2851 [D loss: 0.337855, acc.: 72.22%] [G loss: 3.933310]\n",
      "2852 [D loss: 0.393708, acc.: 88.89%] [G loss: 4.558428]\n",
      "2853 [D loss: 0.033613, acc.: 100.00%] [G loss: 7.204086]\n",
      "2854 [D loss: 0.415966, acc.: 83.33%] [G loss: 4.636466]\n",
      "2855 [D loss: 0.288410, acc.: 83.33%] [G loss: 2.463853]\n",
      "2856 [D loss: 0.233466, acc.: 88.89%] [G loss: 3.733806]\n",
      "2857 [D loss: 0.457634, acc.: 72.22%] [G loss: 3.229301]\n",
      "2858 [D loss: 0.261043, acc.: 83.33%] [G loss: 3.680854]\n",
      "2859 [D loss: 0.264172, acc.: 88.89%] [G loss: 5.085332]\n",
      "2860 [D loss: 0.341733, acc.: 77.78%] [G loss: 5.051946]\n",
      "2861 [D loss: 0.388024, acc.: 77.78%] [G loss: 3.970181]\n",
      "2862 [D loss: 0.133026, acc.: 94.44%] [G loss: 4.232960]\n",
      "2863 [D loss: 0.058377, acc.: 100.00%] [G loss: 4.519199]\n",
      "2864 [D loss: 0.521067, acc.: 83.33%] [G loss: 4.127627]\n",
      "2865 [D loss: 0.081392, acc.: 100.00%] [G loss: 6.820618]\n",
      "2866 [D loss: 0.416877, acc.: 83.33%] [G loss: 3.582219]\n",
      "2867 [D loss: 0.111299, acc.: 100.00%] [G loss: 5.887030]\n",
      "2868 [D loss: 0.218038, acc.: 88.89%] [G loss: 4.364367]\n",
      "2869 [D loss: 0.062319, acc.: 100.00%] [G loss: 3.416279]\n",
      "2870 [D loss: 0.382588, acc.: 72.22%] [G loss: 3.100427]\n",
      "2871 [D loss: 0.078811, acc.: 100.00%] [G loss: 4.600578]\n",
      "2872 [D loss: 0.609942, acc.: 77.78%] [G loss: 1.594888]\n",
      "2873 [D loss: 0.269400, acc.: 83.33%] [G loss: 2.371811]\n",
      "2874 [D loss: 0.426746, acc.: 88.89%] [G loss: 4.282976]\n",
      "2875 [D loss: 0.437768, acc.: 83.33%] [G loss: 3.250368]\n",
      "2876 [D loss: 0.320261, acc.: 83.33%] [G loss: 3.808108]\n",
      "2877 [D loss: 0.619225, acc.: 72.22%] [G loss: 3.412623]\n",
      "2878 [D loss: 0.210117, acc.: 88.89%] [G loss: 4.199930]\n",
      "2879 [D loss: 0.245917, acc.: 83.33%] [G loss: 5.380342]\n",
      "2880 [D loss: 0.607173, acc.: 83.33%] [G loss: 5.205856]\n",
      "2881 [D loss: 0.602969, acc.: 77.78%] [G loss: 2.866689]\n",
      "2882 [D loss: 0.042239, acc.: 100.00%] [G loss: 3.671980]\n",
      "2883 [D loss: 0.289360, acc.: 83.33%] [G loss: 4.184495]\n",
      "2884 [D loss: 0.259413, acc.: 88.89%] [G loss: 4.361153]\n",
      "2885 [D loss: 0.554565, acc.: 88.89%] [G loss: 5.384353]\n",
      "2886 [D loss: 0.305022, acc.: 88.89%] [G loss: 4.268552]\n",
      "2887 [D loss: 0.329586, acc.: 88.89%] [G loss: 4.279431]\n",
      "2888 [D loss: 0.128116, acc.: 94.44%] [G loss: 3.675759]\n",
      "2889 [D loss: 0.341160, acc.: 83.33%] [G loss: 3.612984]\n",
      "2890 [D loss: 0.125806, acc.: 94.44%] [G loss: 4.039019]\n",
      "2891 [D loss: 0.394275, acc.: 88.89%] [G loss: 4.107740]\n",
      "2892 [D loss: 0.083644, acc.: 94.44%] [G loss: 5.759138]\n",
      "2893 [D loss: 0.396434, acc.: 83.33%] [G loss: 4.583704]\n",
      "2894 [D loss: 0.262566, acc.: 88.89%] [G loss: 4.330236]\n",
      "2895 [D loss: 0.092507, acc.: 100.00%] [G loss: 4.581029]\n",
      "2896 [D loss: 0.337367, acc.: 77.78%] [G loss: 5.278800]\n",
      "2897 [D loss: 0.536337, acc.: 72.22%] [G loss: 4.331512]\n",
      "2898 [D loss: 0.268032, acc.: 83.33%] [G loss: 4.782776]\n",
      "2899 [D loss: 0.197368, acc.: 88.89%] [G loss: 5.576990]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2900 [D loss: 0.187908, acc.: 88.89%] [G loss: 5.462934]\n",
      "2901 [D loss: 0.089208, acc.: 100.00%] [G loss: 4.518157]\n",
      "2902 [D loss: 0.140140, acc.: 94.44%] [G loss: 4.073110]\n",
      "2903 [D loss: 0.453655, acc.: 77.78%] [G loss: 4.020040]\n",
      "2904 [D loss: 0.431195, acc.: 94.44%] [G loss: 4.182270]\n",
      "2905 [D loss: 0.303185, acc.: 94.44%] [G loss: 3.885612]\n",
      "2906 [D loss: 0.056752, acc.: 100.00%] [G loss: 6.828984]\n",
      "2907 [D loss: 0.090519, acc.: 94.44%] [G loss: 5.098119]\n",
      "2908 [D loss: 0.161219, acc.: 88.89%] [G loss: 4.427610]\n",
      "2909 [D loss: 0.089450, acc.: 100.00%] [G loss: 6.106142]\n",
      "2910 [D loss: 0.067817, acc.: 94.44%] [G loss: 4.918319]\n",
      "2911 [D loss: 0.062035, acc.: 100.00%] [G loss: 5.423387]\n",
      "2912 [D loss: 0.072550, acc.: 94.44%] [G loss: 4.825541]\n",
      "2913 [D loss: 0.055145, acc.: 100.00%] [G loss: 3.743259]\n",
      "2914 [D loss: 0.025142, acc.: 100.00%] [G loss: 5.693301]\n",
      "2915 [D loss: 0.290710, acc.: 88.89%] [G loss: 3.403687]\n",
      "2916 [D loss: 0.121422, acc.: 100.00%] [G loss: 4.536326]\n",
      "2917 [D loss: 0.148720, acc.: 88.89%] [G loss: 4.793461]\n",
      "2918 [D loss: 0.212288, acc.: 83.33%] [G loss: 3.512717]\n",
      "2919 [D loss: 0.288666, acc.: 88.89%] [G loss: 4.368041]\n",
      "2920 [D loss: 0.232885, acc.: 88.89%] [G loss: 5.106325]\n",
      "2921 [D loss: 0.202370, acc.: 94.44%] [G loss: 2.957517]\n",
      "2922 [D loss: 0.686564, acc.: 61.11%] [G loss: 3.001934]\n",
      "2923 [D loss: 0.208199, acc.: 94.44%] [G loss: 3.707091]\n",
      "2924 [D loss: 0.466580, acc.: 83.33%] [G loss: 3.197539]\n",
      "2925 [D loss: 0.355251, acc.: 83.33%] [G loss: 2.637443]\n",
      "2926 [D loss: 0.894813, acc.: 61.11%] [G loss: 4.210721]\n",
      "2927 [D loss: 0.075901, acc.: 100.00%] [G loss: 6.304518]\n",
      "2928 [D loss: 0.235313, acc.: 94.44%] [G loss: 3.998203]\n",
      "2929 [D loss: 0.372459, acc.: 83.33%] [G loss: 3.227908]\n",
      "2930 [D loss: 0.294346, acc.: 83.33%] [G loss: 3.019923]\n",
      "2931 [D loss: 0.256846, acc.: 94.44%] [G loss: 2.968373]\n",
      "2932 [D loss: 0.098450, acc.: 100.00%] [G loss: 4.483503]\n",
      "2933 [D loss: 0.496387, acc.: 61.11%] [G loss: 3.742448]\n",
      "2934 [D loss: 0.864712, acc.: 50.00%] [G loss: 5.415426]\n",
      "2935 [D loss: 0.333668, acc.: 83.33%] [G loss: 4.336135]\n",
      "2936 [D loss: 0.215156, acc.: 100.00%] [G loss: 3.438693]\n",
      "2937 [D loss: 0.267970, acc.: 88.89%] [G loss: 4.090666]\n",
      "2938 [D loss: 0.308409, acc.: 83.33%] [G loss: 5.327508]\n",
      "2939 [D loss: 0.098385, acc.: 94.44%] [G loss: 4.863148]\n",
      "2940 [D loss: 0.175934, acc.: 94.44%] [G loss: 4.059967]\n",
      "2941 [D loss: 0.183913, acc.: 100.00%] [G loss: 3.219732]\n",
      "2942 [D loss: 0.775651, acc.: 66.67%] [G loss: 4.480291]\n",
      "2943 [D loss: 0.395102, acc.: 88.89%] [G loss: 5.531327]\n",
      "2944 [D loss: 1.041775, acc.: 61.11%] [G loss: 3.667742]\n",
      "2945 [D loss: 0.670939, acc.: 83.33%] [G loss: 2.641209]\n",
      "2946 [D loss: 0.066476, acc.: 100.00%] [G loss: 3.613733]\n",
      "2947 [D loss: 0.832522, acc.: 66.67%] [G loss: 2.882381]\n",
      "2948 [D loss: 0.322717, acc.: 83.33%] [G loss: 5.420814]\n",
      "2949 [D loss: 0.767678, acc.: 72.22%] [G loss: 4.357584]\n",
      "2950 [D loss: 0.272087, acc.: 88.89%] [G loss: 3.658091]\n",
      "2951 [D loss: 0.029157, acc.: 100.00%] [G loss: 6.059320]\n",
      "2952 [D loss: 0.043590, acc.: 100.00%] [G loss: 5.102468]\n",
      "2953 [D loss: 0.084394, acc.: 100.00%] [G loss: 4.073482]\n",
      "2954 [D loss: 0.335285, acc.: 88.89%] [G loss: 2.851914]\n",
      "2955 [D loss: 0.292130, acc.: 88.89%] [G loss: 4.419349]\n",
      "2956 [D loss: 0.180184, acc.: 94.44%] [G loss: 4.663773]\n",
      "2957 [D loss: 0.127137, acc.: 100.00%] [G loss: 5.259219]\n",
      "2958 [D loss: 0.298664, acc.: 88.89%] [G loss: 4.343628]\n",
      "2959 [D loss: 0.449965, acc.: 83.33%] [G loss: 2.809031]\n",
      "2960 [D loss: 0.892478, acc.: 55.56%] [G loss: 2.557054]\n",
      "2961 [D loss: 0.264430, acc.: 94.44%] [G loss: 3.431858]\n",
      "2962 [D loss: 0.636585, acc.: 88.89%] [G loss: 3.159333]\n",
      "2963 [D loss: 0.154356, acc.: 94.44%] [G loss: 3.399829]\n",
      "2964 [D loss: 0.101691, acc.: 100.00%] [G loss: 4.242805]\n",
      "2965 [D loss: 0.194902, acc.: 94.44%] [G loss: 4.650982]\n",
      "2966 [D loss: 0.520289, acc.: 77.78%] [G loss: 4.743587]\n",
      "2967 [D loss: 0.194558, acc.: 94.44%] [G loss: 4.962442]\n",
      "2968 [D loss: 1.050222, acc.: 44.44%] [G loss: 2.804777]\n",
      "2969 [D loss: 0.237059, acc.: 83.33%] [G loss: 5.443479]\n",
      "2970 [D loss: 0.046349, acc.: 100.00%] [G loss: 4.439968]\n",
      "2971 [D loss: 0.089190, acc.: 94.44%] [G loss: 5.290037]\n",
      "2972 [D loss: 0.064720, acc.: 100.00%] [G loss: 4.475542]\n",
      "2973 [D loss: 0.266134, acc.: 94.44%] [G loss: 4.190191]\n",
      "2974 [D loss: 0.194013, acc.: 94.44%] [G loss: 3.823808]\n",
      "2975 [D loss: 0.148126, acc.: 88.89%] [G loss: 4.574838]\n",
      "2976 [D loss: 0.094481, acc.: 100.00%] [G loss: 6.232023]\n",
      "2977 [D loss: 0.147244, acc.: 94.44%] [G loss: 4.742687]\n",
      "2978 [D loss: 0.133101, acc.: 94.44%] [G loss: 4.293345]\n",
      "2979 [D loss: 0.282081, acc.: 83.33%] [G loss: 4.456743]\n",
      "2980 [D loss: 0.323742, acc.: 88.89%] [G loss: 5.016431]\n",
      "2981 [D loss: 0.134404, acc.: 94.44%] [G loss: 3.705224]\n",
      "2982 [D loss: 0.259856, acc.: 83.33%] [G loss: 3.739624]\n",
      "2983 [D loss: 0.143712, acc.: 94.44%] [G loss: 4.276439]\n",
      "2984 [D loss: 0.140458, acc.: 94.44%] [G loss: 5.825624]\n",
      "2985 [D loss: 0.760201, acc.: 66.67%] [G loss: 3.567894]\n",
      "2986 [D loss: 0.322264, acc.: 88.89%] [G loss: 4.009243]\n",
      "2987 [D loss: 0.170396, acc.: 94.44%] [G loss: 4.362716]\n",
      "2988 [D loss: 0.133665, acc.: 88.89%] [G loss: 2.725613]\n",
      "2989 [D loss: 0.086451, acc.: 100.00%] [G loss: 4.723668]\n",
      "2990 [D loss: 0.076307, acc.: 100.00%] [G loss: 3.742841]\n",
      "2991 [D loss: 0.544433, acc.: 77.78%] [G loss: 4.538767]\n",
      "2992 [D loss: 0.157276, acc.: 94.44%] [G loss: 4.085050]\n",
      "2993 [D loss: 0.410542, acc.: 83.33%] [G loss: 2.885969]\n",
      "2994 [D loss: 0.172526, acc.: 94.44%] [G loss: 3.666999]\n",
      "2995 [D loss: 0.036439, acc.: 100.00%] [G loss: 3.600020]\n",
      "2996 [D loss: 0.128334, acc.: 94.44%] [G loss: 4.253076]\n",
      "2997 [D loss: 0.147658, acc.: 88.89%] [G loss: 4.157250]\n",
      "2998 [D loss: 0.276098, acc.: 94.44%] [G loss: 4.550631]\n",
      "2999 [D loss: 0.590401, acc.: 61.11%] [G loss: 2.870773]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 [D loss: 0.348737, acc.: 88.89%] [G loss: 5.010538]\n",
      "3001 [D loss: 0.714590, acc.: 66.67%] [G loss: 3.212643]\n",
      "3002 [D loss: 0.190637, acc.: 94.44%] [G loss: 4.272011]\n",
      "3003 [D loss: 0.098750, acc.: 100.00%] [G loss: 5.092846]\n",
      "3004 [D loss: 0.405718, acc.: 77.78%] [G loss: 3.924322]\n",
      "3005 [D loss: 0.101775, acc.: 100.00%] [G loss: 5.400501]\n",
      "3006 [D loss: 0.654770, acc.: 77.78%] [G loss: 3.664393]\n",
      "3007 [D loss: 0.203850, acc.: 94.44%] [G loss: 2.660458]\n",
      "3008 [D loss: 0.535972, acc.: 77.78%] [G loss: 4.919063]\n",
      "3009 [D loss: 0.086666, acc.: 94.44%] [G loss: 7.049635]\n",
      "3010 [D loss: 0.448900, acc.: 66.67%] [G loss: 2.111203]\n",
      "3011 [D loss: 0.341402, acc.: 83.33%] [G loss: 2.621184]\n",
      "3012 [D loss: 0.163431, acc.: 94.44%] [G loss: 2.731476]\n",
      "3013 [D loss: 0.188458, acc.: 94.44%] [G loss: 3.863542]\n",
      "3014 [D loss: 0.285213, acc.: 88.89%] [G loss: 4.527544]\n",
      "3015 [D loss: 0.806608, acc.: 55.56%] [G loss: 2.058298]\n",
      "3016 [D loss: 0.473254, acc.: 77.78%] [G loss: 3.397225]\n",
      "3017 [D loss: 0.118605, acc.: 94.44%] [G loss: 3.904119]\n",
      "3018 [D loss: 0.340509, acc.: 88.89%] [G loss: 3.389632]\n",
      "3019 [D loss: 0.057615, acc.: 100.00%] [G loss: 5.092068]\n",
      "3020 [D loss: 0.504409, acc.: 55.56%] [G loss: 3.631723]\n",
      "3021 [D loss: 0.048409, acc.: 100.00%] [G loss: 2.924031]\n",
      "3022 [D loss: 0.061704, acc.: 100.00%] [G loss: 4.741838]\n",
      "3023 [D loss: 0.269357, acc.: 88.89%] [G loss: 3.722527]\n",
      "3024 [D loss: 0.157826, acc.: 94.44%] [G loss: 3.295533]\n",
      "3025 [D loss: 0.414082, acc.: 77.78%] [G loss: 4.036915]\n",
      "3026 [D loss: 0.057803, acc.: 100.00%] [G loss: 3.987566]\n",
      "3027 [D loss: 0.559489, acc.: 66.67%] [G loss: 2.171407]\n",
      "3028 [D loss: 0.275740, acc.: 88.89%] [G loss: 5.685895]\n",
      "3029 [D loss: 0.692302, acc.: 66.67%] [G loss: 2.066077]\n",
      "3030 [D loss: 0.105638, acc.: 100.00%] [G loss: 3.684551]\n",
      "3031 [D loss: 0.174106, acc.: 94.44%] [G loss: 3.655709]\n",
      "3032 [D loss: 0.348699, acc.: 88.89%] [G loss: 3.952101]\n",
      "3033 [D loss: 0.097354, acc.: 100.00%] [G loss: 4.171523]\n",
      "3034 [D loss: 0.775132, acc.: 61.11%] [G loss: 3.231211]\n",
      "3035 [D loss: 0.427296, acc.: 72.22%] [G loss: 4.958225]\n",
      "3036 [D loss: 0.170627, acc.: 88.89%] [G loss: 3.367223]\n",
      "3037 [D loss: 0.557718, acc.: 77.78%] [G loss: 5.185337]\n",
      "3038 [D loss: 0.158582, acc.: 94.44%] [G loss: 4.450504]\n",
      "3039 [D loss: 0.143571, acc.: 100.00%] [G loss: 4.547455]\n",
      "3040 [D loss: 0.295613, acc.: 83.33%] [G loss: 4.076304]\n",
      "3041 [D loss: 0.141401, acc.: 100.00%] [G loss: 2.803921]\n",
      "3042 [D loss: 0.305854, acc.: 83.33%] [G loss: 3.498695]\n",
      "3043 [D loss: 0.454626, acc.: 77.78%] [G loss: 3.211871]\n",
      "3044 [D loss: 0.340924, acc.: 83.33%] [G loss: 3.483850]\n",
      "3045 [D loss: 0.326881, acc.: 83.33%] [G loss: 4.302193]\n",
      "3046 [D loss: 0.318874, acc.: 88.89%] [G loss: 3.216956]\n",
      "3047 [D loss: 0.194690, acc.: 83.33%] [G loss: 3.119341]\n",
      "3048 [D loss: 0.422554, acc.: 83.33%] [G loss: 3.890053]\n",
      "3049 [D loss: 0.213064, acc.: 88.89%] [G loss: 5.200783]\n",
      "3050 [D loss: 0.617735, acc.: 72.22%] [G loss: 3.219625]\n",
      "3051 [D loss: 0.093492, acc.: 100.00%] [G loss: 4.826380]\n",
      "3052 [D loss: 0.149053, acc.: 94.44%] [G loss: 2.951405]\n",
      "3053 [D loss: 0.364809, acc.: 88.89%] [G loss: 3.968244]\n",
      "3054 [D loss: 0.097791, acc.: 100.00%] [G loss: 5.963669]\n",
      "3055 [D loss: 0.058246, acc.: 100.00%] [G loss: 5.343024]\n",
      "3056 [D loss: 0.581408, acc.: 66.67%] [G loss: 2.778163]\n",
      "3057 [D loss: 0.194209, acc.: 88.89%] [G loss: 3.709743]\n",
      "3058 [D loss: 0.087820, acc.: 100.00%] [G loss: 3.339343]\n",
      "3059 [D loss: 0.086524, acc.: 100.00%] [G loss: 4.369040]\n",
      "3060 [D loss: 0.208803, acc.: 94.44%] [G loss: 5.079507]\n",
      "3061 [D loss: 1.429724, acc.: 66.67%] [G loss: 2.100585]\n",
      "3062 [D loss: 0.241186, acc.: 94.44%] [G loss: 3.378827]\n",
      "3063 [D loss: 0.177588, acc.: 83.33%] [G loss: 3.949453]\n",
      "3064 [D loss: 0.261040, acc.: 83.33%] [G loss: 4.416714]\n",
      "3065 [D loss: 0.326696, acc.: 83.33%] [G loss: 3.493474]\n",
      "3066 [D loss: 0.177932, acc.: 94.44%] [G loss: 4.008036]\n",
      "3067 [D loss: 0.267248, acc.: 88.89%] [G loss: 4.281002]\n",
      "3068 [D loss: 0.191814, acc.: 94.44%] [G loss: 2.779855]\n",
      "3069 [D loss: 0.236772, acc.: 94.44%] [G loss: 2.469556]\n",
      "3070 [D loss: 0.192519, acc.: 94.44%] [G loss: 5.142073]\n",
      "3071 [D loss: 0.280423, acc.: 83.33%] [G loss: 2.257535]\n",
      "3072 [D loss: 0.133899, acc.: 94.44%] [G loss: 3.812426]\n",
      "3073 [D loss: 0.085391, acc.: 100.00%] [G loss: 3.121263]\n",
      "3074 [D loss: 0.504235, acc.: 83.33%] [G loss: 3.328230]\n",
      "3075 [D loss: 0.354375, acc.: 83.33%] [G loss: 3.189682]\n",
      "3076 [D loss: 0.052622, acc.: 100.00%] [G loss: 2.934473]\n",
      "3077 [D loss: 0.085256, acc.: 100.00%] [G loss: 3.663899]\n",
      "3078 [D loss: 0.092114, acc.: 94.44%] [G loss: 4.320365]\n",
      "3079 [D loss: 0.384717, acc.: 83.33%] [G loss: 3.034894]\n",
      "3080 [D loss: 0.128273, acc.: 94.44%] [G loss: 6.141915]\n",
      "3081 [D loss: 0.330274, acc.: 88.89%] [G loss: 2.818175]\n",
      "3082 [D loss: 0.248933, acc.: 88.89%] [G loss: 3.296478]\n",
      "3083 [D loss: 0.528328, acc.: 72.22%] [G loss: 3.374975]\n",
      "3084 [D loss: 0.278085, acc.: 88.89%] [G loss: 3.554856]\n",
      "3085 [D loss: 0.508546, acc.: 77.78%] [G loss: 4.037876]\n",
      "3086 [D loss: 0.229422, acc.: 88.89%] [G loss: 4.832379]\n",
      "3087 [D loss: 0.069739, acc.: 100.00%] [G loss: 6.048213]\n",
      "3088 [D loss: 0.709249, acc.: 50.00%] [G loss: 2.586971]\n",
      "3089 [D loss: 0.948819, acc.: 66.67%] [G loss: 4.350597]\n",
      "3090 [D loss: 0.747836, acc.: 72.22%] [G loss: 3.528725]\n",
      "3091 [D loss: 0.379237, acc.: 83.33%] [G loss: 1.930798]\n",
      "3092 [D loss: 0.041319, acc.: 100.00%] [G loss: 6.477599]\n",
      "3093 [D loss: 0.559825, acc.: 83.33%] [G loss: 3.573508]\n",
      "3094 [D loss: 1.416437, acc.: 50.00%] [G loss: 4.186658]\n",
      "3095 [D loss: 0.252210, acc.: 88.89%] [G loss: 3.005311]\n",
      "3096 [D loss: 0.337747, acc.: 88.89%] [G loss: 3.849942]\n",
      "3097 [D loss: 0.396820, acc.: 88.89%] [G loss: 3.608770]\n",
      "3098 [D loss: 0.242200, acc.: 88.89%] [G loss: 3.289550]\n",
      "3099 [D loss: 0.326347, acc.: 83.33%] [G loss: 4.493192]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3100 [D loss: 0.941267, acc.: 77.78%] [G loss: 3.093904]\n",
      "3101 [D loss: 0.685185, acc.: 72.22%] [G loss: 4.223443]\n",
      "3102 [D loss: 0.294922, acc.: 83.33%] [G loss: 5.634171]\n",
      "3103 [D loss: 0.255462, acc.: 94.44%] [G loss: 4.072022]\n",
      "3104 [D loss: 0.442496, acc.: 72.22%] [G loss: 2.743587]\n",
      "3105 [D loss: 0.484289, acc.: 83.33%] [G loss: 3.303596]\n",
      "3106 [D loss: 0.188853, acc.: 94.44%] [G loss: 4.486198]\n",
      "3107 [D loss: 0.846452, acc.: 66.67%] [G loss: 3.000647]\n",
      "3108 [D loss: 0.116653, acc.: 94.44%] [G loss: 4.755589]\n",
      "3109 [D loss: 0.361966, acc.: 94.44%] [G loss: 3.317606]\n",
      "3110 [D loss: 0.229052, acc.: 88.89%] [G loss: 4.539209]\n",
      "3111 [D loss: 0.351689, acc.: 77.78%] [G loss: 3.246982]\n",
      "3112 [D loss: 0.187830, acc.: 94.44%] [G loss: 4.528011]\n",
      "3113 [D loss: 0.022106, acc.: 100.00%] [G loss: 4.936742]\n",
      "3114 [D loss: 0.116288, acc.: 100.00%] [G loss: 3.407055]\n",
      "3115 [D loss: 0.248323, acc.: 94.44%] [G loss: 5.441690]\n",
      "3116 [D loss: 0.335974, acc.: 83.33%] [G loss: 3.254968]\n",
      "3117 [D loss: 0.124270, acc.: 94.44%] [G loss: 4.328708]\n",
      "3118 [D loss: 0.523946, acc.: 72.22%] [G loss: 2.230224]\n",
      "3119 [D loss: 0.619633, acc.: 66.67%] [G loss: 2.559233]\n",
      "3120 [D loss: 0.430695, acc.: 83.33%] [G loss: 4.366764]\n",
      "3121 [D loss: 0.303748, acc.: 88.89%] [G loss: 4.456003]\n",
      "3122 [D loss: 0.491031, acc.: 88.89%] [G loss: 3.916302]\n",
      "3123 [D loss: 0.211332, acc.: 94.44%] [G loss: 5.467387]\n",
      "3124 [D loss: 0.283219, acc.: 83.33%] [G loss: 2.974561]\n",
      "3125 [D loss: 0.523239, acc.: 77.78%] [G loss: 4.004481]\n",
      "3126 [D loss: 0.186247, acc.: 94.44%] [G loss: 4.138083]\n",
      "3127 [D loss: 0.584546, acc.: 77.78%] [G loss: 2.870085]\n",
      "3128 [D loss: 0.587813, acc.: 66.67%] [G loss: 3.023613]\n",
      "3129 [D loss: 0.068987, acc.: 100.00%] [G loss: 4.222994]\n",
      "3130 [D loss: 0.698622, acc.: 72.22%] [G loss: 2.353216]\n",
      "3131 [D loss: 0.431057, acc.: 88.89%] [G loss: 4.599864]\n",
      "3132 [D loss: 0.482005, acc.: 72.22%] [G loss: 4.635486]\n",
      "3133 [D loss: 0.818562, acc.: 66.67%] [G loss: 3.389907]\n",
      "3134 [D loss: 0.083645, acc.: 100.00%] [G loss: 4.924780]\n",
      "3135 [D loss: 0.087811, acc.: 100.00%] [G loss: 3.999363]\n",
      "3136 [D loss: 0.366468, acc.: 88.89%] [G loss: 4.760955]\n",
      "3137 [D loss: 0.385513, acc.: 77.78%] [G loss: 2.432326]\n",
      "3138 [D loss: 0.135394, acc.: 94.44%] [G loss: 4.668039]\n",
      "3139 [D loss: 0.394133, acc.: 88.89%] [G loss: 2.753720]\n",
      "3140 [D loss: 0.444399, acc.: 88.89%] [G loss: 2.787268]\n",
      "3141 [D loss: 0.253466, acc.: 88.89%] [G loss: 5.337076]\n",
      "3142 [D loss: 0.352142, acc.: 83.33%] [G loss: 3.805211]\n",
      "3143 [D loss: 0.400720, acc.: 77.78%] [G loss: 2.506084]\n",
      "3144 [D loss: 0.465361, acc.: 88.89%] [G loss: 3.509808]\n",
      "3145 [D loss: 0.477313, acc.: 83.33%] [G loss: 3.779969]\n",
      "3146 [D loss: 0.445198, acc.: 94.44%] [G loss: 3.369681]\n",
      "3147 [D loss: 0.180072, acc.: 88.89%] [G loss: 3.157560]\n",
      "3148 [D loss: 0.064901, acc.: 100.00%] [G loss: 4.689972]\n",
      "3149 [D loss: 0.325508, acc.: 77.78%] [G loss: 2.510495]\n",
      "3150 [D loss: 0.058257, acc.: 100.00%] [G loss: 4.443516]\n",
      "3151 [D loss: 0.510472, acc.: 66.67%] [G loss: 4.413816]\n",
      "3152 [D loss: 1.388971, acc.: 50.00%] [G loss: 2.268836]\n",
      "3153 [D loss: 0.220027, acc.: 88.89%] [G loss: 5.538334]\n",
      "3154 [D loss: 0.300645, acc.: 88.89%] [G loss: 4.526974]\n",
      "3155 [D loss: 0.489565, acc.: 77.78%] [G loss: 3.015756]\n",
      "3156 [D loss: 0.339574, acc.: 94.44%] [G loss: 3.818097]\n",
      "3157 [D loss: 0.138987, acc.: 100.00%] [G loss: 5.607450]\n",
      "3158 [D loss: 0.763143, acc.: 66.67%] [G loss: 2.581104]\n",
      "3159 [D loss: 0.685107, acc.: 66.67%] [G loss: 2.019905]\n",
      "3160 [D loss: 0.446694, acc.: 77.78%] [G loss: 2.745967]\n",
      "3161 [D loss: 0.119496, acc.: 94.44%] [G loss: 4.387891]\n",
      "3162 [D loss: 0.153700, acc.: 94.44%] [G loss: 4.053621]\n",
      "3163 [D loss: 0.275353, acc.: 88.89%] [G loss: 3.569278]\n",
      "3164 [D loss: 0.086019, acc.: 94.44%] [G loss: 5.888053]\n",
      "3165 [D loss: 0.017815, acc.: 100.00%] [G loss: 5.390109]\n",
      "3166 [D loss: 0.168414, acc.: 94.44%] [G loss: 3.078764]\n",
      "3167 [D loss: 0.222856, acc.: 94.44%] [G loss: 4.075680]\n",
      "3168 [D loss: 0.237656, acc.: 94.44%] [G loss: 4.352823]\n",
      "3169 [D loss: 0.419493, acc.: 77.78%] [G loss: 4.083959]\n",
      "3170 [D loss: 0.168417, acc.: 94.44%] [G loss: 2.723056]\n",
      "3171 [D loss: 0.493491, acc.: 72.22%] [G loss: 4.779417]\n",
      "3172 [D loss: 0.333582, acc.: 83.33%] [G loss: 3.130011]\n",
      "3173 [D loss: 0.084290, acc.: 94.44%] [G loss: 5.343466]\n",
      "3174 [D loss: 0.812401, acc.: 55.56%] [G loss: 2.418720]\n",
      "3175 [D loss: 0.195169, acc.: 94.44%] [G loss: 2.917916]\n",
      "3176 [D loss: 0.654809, acc.: 77.78%] [G loss: 5.056870]\n",
      "3177 [D loss: 0.274155, acc.: 88.89%] [G loss: 5.246912]\n",
      "3178 [D loss: 0.660070, acc.: 77.78%] [G loss: 2.687703]\n",
      "3179 [D loss: 0.170258, acc.: 94.44%] [G loss: 3.615501]\n",
      "3180 [D loss: 0.220586, acc.: 83.33%] [G loss: 4.442737]\n",
      "3181 [D loss: 0.371371, acc.: 94.44%] [G loss: 4.761297]\n",
      "3182 [D loss: 0.069780, acc.: 94.44%] [G loss: 5.103772]\n",
      "3183 [D loss: 0.715490, acc.: 77.78%] [G loss: 3.240333]\n",
      "3184 [D loss: 0.162254, acc.: 94.44%] [G loss: 3.502056]\n",
      "3185 [D loss: 0.587512, acc.: 77.78%] [G loss: 4.697967]\n",
      "3186 [D loss: 0.319465, acc.: 83.33%] [G loss: 3.229393]\n",
      "3187 [D loss: 0.141278, acc.: 94.44%] [G loss: 5.855268]\n",
      "3188 [D loss: 0.358757, acc.: 83.33%] [G loss: 3.014171]\n",
      "3189 [D loss: 0.598851, acc.: 83.33%] [G loss: 2.428638]\n",
      "3190 [D loss: 0.321813, acc.: 88.89%] [G loss: 3.000210]\n",
      "3191 [D loss: 0.160051, acc.: 94.44%] [G loss: 3.938947]\n",
      "3192 [D loss: 0.435276, acc.: 88.89%] [G loss: 3.191750]\n",
      "3193 [D loss: 0.454665, acc.: 83.33%] [G loss: 3.901794]\n",
      "3194 [D loss: 0.246638, acc.: 94.44%] [G loss: 3.437471]\n",
      "3195 [D loss: 0.112830, acc.: 100.00%] [G loss: 5.030946]\n",
      "3196 [D loss: 0.469840, acc.: 83.33%] [G loss: 2.201194]\n",
      "3197 [D loss: 0.109086, acc.: 100.00%] [G loss: 5.234118]\n",
      "3198 [D loss: 0.153360, acc.: 94.44%] [G loss: 4.636320]\n",
      "3199 [D loss: 0.354607, acc.: 83.33%] [G loss: 3.048581]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200 [D loss: 0.445207, acc.: 77.78%] [G loss: 4.311087]\n",
      "3201 [D loss: 0.043285, acc.: 100.00%] [G loss: 5.071787]\n",
      "3202 [D loss: 0.452974, acc.: 83.33%] [G loss: 4.923337]\n",
      "3203 [D loss: 0.297879, acc.: 83.33%] [G loss: 4.343475]\n",
      "3204 [D loss: 0.121027, acc.: 100.00%] [G loss: 3.330411]\n",
      "3205 [D loss: 1.004488, acc.: 50.00%] [G loss: 3.664396]\n",
      "3206 [D loss: 0.496264, acc.: 72.22%] [G loss: 2.906135]\n",
      "3207 [D loss: 0.346175, acc.: 77.78%] [G loss: 4.628383]\n",
      "3208 [D loss: 0.093820, acc.: 100.00%] [G loss: 3.892531]\n",
      "3209 [D loss: 0.143572, acc.: 94.44%] [G loss: 5.374906]\n",
      "3210 [D loss: 0.169776, acc.: 94.44%] [G loss: 4.036983]\n",
      "3211 [D loss: 0.201817, acc.: 88.89%] [G loss: 3.449019]\n",
      "3212 [D loss: 0.241428, acc.: 88.89%] [G loss: 4.868309]\n",
      "3213 [D loss: 0.436103, acc.: 83.33%] [G loss: 4.496529]\n",
      "3214 [D loss: 0.434260, acc.: 66.67%] [G loss: 3.128460]\n",
      "3215 [D loss: 0.373222, acc.: 88.89%] [G loss: 4.565698]\n",
      "3216 [D loss: 0.095497, acc.: 100.00%] [G loss: 3.503411]\n",
      "3217 [D loss: 0.167328, acc.: 88.89%] [G loss: 3.272014]\n",
      "3218 [D loss: 0.527445, acc.: 72.22%] [G loss: 3.562609]\n",
      "3219 [D loss: 0.141540, acc.: 100.00%] [G loss: 4.471382]\n",
      "3220 [D loss: 0.541167, acc.: 77.78%] [G loss: 3.017165]\n",
      "3221 [D loss: 0.312179, acc.: 94.44%] [G loss: 5.115678]\n",
      "3222 [D loss: 0.388346, acc.: 88.89%] [G loss: 5.029345]\n",
      "3223 [D loss: 0.107503, acc.: 100.00%] [G loss: 4.093093]\n",
      "3224 [D loss: 0.282539, acc.: 83.33%] [G loss: 3.065787]\n",
      "3225 [D loss: 0.250549, acc.: 83.33%] [G loss: 3.826210]\n",
      "3226 [D loss: 0.314035, acc.: 77.78%] [G loss: 4.290462]\n",
      "3227 [D loss: 0.199778, acc.: 94.44%] [G loss: 5.439398]\n",
      "3228 [D loss: 0.361916, acc.: 83.33%] [G loss: 3.558677]\n",
      "3229 [D loss: 0.507865, acc.: 77.78%] [G loss: 2.930791]\n",
      "3230 [D loss: 0.209989, acc.: 88.89%] [G loss: 2.662034]\n",
      "3231 [D loss: 0.148780, acc.: 94.44%] [G loss: 4.217140]\n",
      "3232 [D loss: 0.507039, acc.: 83.33%] [G loss: 4.590875]\n",
      "3233 [D loss: 0.162426, acc.: 100.00%] [G loss: 5.091016]\n",
      "3234 [D loss: 0.519752, acc.: 77.78%] [G loss: 3.197222]\n",
      "3235 [D loss: 0.237398, acc.: 83.33%] [G loss: 4.611265]\n",
      "3236 [D loss: 0.243432, acc.: 77.78%] [G loss: 4.518677]\n",
      "3237 [D loss: 0.284256, acc.: 88.89%] [G loss: 5.173602]\n",
      "3238 [D loss: 0.062266, acc.: 94.44%] [G loss: 5.191650]\n",
      "3239 [D loss: 0.128414, acc.: 100.00%] [G loss: 5.121966]\n",
      "3240 [D loss: 0.552907, acc.: 72.22%] [G loss: 3.537064]\n",
      "3241 [D loss: 0.156222, acc.: 94.44%] [G loss: 4.649757]\n",
      "3242 [D loss: 0.175016, acc.: 94.44%] [G loss: 4.847710]\n",
      "3243 [D loss: 0.048340, acc.: 100.00%] [G loss: 3.766755]\n",
      "3244 [D loss: 0.057579, acc.: 100.00%] [G loss: 5.592681]\n",
      "3245 [D loss: 0.852386, acc.: 72.22%] [G loss: 2.829581]\n",
      "3246 [D loss: 0.228699, acc.: 94.44%] [G loss: 5.747731]\n",
      "3247 [D loss: 0.354251, acc.: 83.33%] [G loss: 4.476211]\n",
      "3248 [D loss: 0.214288, acc.: 94.44%] [G loss: 5.071698]\n",
      "3249 [D loss: 0.042605, acc.: 100.00%] [G loss: 5.064438]\n",
      "3250 [D loss: 0.565523, acc.: 77.78%] [G loss: 3.922247]\n",
      "3251 [D loss: 0.062401, acc.: 100.00%] [G loss: 4.135590]\n",
      "3252 [D loss: 0.298280, acc.: 88.89%] [G loss: 2.980576]\n",
      "3253 [D loss: 0.199310, acc.: 94.44%] [G loss: 3.526041]\n",
      "3254 [D loss: 0.060391, acc.: 100.00%] [G loss: 3.377347]\n",
      "3255 [D loss: 0.370426, acc.: 88.89%] [G loss: 2.468720]\n",
      "3256 [D loss: 0.533318, acc.: 72.22%] [G loss: 2.559061]\n",
      "3257 [D loss: 0.360748, acc.: 83.33%] [G loss: 5.241039]\n",
      "3258 [D loss: 0.381222, acc.: 83.33%] [G loss: 4.052504]\n",
      "3259 [D loss: 0.808556, acc.: 77.78%] [G loss: 4.188560]\n",
      "3260 [D loss: 0.073790, acc.: 94.44%] [G loss: 4.840216]\n",
      "3261 [D loss: 0.021287, acc.: 100.00%] [G loss: 3.535277]\n",
      "3262 [D loss: 0.313065, acc.: 83.33%] [G loss: 3.529752]\n",
      "3263 [D loss: 0.131934, acc.: 88.89%] [G loss: 5.010633]\n",
      "3264 [D loss: 0.094436, acc.: 94.44%] [G loss: 5.840440]\n",
      "3265 [D loss: 0.553043, acc.: 77.78%] [G loss: 2.905179]\n",
      "3266 [D loss: 0.275084, acc.: 88.89%] [G loss: 4.052156]\n",
      "3267 [D loss: 0.173951, acc.: 88.89%] [G loss: 5.419781]\n",
      "3268 [D loss: 0.542722, acc.: 72.22%] [G loss: 2.758877]\n",
      "3269 [D loss: 0.436104, acc.: 77.78%] [G loss: 3.075004]\n",
      "3270 [D loss: 0.422967, acc.: 88.89%] [G loss: 4.091537]\n",
      "3271 [D loss: 0.456254, acc.: 83.33%] [G loss: 5.391526]\n",
      "3272 [D loss: 0.140758, acc.: 88.89%] [G loss: 4.262113]\n",
      "3273 [D loss: 0.311630, acc.: 77.78%] [G loss: 4.952330]\n",
      "3274 [D loss: 0.472630, acc.: 72.22%] [G loss: 2.957252]\n",
      "3275 [D loss: 0.235610, acc.: 94.44%] [G loss: 4.171576]\n",
      "3276 [D loss: 0.447681, acc.: 77.78%] [G loss: 4.073678]\n",
      "3277 [D loss: 0.232422, acc.: 94.44%] [G loss: 4.260217]\n",
      "3278 [D loss: 0.067648, acc.: 100.00%] [G loss: 5.294429]\n",
      "3279 [D loss: 0.654456, acc.: 72.22%] [G loss: 3.102231]\n",
      "3280 [D loss: 0.398333, acc.: 77.78%] [G loss: 4.024887]\n",
      "3281 [D loss: 0.020134, acc.: 100.00%] [G loss: 4.450149]\n",
      "3282 [D loss: 0.341549, acc.: 88.89%] [G loss: 4.400946]\n",
      "3283 [D loss: 0.222763, acc.: 94.44%] [G loss: 3.005509]\n",
      "3284 [D loss: 0.164364, acc.: 88.89%] [G loss: 3.961522]\n",
      "3285 [D loss: 0.634470, acc.: 72.22%] [G loss: 4.122695]\n",
      "3286 [D loss: 0.366531, acc.: 77.78%] [G loss: 4.556451]\n",
      "3287 [D loss: 0.565739, acc.: 72.22%] [G loss: 5.597984]\n",
      "3288 [D loss: 0.696214, acc.: 77.78%] [G loss: 5.286521]\n",
      "3289 [D loss: 0.584729, acc.: 83.33%] [G loss: 3.581357]\n",
      "3290 [D loss: 0.540642, acc.: 83.33%] [G loss: 4.207425]\n",
      "3291 [D loss: 0.049639, acc.: 100.00%] [G loss: 3.902305]\n",
      "3292 [D loss: 0.232663, acc.: 88.89%] [G loss: 4.011715]\n",
      "3293 [D loss: 0.204053, acc.: 94.44%] [G loss: 3.245573]\n",
      "3294 [D loss: 0.646141, acc.: 77.78%] [G loss: 2.508527]\n",
      "3295 [D loss: 0.339378, acc.: 83.33%] [G loss: 3.304103]\n",
      "3296 [D loss: 0.695627, acc.: 77.78%] [G loss: 3.054002]\n",
      "3297 [D loss: 0.185365, acc.: 94.44%] [G loss: 3.318762]\n",
      "3298 [D loss: 0.404405, acc.: 88.89%] [G loss: 4.528524]\n",
      "3299 [D loss: 0.674177, acc.: 66.67%] [G loss: 3.082319]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3300 [D loss: 0.433776, acc.: 88.89%] [G loss: 4.078556]\n",
      "3301 [D loss: 0.708692, acc.: 72.22%] [G loss: 3.383038]\n",
      "3302 [D loss: 0.046814, acc.: 100.00%] [G loss: 3.281778]\n",
      "3303 [D loss: 0.671680, acc.: 61.11%] [G loss: 3.251548]\n",
      "3304 [D loss: 0.187547, acc.: 94.44%] [G loss: 3.558368]\n",
      "3305 [D loss: 0.518242, acc.: 72.22%] [G loss: 3.452242]\n",
      "3306 [D loss: 0.906801, acc.: 72.22%] [G loss: 2.282326]\n",
      "3307 [D loss: 0.131948, acc.: 94.44%] [G loss: 4.943842]\n",
      "3308 [D loss: 0.791270, acc.: 61.11%] [G loss: 2.562274]\n",
      "3309 [D loss: 0.198714, acc.: 88.89%] [G loss: 3.405733]\n",
      "3310 [D loss: 0.453892, acc.: 83.33%] [G loss: 2.504648]\n",
      "3311 [D loss: 0.267706, acc.: 94.44%] [G loss: 4.614065]\n",
      "3312 [D loss: 0.314740, acc.: 94.44%] [G loss: 2.880034]\n",
      "3313 [D loss: 0.234756, acc.: 88.89%] [G loss: 4.384359]\n",
      "3314 [D loss: 0.310048, acc.: 88.89%] [G loss: 3.321607]\n",
      "3315 [D loss: 0.357739, acc.: 77.78%] [G loss: 2.472865]\n",
      "3316 [D loss: 0.042386, acc.: 100.00%] [G loss: 5.081974]\n",
      "3317 [D loss: 0.170596, acc.: 94.44%] [G loss: 4.715971]\n",
      "3318 [D loss: 0.574052, acc.: 77.78%] [G loss: 3.529029]\n",
      "3319 [D loss: 0.223461, acc.: 94.44%] [G loss: 3.795764]\n",
      "3320 [D loss: 0.465556, acc.: 77.78%] [G loss: 3.194133]\n",
      "3321 [D loss: 0.439428, acc.: 77.78%] [G loss: 4.457561]\n",
      "3322 [D loss: 0.365783, acc.: 88.89%] [G loss: 4.221328]\n",
      "3323 [D loss: 0.174709, acc.: 94.44%] [G loss: 5.458700]\n",
      "3324 [D loss: 0.182278, acc.: 94.44%] [G loss: 3.619113]\n",
      "3325 [D loss: 0.330333, acc.: 83.33%] [G loss: 4.056303]\n",
      "3326 [D loss: 0.591784, acc.: 72.22%] [G loss: 2.723819]\n",
      "3327 [D loss: 0.119211, acc.: 94.44%] [G loss: 5.180056]\n",
      "3328 [D loss: 0.481428, acc.: 88.89%] [G loss: 2.970864]\n",
      "3329 [D loss: 0.109786, acc.: 100.00%] [G loss: 3.469221]\n",
      "3330 [D loss: 0.054604, acc.: 100.00%] [G loss: 3.919585]\n",
      "3331 [D loss: 0.304640, acc.: 88.89%] [G loss: 4.060257]\n",
      "3332 [D loss: 0.233191, acc.: 94.44%] [G loss: 5.041953]\n",
      "3333 [D loss: 0.065021, acc.: 100.00%] [G loss: 4.292553]\n",
      "3334 [D loss: 0.175927, acc.: 94.44%] [G loss: 4.755981]\n",
      "3335 [D loss: 0.111137, acc.: 100.00%] [G loss: 5.004751]\n",
      "3336 [D loss: 0.098696, acc.: 94.44%] [G loss: 5.433700]\n",
      "3337 [D loss: 0.352110, acc.: 88.89%] [G loss: 3.803343]\n",
      "3338 [D loss: 0.191560, acc.: 94.44%] [G loss: 3.891060]\n",
      "3339 [D loss: 0.485252, acc.: 88.89%] [G loss: 4.530360]\n",
      "3340 [D loss: 0.034454, acc.: 100.00%] [G loss: 5.546080]\n",
      "3341 [D loss: 0.091606, acc.: 100.00%] [G loss: 5.015308]\n",
      "3342 [D loss: 0.435981, acc.: 83.33%] [G loss: 3.784514]\n",
      "3343 [D loss: 0.447966, acc.: 77.78%] [G loss: 3.229329]\n",
      "3344 [D loss: 0.564712, acc.: 77.78%] [G loss: 3.156600]\n",
      "3345 [D loss: 0.442237, acc.: 77.78%] [G loss: 4.889275]\n",
      "3346 [D loss: 0.062100, acc.: 100.00%] [G loss: 4.922097]\n",
      "3347 [D loss: 0.312852, acc.: 88.89%] [G loss: 4.265054]\n",
      "3348 [D loss: 0.306181, acc.: 88.89%] [G loss: 4.249341]\n",
      "3349 [D loss: 0.370784, acc.: 83.33%] [G loss: 4.896968]\n",
      "3350 [D loss: 0.200396, acc.: 88.89%] [G loss: 2.871148]\n",
      "3351 [D loss: 0.200424, acc.: 94.44%] [G loss: 4.234448]\n",
      "3352 [D loss: 0.341953, acc.: 88.89%] [G loss: 3.560074]\n",
      "3353 [D loss: 0.376837, acc.: 77.78%] [G loss: 4.428492]\n",
      "3354 [D loss: 0.233600, acc.: 88.89%] [G loss: 3.613195]\n",
      "3355 [D loss: 0.171754, acc.: 88.89%] [G loss: 3.590479]\n",
      "3356 [D loss: 0.104502, acc.: 94.44%] [G loss: 4.126061]\n",
      "3357 [D loss: 0.493294, acc.: 77.78%] [G loss: 4.714683]\n",
      "3358 [D loss: 0.279044, acc.: 88.89%] [G loss: 3.689012]\n",
      "3359 [D loss: 0.695873, acc.: 55.56%] [G loss: 2.741896]\n",
      "3360 [D loss: 0.138738, acc.: 100.00%] [G loss: 3.761314]\n",
      "3361 [D loss: 0.087880, acc.: 94.44%] [G loss: 4.553086]\n",
      "3362 [D loss: 0.173740, acc.: 94.44%] [G loss: 4.464460]\n",
      "3363 [D loss: 0.324220, acc.: 83.33%] [G loss: 3.151689]\n",
      "3364 [D loss: 0.690820, acc.: 61.11%] [G loss: 4.772631]\n",
      "3365 [D loss: 0.311982, acc.: 72.22%] [G loss: 4.603496]\n",
      "3366 [D loss: 0.149809, acc.: 94.44%] [G loss: 4.577827]\n",
      "3367 [D loss: 0.266173, acc.: 83.33%] [G loss: 5.608943]\n",
      "3368 [D loss: 0.390023, acc.: 77.78%] [G loss: 3.708190]\n",
      "3369 [D loss: 0.571396, acc.: 77.78%] [G loss: 2.807963]\n",
      "3370 [D loss: 0.467951, acc.: 66.67%] [G loss: 3.785408]\n",
      "3371 [D loss: 0.752758, acc.: 66.67%] [G loss: 4.344893]\n",
      "3372 [D loss: 0.289766, acc.: 83.33%] [G loss: 5.626464]\n",
      "3373 [D loss: 0.295413, acc.: 88.89%] [G loss: 4.249498]\n",
      "3374 [D loss: 0.502417, acc.: 66.67%] [G loss: 4.022789]\n",
      "3375 [D loss: 0.362098, acc.: 77.78%] [G loss: 4.502455]\n",
      "3376 [D loss: 0.351870, acc.: 83.33%] [G loss: 5.838385]\n",
      "3377 [D loss: 0.335798, acc.: 83.33%] [G loss: 5.424593]\n",
      "3378 [D loss: 0.154910, acc.: 94.44%] [G loss: 4.475364]\n",
      "3379 [D loss: 0.064335, acc.: 100.00%] [G loss: 7.330956]\n",
      "3380 [D loss: 0.566546, acc.: 88.89%] [G loss: 4.547800]\n",
      "3381 [D loss: 0.041668, acc.: 100.00%] [G loss: 5.891238]\n",
      "3382 [D loss: 0.176354, acc.: 94.44%] [G loss: 4.798553]\n",
      "3383 [D loss: 0.506302, acc.: 83.33%] [G loss: 4.173089]\n",
      "3384 [D loss: 0.023840, acc.: 100.00%] [G loss: 5.655411]\n",
      "3385 [D loss: 0.438781, acc.: 83.33%] [G loss: 2.647400]\n",
      "3386 [D loss: 0.351334, acc.: 83.33%] [G loss: 4.042328]\n",
      "3387 [D loss: 0.061203, acc.: 100.00%] [G loss: 3.787330]\n",
      "3388 [D loss: 0.418550, acc.: 72.22%] [G loss: 3.210280]\n",
      "3389 [D loss: 0.080281, acc.: 100.00%] [G loss: 5.931797]\n",
      "3390 [D loss: 0.567398, acc.: 72.22%] [G loss: 3.937520]\n",
      "3391 [D loss: 0.276802, acc.: 88.89%] [G loss: 5.930659]\n",
      "3392 [D loss: 0.595886, acc.: 83.33%] [G loss: 3.279271]\n",
      "3393 [D loss: 0.722991, acc.: 72.22%] [G loss: 3.680368]\n",
      "3394 [D loss: 0.055656, acc.: 100.00%] [G loss: 4.522082]\n",
      "3395 [D loss: 0.336456, acc.: 83.33%] [G loss: 4.104595]\n",
      "3396 [D loss: 0.887952, acc.: 66.67%] [G loss: 3.103995]\n",
      "3397 [D loss: 0.266143, acc.: 94.44%] [G loss: 7.284603]\n",
      "3398 [D loss: 0.580543, acc.: 72.22%] [G loss: 3.858395]\n",
      "3399 [D loss: 0.091339, acc.: 94.44%] [G loss: 4.683897]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3400 [D loss: 0.354383, acc.: 83.33%] [G loss: 3.586464]\n",
      "3401 [D loss: 0.357022, acc.: 77.78%] [G loss: 4.223371]\n",
      "3402 [D loss: 0.117944, acc.: 100.00%] [G loss: 6.634307]\n",
      "3403 [D loss: 0.335177, acc.: 83.33%] [G loss: 4.039185]\n",
      "3404 [D loss: 0.616203, acc.: 72.22%] [G loss: 3.409747]\n",
      "3405 [D loss: 0.294200, acc.: 83.33%] [G loss: 4.256642]\n",
      "3406 [D loss: 0.270355, acc.: 88.89%] [G loss: 3.929876]\n",
      "3407 [D loss: 0.404767, acc.: 77.78%] [G loss: 3.659244]\n",
      "3408 [D loss: 0.388214, acc.: 83.33%] [G loss: 4.308411]\n",
      "3409 [D loss: 0.333927, acc.: 94.44%] [G loss: 2.658614]\n",
      "3410 [D loss: 0.412832, acc.: 72.22%] [G loss: 5.097488]\n",
      "3411 [D loss: 0.107701, acc.: 100.00%] [G loss: 5.495020]\n",
      "3412 [D loss: 0.187712, acc.: 88.89%] [G loss: 5.104458]\n",
      "3413 [D loss: 0.679026, acc.: 83.33%] [G loss: 4.247982]\n",
      "3414 [D loss: 0.846521, acc.: 61.11%] [G loss: 3.110122]\n",
      "3415 [D loss: 0.507354, acc.: 83.33%] [G loss: 3.451622]\n",
      "3416 [D loss: 0.273273, acc.: 88.89%] [G loss: 4.181192]\n",
      "3417 [D loss: 0.273178, acc.: 83.33%] [G loss: 4.215909]\n",
      "3418 [D loss: 0.362146, acc.: 88.89%] [G loss: 2.420471]\n",
      "3419 [D loss: 0.361579, acc.: 88.89%] [G loss: 3.157900]\n",
      "3420 [D loss: 0.301153, acc.: 88.89%] [G loss: 3.745214]\n",
      "3421 [D loss: 0.043488, acc.: 100.00%] [G loss: 2.932934]\n",
      "3422 [D loss: 0.094720, acc.: 94.44%] [G loss: 4.375350]\n",
      "3423 [D loss: 0.609553, acc.: 61.11%] [G loss: 3.426835]\n",
      "3424 [D loss: 0.453230, acc.: 72.22%] [G loss: 4.139542]\n",
      "3425 [D loss: 0.293215, acc.: 88.89%] [G loss: 4.584927]\n",
      "3426 [D loss: 0.275173, acc.: 94.44%] [G loss: 3.396198]\n",
      "3427 [D loss: 0.050376, acc.: 100.00%] [G loss: 3.129007]\n",
      "3428 [D loss: 0.513488, acc.: 83.33%] [G loss: 2.983112]\n",
      "3429 [D loss: 0.283701, acc.: 83.33%] [G loss: 4.623953]\n",
      "3430 [D loss: 0.446192, acc.: 88.89%] [G loss: 3.212523]\n",
      "3431 [D loss: 0.230781, acc.: 88.89%] [G loss: 3.192982]\n",
      "3432 [D loss: 0.466757, acc.: 83.33%] [G loss: 3.021901]\n",
      "3433 [D loss: 0.408102, acc.: 83.33%] [G loss: 3.060937]\n",
      "3434 [D loss: 0.172432, acc.: 88.89%] [G loss: 4.602273]\n",
      "3435 [D loss: 0.259821, acc.: 94.44%] [G loss: 5.649633]\n",
      "3436 [D loss: 0.167464, acc.: 88.89%] [G loss: 6.009787]\n",
      "3437 [D loss: 0.350688, acc.: 88.89%] [G loss: 3.721660]\n",
      "3438 [D loss: 0.291025, acc.: 94.44%] [G loss: 3.288294]\n",
      "3439 [D loss: 0.420134, acc.: 83.33%] [G loss: 3.903722]\n",
      "3440 [D loss: 0.126814, acc.: 94.44%] [G loss: 3.925988]\n",
      "3441 [D loss: 0.600502, acc.: 83.33%] [G loss: 4.283045]\n",
      "3442 [D loss: 0.644013, acc.: 55.56%] [G loss: 3.837936]\n",
      "3443 [D loss: 0.124975, acc.: 100.00%] [G loss: 3.772538]\n",
      "3444 [D loss: 0.025748, acc.: 100.00%] [G loss: 5.456758]\n",
      "3445 [D loss: 0.089872, acc.: 94.44%] [G loss: 5.659056]\n",
      "3446 [D loss: 0.221391, acc.: 88.89%] [G loss: 3.037165]\n",
      "3447 [D loss: 0.130094, acc.: 94.44%] [G loss: 3.973386]\n",
      "3448 [D loss: 0.243345, acc.: 88.89%] [G loss: 3.771550]\n",
      "3449 [D loss: 0.114363, acc.: 100.00%] [G loss: 3.645757]\n",
      "3450 [D loss: 0.193203, acc.: 88.89%] [G loss: 3.547034]\n",
      "3451 [D loss: 0.089055, acc.: 100.00%] [G loss: 5.638762]\n",
      "3452 [D loss: 0.430616, acc.: 77.78%] [G loss: 3.809545]\n",
      "3453 [D loss: 0.096812, acc.: 100.00%] [G loss: 5.183493]\n",
      "3454 [D loss: 0.181560, acc.: 94.44%] [G loss: 4.726960]\n",
      "3455 [D loss: 0.085152, acc.: 100.00%] [G loss: 4.851914]\n",
      "3456 [D loss: 0.193738, acc.: 94.44%] [G loss: 4.603721]\n",
      "3457 [D loss: 0.037945, acc.: 100.00%] [G loss: 6.960021]\n",
      "3458 [D loss: 0.650239, acc.: 72.22%] [G loss: 2.120241]\n",
      "3459 [D loss: 0.355685, acc.: 72.22%] [G loss: 4.575747]\n",
      "3460 [D loss: 0.138884, acc.: 88.89%] [G loss: 4.032670]\n",
      "3461 [D loss: 0.145745, acc.: 94.44%] [G loss: 4.888137]\n",
      "3462 [D loss: 0.138780, acc.: 100.00%] [G loss: 5.726517]\n",
      "3463 [D loss: 0.111359, acc.: 100.00%] [G loss: 4.637512]\n",
      "3464 [D loss: 0.271114, acc.: 83.33%] [G loss: 3.664317]\n",
      "3465 [D loss: 0.203729, acc.: 88.89%] [G loss: 4.459054]\n",
      "3466 [D loss: 0.378088, acc.: 83.33%] [G loss: 3.425666]\n",
      "3467 [D loss: 0.454529, acc.: 77.78%] [G loss: 3.029193]\n",
      "3468 [D loss: 0.250784, acc.: 88.89%] [G loss: 3.730112]\n",
      "3469 [D loss: 0.423068, acc.: 77.78%] [G loss: 3.677617]\n",
      "3470 [D loss: 1.036926, acc.: 55.56%] [G loss: 5.645720]\n",
      "3471 [D loss: 0.184975, acc.: 94.44%] [G loss: 5.553123]\n",
      "3472 [D loss: 1.009721, acc.: 50.00%] [G loss: 3.999337]\n",
      "3473 [D loss: 0.450315, acc.: 83.33%] [G loss: 4.590368]\n",
      "3474 [D loss: 0.256385, acc.: 94.44%] [G loss: 3.965641]\n",
      "3475 [D loss: 0.539669, acc.: 83.33%] [G loss: 3.281601]\n",
      "3476 [D loss: 0.194091, acc.: 88.89%] [G loss: 4.171043]\n",
      "3477 [D loss: 0.369761, acc.: 83.33%] [G loss: 5.554812]\n",
      "3478 [D loss: 0.060029, acc.: 100.00%] [G loss: 3.152908]\n",
      "3479 [D loss: 0.089225, acc.: 100.00%] [G loss: 3.632505]\n",
      "3480 [D loss: 0.518562, acc.: 83.33%] [G loss: 4.191217]\n",
      "3481 [D loss: 1.045994, acc.: 61.11%] [G loss: 3.858892]\n",
      "3482 [D loss: 0.077764, acc.: 100.00%] [G loss: 5.961405]\n",
      "3483 [D loss: 0.194138, acc.: 88.89%] [G loss: 4.444834]\n",
      "3484 [D loss: 0.077704, acc.: 94.44%] [G loss: 5.848614]\n",
      "3485 [D loss: 0.072121, acc.: 100.00%] [G loss: 6.954309]\n",
      "3486 [D loss: 0.382007, acc.: 83.33%] [G loss: 4.519671]\n",
      "3487 [D loss: 0.032632, acc.: 100.00%] [G loss: 5.666417]\n",
      "3488 [D loss: 0.205825, acc.: 88.89%] [G loss: 4.123802]\n",
      "3489 [D loss: 0.035464, acc.: 100.00%] [G loss: 4.107964]\n",
      "3490 [D loss: 0.650603, acc.: 77.78%] [G loss: 2.499615]\n",
      "3491 [D loss: 0.374052, acc.: 77.78%] [G loss: 3.964104]\n",
      "3492 [D loss: 0.993401, acc.: 61.11%] [G loss: 2.765815]\n",
      "3493 [D loss: 0.114243, acc.: 94.44%] [G loss: 5.424327]\n",
      "3494 [D loss: 0.345618, acc.: 88.89%] [G loss: 5.305157]\n",
      "3495 [D loss: 0.083514, acc.: 94.44%] [G loss: 6.351603]\n",
      "3496 [D loss: 0.085146, acc.: 94.44%] [G loss: 5.167952]\n",
      "3497 [D loss: 0.316520, acc.: 94.44%] [G loss: 3.589638]\n",
      "3498 [D loss: 0.278727, acc.: 94.44%] [G loss: 3.062800]\n",
      "3499 [D loss: 0.363292, acc.: 72.22%] [G loss: 3.758449]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500 [D loss: 0.080818, acc.: 94.44%] [G loss: 5.971665]\n",
      "3501 [D loss: 0.468918, acc.: 83.33%] [G loss: 5.895961]\n",
      "3502 [D loss: 0.310438, acc.: 83.33%] [G loss: 4.309778]\n",
      "3503 [D loss: 0.336820, acc.: 77.78%] [G loss: 3.900460]\n",
      "3504 [D loss: 0.127685, acc.: 100.00%] [G loss: 4.947404]\n",
      "3505 [D loss: 0.055759, acc.: 100.00%] [G loss: 3.183752]\n",
      "3506 [D loss: 0.163576, acc.: 94.44%] [G loss: 5.060567]\n",
      "3507 [D loss: 0.328444, acc.: 83.33%] [G loss: 4.471160]\n",
      "3508 [D loss: 0.110257, acc.: 94.44%] [G loss: 4.345694]\n",
      "3509 [D loss: 0.173484, acc.: 88.89%] [G loss: 3.954182]\n",
      "3510 [D loss: 0.084395, acc.: 100.00%] [G loss: 5.937687]\n",
      "3511 [D loss: 0.170849, acc.: 94.44%] [G loss: 4.143595]\n",
      "3512 [D loss: 0.700789, acc.: 66.67%] [G loss: 3.652858]\n",
      "3513 [D loss: 0.434092, acc.: 88.89%] [G loss: 5.460903]\n",
      "3514 [D loss: 0.080135, acc.: 100.00%] [G loss: 5.762774]\n",
      "3515 [D loss: 0.178269, acc.: 94.44%] [G loss: 4.346514]\n",
      "3516 [D loss: 0.281840, acc.: 88.89%] [G loss: 3.500247]\n",
      "3517 [D loss: 0.122754, acc.: 94.44%] [G loss: 4.167746]\n",
      "3518 [D loss: 0.249162, acc.: 88.89%] [G loss: 3.989642]\n",
      "3519 [D loss: 0.208299, acc.: 94.44%] [G loss: 4.605459]\n",
      "3520 [D loss: 0.598575, acc.: 77.78%] [G loss: 3.267826]\n",
      "3521 [D loss: 0.203253, acc.: 94.44%] [G loss: 4.143972]\n",
      "3522 [D loss: 0.151309, acc.: 83.33%] [G loss: 3.771778]\n",
      "3523 [D loss: 0.234111, acc.: 88.89%] [G loss: 5.505467]\n",
      "3524 [D loss: 0.300883, acc.: 77.78%] [G loss: 3.537986]\n",
      "3525 [D loss: 0.396132, acc.: 83.33%] [G loss: 3.884492]\n",
      "3526 [D loss: 0.443850, acc.: 77.78%] [G loss: 4.068834]\n",
      "3527 [D loss: 0.041743, acc.: 100.00%] [G loss: 5.349565]\n",
      "3528 [D loss: 0.132745, acc.: 94.44%] [G loss: 5.370954]\n",
      "3529 [D loss: 0.034417, acc.: 100.00%] [G loss: 6.801944]\n",
      "3530 [D loss: 0.262263, acc.: 94.44%] [G loss: 5.421200]\n",
      "3531 [D loss: 0.353075, acc.: 77.78%] [G loss: 3.437636]\n",
      "3532 [D loss: 0.659660, acc.: 72.22%] [G loss: 3.183944]\n",
      "3533 [D loss: 0.247626, acc.: 88.89%] [G loss: 3.775398]\n",
      "3534 [D loss: 0.065392, acc.: 94.44%] [G loss: 4.436768]\n",
      "3535 [D loss: 0.198113, acc.: 88.89%] [G loss: 4.042938]\n",
      "3536 [D loss: 0.084358, acc.: 100.00%] [G loss: 6.167389]\n",
      "3537 [D loss: 0.280034, acc.: 83.33%] [G loss: 2.871665]\n",
      "3538 [D loss: 0.119178, acc.: 100.00%] [G loss: 3.000066]\n",
      "3539 [D loss: 0.420918, acc.: 83.33%] [G loss: 3.113467]\n",
      "3540 [D loss: 0.073543, acc.: 100.00%] [G loss: 5.683333]\n",
      "3541 [D loss: 0.389601, acc.: 88.89%] [G loss: 3.324189]\n",
      "3542 [D loss: 0.483659, acc.: 66.67%] [G loss: 4.276305]\n",
      "3543 [D loss: 0.220875, acc.: 83.33%] [G loss: 5.806742]\n",
      "3544 [D loss: 0.076068, acc.: 100.00%] [G loss: 5.249640]\n",
      "3545 [D loss: 0.128513, acc.: 94.44%] [G loss: 4.736124]\n",
      "3546 [D loss: 0.029058, acc.: 100.00%] [G loss: 6.353957]\n",
      "3547 [D loss: 0.080355, acc.: 100.00%] [G loss: 5.069212]\n",
      "3548 [D loss: 0.253358, acc.: 88.89%] [G loss: 3.974597]\n",
      "3549 [D loss: 0.149705, acc.: 94.44%] [G loss: 3.508613]\n",
      "3550 [D loss: 1.052109, acc.: 61.11%] [G loss: 3.417639]\n",
      "3551 [D loss: 0.671873, acc.: 66.67%] [G loss: 3.649840]\n",
      "3552 [D loss: 0.715781, acc.: 72.22%] [G loss: 3.642985]\n",
      "3553 [D loss: 0.231285, acc.: 88.89%] [G loss: 3.447416]\n",
      "3554 [D loss: 0.149463, acc.: 94.44%] [G loss: 6.482491]\n",
      "3555 [D loss: 0.020725, acc.: 100.00%] [G loss: 7.146674]\n",
      "3556 [D loss: 0.720268, acc.: 83.33%] [G loss: 4.030546]\n",
      "3557 [D loss: 0.188443, acc.: 88.89%] [G loss: 4.882335]\n",
      "3558 [D loss: 0.548653, acc.: 72.22%] [G loss: 2.732943]\n",
      "3559 [D loss: 0.157036, acc.: 88.89%] [G loss: 4.640471]\n",
      "3560 [D loss: 0.541992, acc.: 77.78%] [G loss: 4.082490]\n",
      "3561 [D loss: 0.206318, acc.: 88.89%] [G loss: 4.821268]\n",
      "3562 [D loss: 0.121901, acc.: 94.44%] [G loss: 5.685514]\n",
      "3563 [D loss: 0.004230, acc.: 100.00%] [G loss: 5.553860]\n",
      "3564 [D loss: 0.286584, acc.: 88.89%] [G loss: 5.153427]\n",
      "3565 [D loss: 0.268406, acc.: 88.89%] [G loss: 5.419600]\n",
      "3566 [D loss: 0.369325, acc.: 83.33%] [G loss: 4.079523]\n",
      "3567 [D loss: 0.020804, acc.: 100.00%] [G loss: 6.711100]\n",
      "3568 [D loss: 0.030064, acc.: 100.00%] [G loss: 5.973948]\n",
      "3569 [D loss: 0.131024, acc.: 94.44%] [G loss: 3.615669]\n",
      "3570 [D loss: 0.571843, acc.: 77.78%] [G loss: 2.622631]\n",
      "3571 [D loss: 0.146467, acc.: 100.00%] [G loss: 3.666551]\n",
      "3572 [D loss: 0.420387, acc.: 77.78%] [G loss: 4.002246]\n",
      "3573 [D loss: 0.097998, acc.: 100.00%] [G loss: 4.768876]\n",
      "3574 [D loss: 0.154075, acc.: 88.89%] [G loss: 4.378349]\n",
      "3575 [D loss: 0.621466, acc.: 72.22%] [G loss: 4.327274]\n",
      "3576 [D loss: 0.063489, acc.: 100.00%] [G loss: 4.065901]\n",
      "3577 [D loss: 0.169567, acc.: 94.44%] [G loss: 4.526722]\n",
      "3578 [D loss: 0.373277, acc.: 83.33%] [G loss: 3.556484]\n",
      "3579 [D loss: 0.432039, acc.: 83.33%] [G loss: 3.459628]\n",
      "3580 [D loss: 0.655758, acc.: 66.67%] [G loss: 2.991918]\n",
      "3581 [D loss: 0.046560, acc.: 100.00%] [G loss: 5.248147]\n",
      "3582 [D loss: 0.457535, acc.: 94.44%] [G loss: 4.461364]\n",
      "3583 [D loss: 0.490221, acc.: 77.78%] [G loss: 3.471423]\n",
      "3584 [D loss: 0.233420, acc.: 94.44%] [G loss: 5.057731]\n",
      "3585 [D loss: 0.340801, acc.: 88.89%] [G loss: 3.761494]\n",
      "3586 [D loss: 0.265197, acc.: 88.89%] [G loss: 4.129267]\n",
      "3587 [D loss: 0.034144, acc.: 100.00%] [G loss: 4.537849]\n",
      "3588 [D loss: 0.085842, acc.: 100.00%] [G loss: 3.740800]\n",
      "3589 [D loss: 0.227184, acc.: 88.89%] [G loss: 2.676682]\n",
      "3590 [D loss: 0.252153, acc.: 94.44%] [G loss: 4.920547]\n",
      "3591 [D loss: 0.037851, acc.: 100.00%] [G loss: 4.477527]\n",
      "3592 [D loss: 0.219528, acc.: 88.89%] [G loss: 5.669616]\n",
      "3593 [D loss: 0.456809, acc.: 83.33%] [G loss: 3.205244]\n",
      "3594 [D loss: 0.394508, acc.: 88.89%] [G loss: 4.383648]\n",
      "3595 [D loss: 0.342815, acc.: 83.33%] [G loss: 4.505978]\n",
      "3596 [D loss: 0.127172, acc.: 88.89%] [G loss: 4.718293]\n",
      "3597 [D loss: 0.095975, acc.: 100.00%] [G loss: 3.967711]\n",
      "3598 [D loss: 0.165102, acc.: 88.89%] [G loss: 2.840838]\n",
      "3599 [D loss: 0.105055, acc.: 94.44%] [G loss: 5.795898]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600 [D loss: 0.051785, acc.: 100.00%] [G loss: 4.564919]\n",
      "3601 [D loss: 0.310256, acc.: 88.89%] [G loss: 3.101705]\n",
      "3602 [D loss: 0.009957, acc.: 100.00%] [G loss: 8.230158]\n",
      "3603 [D loss: 0.174064, acc.: 94.44%] [G loss: 4.424785]\n",
      "3604 [D loss: 0.365342, acc.: 83.33%] [G loss: 7.141794]\n",
      "3605 [D loss: 0.364450, acc.: 77.78%] [G loss: 4.947406]\n",
      "3606 [D loss: 0.097538, acc.: 94.44%] [G loss: 5.111593]\n",
      "3607 [D loss: 0.112035, acc.: 94.44%] [G loss: 4.786398]\n",
      "3608 [D loss: 0.054425, acc.: 100.00%] [G loss: 6.546942]\n",
      "3609 [D loss: 0.347509, acc.: 83.33%] [G loss: 4.196056]\n",
      "3610 [D loss: 0.036646, acc.: 100.00%] [G loss: 5.637110]\n",
      "3611 [D loss: 0.076786, acc.: 94.44%] [G loss: 5.970282]\n",
      "3612 [D loss: 0.530788, acc.: 88.89%] [G loss: 4.579981]\n",
      "3613 [D loss: 0.136717, acc.: 94.44%] [G loss: 6.530170]\n",
      "3614 [D loss: 0.079660, acc.: 94.44%] [G loss: 5.288044]\n",
      "3615 [D loss: 0.096776, acc.: 100.00%] [G loss: 5.461411]\n",
      "3616 [D loss: 0.115968, acc.: 94.44%] [G loss: 4.247103]\n",
      "3617 [D loss: 0.237734, acc.: 88.89%] [G loss: 3.961892]\n",
      "3618 [D loss: 0.320815, acc.: 83.33%] [G loss: 3.240066]\n",
      "3619 [D loss: 0.229902, acc.: 88.89%] [G loss: 6.125020]\n",
      "3620 [D loss: 0.073347, acc.: 100.00%] [G loss: 6.594207]\n",
      "3621 [D loss: 0.328066, acc.: 88.89%] [G loss: 4.207889]\n",
      "3622 [D loss: 0.016569, acc.: 100.00%] [G loss: 5.594236]\n",
      "3623 [D loss: 0.094158, acc.: 100.00%] [G loss: 4.235661]\n",
      "3624 [D loss: 0.179833, acc.: 94.44%] [G loss: 5.068126]\n",
      "3625 [D loss: 0.187966, acc.: 88.89%] [G loss: 5.770768]\n",
      "3626 [D loss: 0.101487, acc.: 100.00%] [G loss: 4.129519]\n",
      "3627 [D loss: 0.581644, acc.: 77.78%] [G loss: 4.581064]\n",
      "3628 [D loss: 0.126312, acc.: 94.44%] [G loss: 4.875091]\n",
      "3629 [D loss: 0.515127, acc.: 83.33%] [G loss: 5.372781]\n",
      "3630 [D loss: 0.291270, acc.: 77.78%] [G loss: 4.708056]\n",
      "3631 [D loss: 0.398058, acc.: 88.89%] [G loss: 5.451373]\n",
      "3632 [D loss: 0.691213, acc.: 72.22%] [G loss: 4.302857]\n",
      "3633 [D loss: 0.331794, acc.: 83.33%] [G loss: 4.730687]\n",
      "3634 [D loss: 0.085638, acc.: 100.00%] [G loss: 4.883321]\n",
      "3635 [D loss: 0.020842, acc.: 100.00%] [G loss: 5.614846]\n",
      "3636 [D loss: 0.233144, acc.: 94.44%] [G loss: 5.359438]\n",
      "3637 [D loss: 0.289095, acc.: 88.89%] [G loss: 5.340501]\n",
      "3638 [D loss: 0.238776, acc.: 83.33%] [G loss: 3.181757]\n",
      "3639 [D loss: 0.227106, acc.: 83.33%] [G loss: 3.696329]\n",
      "3640 [D loss: 0.213509, acc.: 94.44%] [G loss: 3.910403]\n",
      "3641 [D loss: 0.096983, acc.: 94.44%] [G loss: 4.943549]\n",
      "3642 [D loss: 0.013947, acc.: 100.00%] [G loss: 6.495663]\n",
      "3643 [D loss: 0.494058, acc.: 72.22%] [G loss: 2.915799]\n",
      "3644 [D loss: 0.173930, acc.: 94.44%] [G loss: 4.625669]\n",
      "3645 [D loss: 0.105210, acc.: 100.00%] [G loss: 3.348156]\n",
      "3646 [D loss: 0.251029, acc.: 88.89%] [G loss: 3.619941]\n",
      "3647 [D loss: 0.420695, acc.: 83.33%] [G loss: 5.497035]\n",
      "3648 [D loss: 0.089318, acc.: 100.00%] [G loss: 4.922897]\n",
      "3649 [D loss: 0.125016, acc.: 94.44%] [G loss: 4.347076]\n",
      "3650 [D loss: 0.040796, acc.: 100.00%] [G loss: 4.479510]\n",
      "3651 [D loss: 0.069848, acc.: 100.00%] [G loss: 3.862384]\n",
      "3652 [D loss: 0.119565, acc.: 100.00%] [G loss: 4.204212]\n",
      "3653 [D loss: 0.065793, acc.: 100.00%] [G loss: 5.546889]\n",
      "3654 [D loss: 0.347524, acc.: 77.78%] [G loss: 5.132908]\n",
      "3655 [D loss: 0.115504, acc.: 94.44%] [G loss: 4.563569]\n",
      "3656 [D loss: 0.458667, acc.: 77.78%] [G loss: 4.116362]\n",
      "3657 [D loss: 0.529890, acc.: 77.78%] [G loss: 4.121470]\n",
      "3658 [D loss: 0.069239, acc.: 100.00%] [G loss: 4.620729]\n",
      "3659 [D loss: 0.674933, acc.: 83.33%] [G loss: 4.495709]\n",
      "3660 [D loss: 0.492673, acc.: 83.33%] [G loss: 4.698391]\n",
      "3661 [D loss: 0.076933, acc.: 100.00%] [G loss: 4.956402]\n",
      "3662 [D loss: 0.529092, acc.: 77.78%] [G loss: 2.872922]\n",
      "3663 [D loss: 0.204817, acc.: 88.89%] [G loss: 3.730330]\n",
      "3664 [D loss: 0.184781, acc.: 88.89%] [G loss: 6.034163]\n",
      "3665 [D loss: 0.271796, acc.: 88.89%] [G loss: 3.962009]\n",
      "3666 [D loss: 0.093908, acc.: 94.44%] [G loss: 6.307129]\n",
      "3667 [D loss: 0.181019, acc.: 88.89%] [G loss: 4.503615]\n",
      "3668 [D loss: 0.167636, acc.: 94.44%] [G loss: 3.288694]\n",
      "3669 [D loss: 0.425747, acc.: 77.78%] [G loss: 2.922343]\n",
      "3670 [D loss: 0.029361, acc.: 100.00%] [G loss: 4.971487]\n",
      "3671 [D loss: 0.107279, acc.: 100.00%] [G loss: 5.833747]\n",
      "3672 [D loss: 0.174606, acc.: 94.44%] [G loss: 4.886609]\n",
      "3673 [D loss: 0.130988, acc.: 94.44%] [G loss: 4.706756]\n",
      "3674 [D loss: 0.070839, acc.: 100.00%] [G loss: 4.457891]\n",
      "3675 [D loss: 0.059580, acc.: 100.00%] [G loss: 3.630311]\n",
      "3676 [D loss: 0.589563, acc.: 77.78%] [G loss: 4.051327]\n",
      "3677 [D loss: 0.171397, acc.: 94.44%] [G loss: 5.070002]\n",
      "3678 [D loss: 0.211704, acc.: 94.44%] [G loss: 3.865774]\n",
      "3679 [D loss: 0.571356, acc.: 88.89%] [G loss: 3.484888]\n",
      "3680 [D loss: 0.395070, acc.: 83.33%] [G loss: 4.636044]\n",
      "3681 [D loss: 0.592783, acc.: 77.78%] [G loss: 4.544142]\n",
      "3682 [D loss: 0.082563, acc.: 100.00%] [G loss: 6.435941]\n",
      "3683 [D loss: 0.401975, acc.: 83.33%] [G loss: 3.544820]\n",
      "3684 [D loss: 0.118785, acc.: 94.44%] [G loss: 3.697851]\n",
      "3685 [D loss: 0.268118, acc.: 83.33%] [G loss: 3.167107]\n",
      "3686 [D loss: 0.034983, acc.: 100.00%] [G loss: 6.052929]\n",
      "3687 [D loss: 0.095016, acc.: 100.00%] [G loss: 5.654182]\n",
      "3688 [D loss: 0.008703, acc.: 100.00%] [G loss: 6.629354]\n",
      "3689 [D loss: 0.148446, acc.: 94.44%] [G loss: 3.993494]\n",
      "3690 [D loss: 0.216070, acc.: 88.89%] [G loss: 5.368565]\n",
      "3691 [D loss: 0.241105, acc.: 94.44%] [G loss: 4.195386]\n",
      "3692 [D loss: 0.210048, acc.: 88.89%] [G loss: 6.057508]\n",
      "3693 [D loss: 0.201243, acc.: 94.44%] [G loss: 3.725145]\n",
      "3694 [D loss: 0.138658, acc.: 94.44%] [G loss: 3.484013]\n",
      "3695 [D loss: 0.250422, acc.: 94.44%] [G loss: 3.879756]\n",
      "3696 [D loss: 0.259307, acc.: 83.33%] [G loss: 5.398175]\n",
      "3697 [D loss: 0.123180, acc.: 94.44%] [G loss: 5.481233]\n",
      "3698 [D loss: 0.264481, acc.: 88.89%] [G loss: 3.520227]\n",
      "3699 [D loss: 0.143199, acc.: 94.44%] [G loss: 4.263563]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3700 [D loss: 0.900035, acc.: 66.67%] [G loss: 3.700339]\n",
      "3701 [D loss: 0.759618, acc.: 77.78%] [G loss: 3.743007]\n",
      "3702 [D loss: 0.196926, acc.: 94.44%] [G loss: 4.205482]\n",
      "3703 [D loss: 0.560345, acc.: 72.22%] [G loss: 1.782932]\n",
      "3704 [D loss: 0.331051, acc.: 83.33%] [G loss: 3.537481]\n",
      "3705 [D loss: 0.142546, acc.: 94.44%] [G loss: 4.405816]\n",
      "3706 [D loss: 0.182799, acc.: 88.89%] [G loss: 4.140330]\n",
      "3707 [D loss: 0.686518, acc.: 66.67%] [G loss: 3.458551]\n",
      "3708 [D loss: 0.049612, acc.: 100.00%] [G loss: 5.204874]\n",
      "3709 [D loss: 0.083866, acc.: 100.00%] [G loss: 5.252498]\n",
      "3710 [D loss: 0.114759, acc.: 100.00%] [G loss: 4.262151]\n",
      "3711 [D loss: 0.172241, acc.: 94.44%] [G loss: 3.801466]\n",
      "3712 [D loss: 0.095387, acc.: 94.44%] [G loss: 7.761630]\n",
      "3713 [D loss: 0.179120, acc.: 94.44%] [G loss: 5.573580]\n",
      "3714 [D loss: 0.309821, acc.: 83.33%] [G loss: 3.798273]\n",
      "3715 [D loss: 0.303493, acc.: 88.89%] [G loss: 4.342429]\n",
      "3716 [D loss: 0.262244, acc.: 88.89%] [G loss: 5.330060]\n",
      "3717 [D loss: 0.123139, acc.: 94.44%] [G loss: 6.871464]\n",
      "3718 [D loss: 0.225421, acc.: 88.89%] [G loss: 4.129799]\n",
      "3719 [D loss: 0.102304, acc.: 94.44%] [G loss: 4.313686]\n",
      "3720 [D loss: 0.058793, acc.: 100.00%] [G loss: 4.910589]\n",
      "3721 [D loss: 0.151309, acc.: 94.44%] [G loss: 3.589171]\n",
      "3722 [D loss: 0.298320, acc.: 77.78%] [G loss: 3.051391]\n",
      "3723 [D loss: 0.103051, acc.: 94.44%] [G loss: 3.797021]\n",
      "3724 [D loss: 0.181137, acc.: 88.89%] [G loss: 4.273626]\n",
      "3725 [D loss: 0.407686, acc.: 83.33%] [G loss: 3.403405]\n",
      "3726 [D loss: 0.195010, acc.: 88.89%] [G loss: 4.593339]\n",
      "3727 [D loss: 0.078841, acc.: 94.44%] [G loss: 5.072325]\n",
      "3728 [D loss: 0.990945, acc.: 61.11%] [G loss: 3.543319]\n",
      "3729 [D loss: 0.430529, acc.: 77.78%] [G loss: 3.633817]\n",
      "3730 [D loss: 0.065183, acc.: 94.44%] [G loss: 4.523332]\n",
      "3731 [D loss: 0.061092, acc.: 100.00%] [G loss: 4.778169]\n",
      "3732 [D loss: 0.537607, acc.: 83.33%] [G loss: 4.563817]\n",
      "3733 [D loss: 0.435829, acc.: 83.33%] [G loss: 4.054225]\n",
      "3734 [D loss: 0.049162, acc.: 100.00%] [G loss: 5.074556]\n",
      "3735 [D loss: 0.382136, acc.: 83.33%] [G loss: 4.334908]\n",
      "3736 [D loss: 0.271559, acc.: 88.89%] [G loss: 3.741983]\n",
      "3737 [D loss: 0.572246, acc.: 61.11%] [G loss: 3.728060]\n",
      "3738 [D loss: 0.172370, acc.: 88.89%] [G loss: 5.290059]\n",
      "3739 [D loss: 0.627708, acc.: 88.89%] [G loss: 7.277853]\n",
      "3740 [D loss: 0.120332, acc.: 94.44%] [G loss: 3.977268]\n",
      "3741 [D loss: 0.237037, acc.: 94.44%] [G loss: 3.774562]\n",
      "3742 [D loss: 0.234315, acc.: 88.89%] [G loss: 4.029575]\n",
      "3743 [D loss: 0.045234, acc.: 100.00%] [G loss: 4.945693]\n",
      "3744 [D loss: 0.275318, acc.: 88.89%] [G loss: 3.355824]\n",
      "3745 [D loss: 0.132733, acc.: 100.00%] [G loss: 3.995939]\n",
      "3746 [D loss: 0.018678, acc.: 100.00%] [G loss: 7.235568]\n",
      "3747 [D loss: 0.162017, acc.: 94.44%] [G loss: 4.014528]\n",
      "3748 [D loss: 0.391788, acc.: 83.33%] [G loss: 5.438668]\n",
      "3749 [D loss: 0.519090, acc.: 83.33%] [G loss: 5.307525]\n",
      "3750 [D loss: 0.034919, acc.: 100.00%] [G loss: 6.981964]\n",
      "3751 [D loss: 0.090439, acc.: 100.00%] [G loss: 6.027148]\n",
      "3752 [D loss: 0.045197, acc.: 100.00%] [G loss: 5.833516]\n",
      "3753 [D loss: 0.023679, acc.: 100.00%] [G loss: 6.892859]\n",
      "3754 [D loss: 0.388657, acc.: 83.33%] [G loss: 5.755315]\n",
      "3755 [D loss: 0.223935, acc.: 88.89%] [G loss: 4.204832]\n",
      "3756 [D loss: 0.009843, acc.: 100.00%] [G loss: 5.554976]\n",
      "3757 [D loss: 0.205099, acc.: 88.89%] [G loss: 4.450871]\n",
      "3758 [D loss: 0.445511, acc.: 77.78%] [G loss: 4.528864]\n",
      "3759 [D loss: 0.451853, acc.: 83.33%] [G loss: 3.510709]\n",
      "3760 [D loss: 0.102605, acc.: 94.44%] [G loss: 5.156515]\n",
      "3761 [D loss: 0.409476, acc.: 83.33%] [G loss: 5.248180]\n",
      "3762 [D loss: 0.085945, acc.: 100.00%] [G loss: 5.066629]\n",
      "3763 [D loss: 0.268590, acc.: 88.89%] [G loss: 4.031053]\n",
      "3764 [D loss: 0.157295, acc.: 94.44%] [G loss: 3.150723]\n",
      "3765 [D loss: 0.495412, acc.: 77.78%] [G loss: 3.536376]\n",
      "3766 [D loss: 0.387000, acc.: 83.33%] [G loss: 4.907224]\n",
      "3767 [D loss: 0.064937, acc.: 100.00%] [G loss: 8.061497]\n",
      "3768 [D loss: 0.103795, acc.: 94.44%] [G loss: 5.897102]\n",
      "3769 [D loss: 0.384334, acc.: 88.89%] [G loss: 3.884478]\n",
      "3770 [D loss: 0.085793, acc.: 100.00%] [G loss: 5.999508]\n",
      "3771 [D loss: 0.055399, acc.: 100.00%] [G loss: 4.922091]\n",
      "3772 [D loss: 0.171959, acc.: 88.89%] [G loss: 5.980464]\n",
      "3773 [D loss: 0.169572, acc.: 94.44%] [G loss: 3.700005]\n",
      "3774 [D loss: 0.055252, acc.: 100.00%] [G loss: 5.709217]\n",
      "3775 [D loss: 0.317015, acc.: 88.89%] [G loss: 4.706712]\n",
      "3776 [D loss: 0.298350, acc.: 83.33%] [G loss: 3.368448]\n",
      "3777 [D loss: 0.225239, acc.: 94.44%] [G loss: 7.206771]\n",
      "3778 [D loss: 0.410670, acc.: 72.22%] [G loss: 3.923342]\n",
      "3779 [D loss: 0.340128, acc.: 88.89%] [G loss: 4.855384]\n",
      "3780 [D loss: 0.071333, acc.: 100.00%] [G loss: 5.100295]\n",
      "3781 [D loss: 0.317588, acc.: 94.44%] [G loss: 3.804966]\n",
      "3782 [D loss: 0.041194, acc.: 100.00%] [G loss: 5.462881]\n",
      "3783 [D loss: 0.461409, acc.: 77.78%] [G loss: 3.210834]\n",
      "3784 [D loss: 0.627472, acc.: 77.78%] [G loss: 3.590280]\n",
      "3785 [D loss: 0.146123, acc.: 94.44%] [G loss: 6.221148]\n",
      "3786 [D loss: 0.195807, acc.: 94.44%] [G loss: 3.943707]\n",
      "3787 [D loss: 0.634393, acc.: 55.56%] [G loss: 3.175661]\n",
      "3788 [D loss: 0.301588, acc.: 88.89%] [G loss: 4.187203]\n",
      "3789 [D loss: 0.072746, acc.: 100.00%] [G loss: 5.513595]\n",
      "3790 [D loss: 0.194982, acc.: 88.89%] [G loss: 3.240178]\n",
      "3791 [D loss: 0.191023, acc.: 94.44%] [G loss: 5.335406]\n",
      "3792 [D loss: 0.192197, acc.: 94.44%] [G loss: 6.639820]\n",
      "3793 [D loss: 0.271252, acc.: 88.89%] [G loss: 2.946584]\n",
      "3794 [D loss: 0.409170, acc.: 83.33%] [G loss: 5.663621]\n",
      "3795 [D loss: 0.270233, acc.: 77.78%] [G loss: 4.835852]\n",
      "3796 [D loss: 0.408219, acc.: 83.33%] [G loss: 4.660255]\n",
      "3797 [D loss: 0.276694, acc.: 94.44%] [G loss: 4.180990]\n",
      "3798 [D loss: 0.330140, acc.: 88.89%] [G loss: 3.906031]\n",
      "3799 [D loss: 0.063558, acc.: 100.00%] [G loss: 4.504560]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3800 [D loss: 1.477979, acc.: 33.33%] [G loss: 2.634542]\n",
      "3801 [D loss: 0.331854, acc.: 94.44%] [G loss: 4.398706]\n",
      "3802 [D loss: 0.444461, acc.: 77.78%] [G loss: 3.658218]\n",
      "3803 [D loss: 0.511189, acc.: 77.78%] [G loss: 4.332430]\n",
      "3804 [D loss: 0.319891, acc.: 88.89%] [G loss: 4.388000]\n",
      "3805 [D loss: 0.395834, acc.: 88.89%] [G loss: 4.443105]\n",
      "3806 [D loss: 0.507496, acc.: 77.78%] [G loss: 2.936656]\n",
      "3807 [D loss: 0.226869, acc.: 88.89%] [G loss: 3.118034]\n",
      "3808 [D loss: 0.411245, acc.: 83.33%] [G loss: 3.199531]\n",
      "3809 [D loss: 0.027725, acc.: 100.00%] [G loss: 7.316160]\n",
      "3810 [D loss: 0.041191, acc.: 100.00%] [G loss: 5.908450]\n",
      "3811 [D loss: 0.101973, acc.: 94.44%] [G loss: 5.325889]\n",
      "3812 [D loss: 0.026226, acc.: 100.00%] [G loss: 5.014384]\n",
      "3813 [D loss: 0.159411, acc.: 94.44%] [G loss: 5.427065]\n",
      "3814 [D loss: 0.146637, acc.: 94.44%] [G loss: 4.462573]\n",
      "3815 [D loss: 0.045159, acc.: 100.00%] [G loss: 5.098595]\n",
      "3816 [D loss: 0.045252, acc.: 100.00%] [G loss: 4.118286]\n",
      "3817 [D loss: 0.025341, acc.: 100.00%] [G loss: 7.082606]\n",
      "3818 [D loss: 0.125995, acc.: 94.44%] [G loss: 3.449904]\n",
      "3819 [D loss: 0.361016, acc.: 77.78%] [G loss: 4.728747]\n",
      "3820 [D loss: 0.158464, acc.: 88.89%] [G loss: 4.477146]\n",
      "3821 [D loss: 0.083781, acc.: 94.44%] [G loss: 7.231390]\n",
      "3822 [D loss: 0.152381, acc.: 94.44%] [G loss: 4.546572]\n",
      "3823 [D loss: 0.344193, acc.: 88.89%] [G loss: 3.795681]\n",
      "3824 [D loss: 0.037034, acc.: 100.00%] [G loss: 6.404619]\n",
      "3825 [D loss: 0.137052, acc.: 94.44%] [G loss: 5.355175]\n",
      "3826 [D loss: 0.296890, acc.: 83.33%] [G loss: 3.475383]\n",
      "3827 [D loss: 0.174259, acc.: 88.89%] [G loss: 5.580509]\n",
      "3828 [D loss: 0.472051, acc.: 77.78%] [G loss: 3.277337]\n",
      "3829 [D loss: 0.170468, acc.: 83.33%] [G loss: 5.997511]\n",
      "3830 [D loss: 0.186665, acc.: 94.44%] [G loss: 3.700284]\n",
      "3831 [D loss: 0.066193, acc.: 100.00%] [G loss: 3.651104]\n",
      "3832 [D loss: 0.105203, acc.: 100.00%] [G loss: 5.969438]\n",
      "3833 [D loss: 0.156442, acc.: 94.44%] [G loss: 4.741693]\n",
      "3834 [D loss: 0.255842, acc.: 83.33%] [G loss: 5.361443]\n",
      "3835 [D loss: 0.221106, acc.: 88.89%] [G loss: 4.715908]\n",
      "3836 [D loss: 0.474148, acc.: 77.78%] [G loss: 3.206573]\n",
      "3837 [D loss: 0.055851, acc.: 100.00%] [G loss: 4.551516]\n",
      "3838 [D loss: 0.181685, acc.: 94.44%] [G loss: 4.763192]\n",
      "3839 [D loss: 0.202775, acc.: 94.44%] [G loss: 4.253302]\n",
      "3840 [D loss: 0.101938, acc.: 94.44%] [G loss: 5.685821]\n",
      "3841 [D loss: 0.049656, acc.: 100.00%] [G loss: 6.765486]\n",
      "3842 [D loss: 0.261741, acc.: 88.89%] [G loss: 6.210043]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-64699f43e10c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mg_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-60521bbee355>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epochs, batch_size, save_interval)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[1;31m# Train the discriminator (real classified as ones and generated as zeros)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0md_loss_real\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[0md_loss_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m             \u001b[0md_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "g_loss, d_loss = gan.train(epochs=15001, batch_size=9, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(g_loss)\n",
    "plt.plot(d_loss)\n",
    "plt.title('GAN Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Generator', 'Discriminator'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
